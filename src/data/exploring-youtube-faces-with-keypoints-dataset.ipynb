{"nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"version": "3.6.3", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "cells": [{"source": ["# Exploring \"YouTube Faces with Facial Keypoints\" Dataset\n", "In this script we will demonstrate how to access the files of \"YouTube Faces with Facial Keypoints\" dataset.  \n", "We also perform basic cluster analysis of the shapes in the dataset.  \n", "\n", "I will not hide any code in this script so that we get familiarized with the files, how to load them and how to present them.\n", "\n", "![filmreel](http://kjmultimediasolutions.com/wp-content/uploads/2015/07/filmreel1.jpg)\n", "\n", "This dataset is a processed version of the [YouTube Faces Dataset](https://www.cs.tau.ac.il/~wolf/ytfaces/), that basically contained short videos of celebrities that are publicly available and were downloaded from YouTube. There are multiple videos of each celebrity (up to 6 videos per celebrity).  \n", "Additionally, for this kaggle version of the dataset I've extracted facial keypoints for each frame of each video using [this amazing 2D and 3D Face alignment library](https://github.com/1adrianb/face-alignment) that was recently published.  \n", "\n", "For full description please read the description on the [dataset page](https://www.kaggle.com/selfishgene/youtube-faces-with-facial-keypoints)."], "metadata": {"_cell_guid": "2cd251ac-8dd2-4459-9113-143eb128a93d", "_uuid": "f601c3106f37d6f1d8f0d6ba618d25ff47489f81"}, "cell_type": "markdown"}, {"outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "from mpl_toolkits.mplot3d import Axes3D\n", "import matplotlib.pyplot as plt\n", "import matplotlib\n", "import glob\n", "from sklearn import cluster"], "execution_count": null, "metadata": {"_cell_guid": "00f4d343-bc8a-4c30-8ac2-1df2bdeff52d", "collapsed": true, "_uuid": "9d6dca90117f7f9b84b9da33af6099e1906e55aa"}, "cell_type": "code"}, {"source": ["## Show the basic details of the videos in the dataset\n", "In this particular case, show the \"large\" subset of the dataset that contains all videos of individuals with at least 3 different videos each."], "metadata": {"_cell_guid": "8855cee6-b837-4564-a232-d9539f6bcebc", "_uuid": "c16444edd79dac32e7136fe96b0321b9207a5253"}, "cell_type": "markdown"}, {"outputs": [], "source": ["videoDF = pd.read_csv('../input/youtube_faces_with_keypoints_large.csv')\n", "videoDF.head(15)"], "execution_count": null, "metadata": {"_cell_guid": "9a964678-e837-45ab-94bc-091e0cac1737", "_uuid": "1e3d6d00528353afa80e885994de0e454cc6f448"}, "cell_type": "code"}, {"source": ["Each video in the dataset is defined by the \"video ID\" field.  \n", "For each videoID there exists an \"videoID.npz\" (e.g. \"Kevin_Spacey_2.npz\") that are divided among the archive files.  \n", "\n", "* First, since there are multiple archive files with different names we will create a map between the videoID and the full filepath with which we can load the data.\n", "* Then, we will then remove the rows in the videoDF dataframe that are yet to be uploaded to kaggle and keep only the rows of videos that have been uploaded already"], "metadata": {"_cell_guid": "75673bf3-dfff-4323-ad03-46a59bb0acc4", "_uuid": "9eda9a2252677832a5400625e811d4b98c2c2675"}, "cell_type": "markdown"}, {"outputs": [], "source": ["# create a dictionary that maps videoIDs to full file paths\n", "npzFilesFullPath = glob.glob('../input/youtube_faces_*/*.npz')\n", "videoIDs = [x.split('/')[-1].split('.')[0] for x in npzFilesFullPath]\n", "fullPaths = {}\n", "for videoID, fullPath in zip(videoIDs, npzFilesFullPath):\n", "    fullPaths[videoID] = fullPath\n", "\n", "# remove from the large csv file all videos that weren't uploaded yet\n", "videoDF = videoDF.loc[videoDF.loc[:,'videoID'].isin(fullPaths.keys()),:].reset_index(drop=True)\n", "print('Number of Videos is %d' %(videoDF.shape[0]))\n", "print('Number of Unique Individuals is %d' %(len(videoDF['personName'].unique())))"], "execution_count": null, "metadata": {"_cell_guid": "128df3f2-3789-4158-bd65-e4cd6738a2ca", "_uuid": "24bb93c248a8ddbdf1110394356fda306beac949"}, "cell_type": "code"}, {"source": ["## Show Overview of Dataset Content\n", "Again, right now it's the content of a subset of the dataset that was already uploaded.  \n", "When the dataset size limitation will be lifted, I'll upload more videos. This will be automatically updated whenever I'll re-run the script."], "metadata": {"_cell_guid": "de3163ca-e16a-43d8-9926-b0115a62b692", "_uuid": "c99140cc439d50ca8edbb6d4ccd9042b9637d59c"}, "cell_type": "markdown"}, {"outputs": [], "source": ["# overview of the contents of the dataset\n", "groupedByPerson = videoDF.groupby(\"personName\")\n", "numVidsPerPerson = groupedByPerson.count()['videoID']\n", "groupedByPerson.count().sort_values('videoID', axis=0, ascending=False)\n", "\n", "plt.close('all')\n", "plt.figure(figsize=(25,20))\n", "plt.subplot(2,2,1)\n", "plt.hist(x=numVidsPerPerson,bins=0.5+np.arange(numVidsPerPerson.min()-1,numVidsPerPerson.max()+1))\n", "plt.title('Number of Videos per Person',fontsize=30); \n", "plt.xlabel('Number of Videos',fontsize=25); plt.ylabel('Number of People',fontsize=25)\n", "\n", "plt.subplot(2,2,2)\n", "plt.hist(x=videoDF['videoDuration'],bins=28);\n", "plt.title('Distribution of Video Duration',fontsize=30); \n", "plt.xlabel('duration [frames]',fontsize=25); plt.ylabel('Number of Videos',fontsize=25)\n", "plt.xlim(videoDF['videoDuration'].min()-2,videoDF['videoDuration'].max()+2)\n", "\n", "plt.subplot(2,2,3)\n", "plt.scatter(x=videoDF['imageWidth'], y=videoDF['imageHeight'])\n", "plt.title('Distribution of Image Sizes',fontsize=30)\n", "plt.xlabel('Image Width [pixels]',fontsize=25); plt.ylabel('Image Height [pixels]',fontsize=25)\n", "plt.xlim(0,videoDF['imageWidth'].max() +15)\n", "plt.ylim(0,videoDF['imageHeight'].max()+15)\n", "\n", "plt.subplot(2,2,4)\n", "averageFaceSize_withoutNaNs = np.array(videoDF['averageFaceSize'])\n", "averageFaceSize_withoutNaNs = averageFaceSize_withoutNaNs[np.logical_not(np.isnan(averageFaceSize_withoutNaNs))]\n", "plt.hist(averageFaceSize_withoutNaNs, bins=28)\n", "plt.title('Distribution of Average Face Sizes ',fontsize=30)\n", "plt.xlabel('Average Face Size [pixels]',fontsize=25); plt.ylabel('Number of Videos',fontsize=25);\n"], "execution_count": null, "metadata": {"_kg_hide-input": false, "_cell_guid": "34175adb-6414-4c9e-8f65-82641c1d5144", "_uuid": "cae2e02d70f07b1d4698f18800f66e7cbb467a77"}, "cell_type": "code"}, {"source": ["## 2D and 3D Landmarks Data\n", "The best way to introduce the landmarks data is just to look at the following video:"], "metadata": {"_cell_guid": "a905e73f-0118-4401-9a22-2e8ef5fda365", "_uuid": "419c3c68d1b3a79a6771bfba85a0f447b06a1c75"}, "cell_type": "markdown"}, {"outputs": [], "source": ["from IPython.display import YouTubeVideo\n", "YouTubeVideo('8FdSHl4oNIM',width=640, height=480)"], "execution_count": null, "metadata": {"_cell_guid": "d896a352-fba5-4a70-a0f9-4dac6571df4b", "collapsed": true, "_uuid": "6e46772f600a97565f18bda4f14d104af37c749e"}, "cell_type": "code"}, {"source": ["For further details on this really awsome work, please visit [this project page](https://www.adrianbulat.com/face-alignment/).  \n", "The paper can be found [here](https://www.adrianbulat.com/downloads/FaceAlignment/FaceAlignment.pdf).  \n", "The pytorch code can be found [here](https://github.com/1adrianb/face-alignment).  \n", "\n", "**Note**: I'm in no way associated to any of these guys, just genuinely impressed."], "metadata": {"_cell_guid": "0c9b1ef5-edcc-4867-9e6f-23a6a6587a3f", "_uuid": "ebfe144706e12d8436d78baf6755876bd1ad0114"}, "cell_type": "markdown"}, {"source": ["## Show Images from YouTube Faces with 2D Keypoints Overlaid\n", "This shows how to read the dataset and display it's main content"], "metadata": {"_cell_guid": "949d6e93-d6d0-4c00-b174-2170fbf98936", "_uuid": "d328e42adc151e1feba3d42594f644c89d57f3d6"}, "cell_type": "markdown"}, {"outputs": [], "source": ["# show several frames from each video and overlay 2D keypoints\n", "np.random.seed(3)\n", "numVideos = 4\n", "framesToShowFromVideo = np.array([0.1,0.5,0.9])\n", "numFramesPerVideo = len(framesToShowFromVideo)\n", "\n", "# define which points need to be connected with a line\n", "jawPoints          = [ 0,17]\n", "rigthEyebrowPoints = [17,22]\n", "leftEyebrowPoints  = [22,27]\n", "noseRidgePoints    = [27,31]\n", "noseBasePoints     = [31,36]\n", "rightEyePoints     = [36,42]\n", "leftEyePoints      = [42,48]\n", "outerMouthPoints   = [48,60]\n", "innerMouthPoints   = [60,68]\n", "\n", "listOfAllConnectedPoints = [jawPoints,rigthEyebrowPoints,leftEyebrowPoints,\n", "                            noseRidgePoints,noseBasePoints,\n", "                            rightEyePoints,leftEyePoints,outerMouthPoints,innerMouthPoints]\n", "\n", "# select a random subset of 'numVideos' from the available videos\n", "randVideoIDs = videoDF.loc[np.random.choice(videoDF.index,size=numVideos,replace=False),'videoID']\n", "\n", "fig, axArray = plt.subplots(nrows=numVideos,ncols=numFramesPerVideo,figsize=(14,18))\n", "for i, videoID in enumerate(randVideoIDs):\n", "    # load video\n", "    videoFile = np.load(fullPaths[videoID])\n", "    colorImages = videoFile['colorImages']\n", "    boundingBox = videoFile['boundingBox']\n", "    landmarks2D = videoFile['landmarks2D']\n", "    landmarks3D = videoFile['landmarks3D']\n", "\n", "    # select frames and show their content\n", "    selectedFrames = (framesToShowFromVideo*(colorImages.shape[3]-1)).astype(int)\n", "    for j, frameInd in enumerate(selectedFrames):\n", "        axArray[i][j].imshow(colorImages[:,:,:,frameInd])\n", "        axArray[i][j].scatter(x=landmarks2D[:,0,frameInd],y=landmarks2D[:,1,frameInd],s=9,c='r')\n", "        for conPts in listOfAllConnectedPoints:\n", "            xPts = landmarks2D[conPts[0]:conPts[-1],0,frameInd]\n", "            yPts = landmarks2D[conPts[0]:conPts[-1],1,frameInd]\n", "            axArray[i][j].plot(xPts,yPts,c='w',lw=1)\n", "        axArray[i][j].set_title('\"%s\" (t=%d)' %(videoID,frameInd), fontsize=12)\n", "        axArray[i][j].set_axis_off()"], "execution_count": null, "metadata": {"_cell_guid": "8c2e5c20-68aa-463b-acdc-642e2cc54389", "collapsed": true, "_uuid": "ebe3f793ac48eeff0a972257cdb49ae6370097b2"}, "cell_type": "code"}, {"source": ["We can see a small amount of imprefections in the keypoints predictions outputed by the alignment library, but overall this is really amazing performace."], "metadata": {"_cell_guid": "4be8bd4e-03e7-4788-a9dc-7edd74041470", "_uuid": "9955ba926854dc1f1d1f53245bbf16662749ab6a"}, "cell_type": "markdown"}, {"source": ["## Show 3D Keypoints\n", "Interestingly, this work can also extract 3D Keypoints, this shows how one can display the 3D landmarks"], "metadata": {"_cell_guid": "19294b29-a7aa-483f-9bf4-5b28371a003f", "_uuid": "6120f07eb5077e3e76a42d191131dc3ca2d61920"}, "cell_type": "markdown"}, {"outputs": [], "source": ["# show several 3D keypoints\n", "numVideos = 4\n", "framesToShowFromVideo = np.array([0.2,0.5,0.8])\n", "numFramesPerVideo = len(framesToShowFromVideo)\n", "\n", "# select a random subset of 'numVideos' from the available videos\n", "randVideoIDs = videoDF.loc[np.random.choice(videoDF.index,size=numVideos,replace=False),'videoID']\n", "\n", "fig = plt.figure(figsize=(14,14))\n", "for i, videoID in enumerate(randVideoIDs):\n", "    # load video\n", "    videoFile = np.load(fullPaths[videoID])\n", "    colorImages = videoFile['colorImages']\n", "    boundingBox = videoFile['boundingBox']\n", "    landmarks2D = videoFile['landmarks2D']\n", "    landmarks3D = videoFile['landmarks3D']\n", "\n", "    # select frames and show their content\n", "    selectedFrames = (framesToShowFromVideo*(colorImages.shape[3]-1)).astype(int)\n", "    for j, frameInd in enumerate(selectedFrames):\n", "        subplotInd = i*numFramesPerVideo + j+1\n", "        ax = fig.add_subplot(numVideos, numFramesPerVideo, subplotInd, projection='3d')\n", "        ax.scatter(landmarks3D[:,0,frameInd], landmarks3D[:,1,frameInd], landmarks3D[:,2,frameInd],c='r')\n", "        for conPts in listOfAllConnectedPoints:\n", "            xPts = landmarks3D[conPts[0]:conPts[-1],0,frameInd]\n", "            yPts = landmarks3D[conPts[0]:conPts[-1],1,frameInd]\n", "            zPts = landmarks3D[conPts[0]:conPts[-1],2,frameInd]\n", "            ax.plot3D(xPts,yPts,zPts,color='g')         \n", "        ax.set_xlim(ax.get_xlim()[::-1])\n", "        ax.view_init(elev=96, azim=90)\n", "        ax.set_title('\"%s\" (t=%d)' %(videoID,frameInd), fontsize=12)\n", "        \n", "plt.tight_layout()"], "execution_count": null, "metadata": {"_cell_guid": "a852c744-28f0-457e-8f81-1313439074d5", "collapsed": true, "_uuid": "772c6f2babbb8076786dc4ef912af0709777933c"}, "cell_type": "code"}, {"source": ["## Basic EDA\n", "## Normalize 2D and 3D shapes"], "metadata": {"_cell_guid": "be144568-ae55-4404-b34f-11bb73199eca", "_uuid": "3d32b79c97ece4871fc97debd3fb49cd5cecedff"}, "cell_type": "markdown"}, {"outputs": [], "source": ["# collect all 2D and 3D shapes from all frames from all videos to a single numpy array matrix\n", "totalNumberOfFrames = videoDF['videoDuration'].sum()\n", "landmarks2D_all = np.zeros((68,2,int(totalNumberOfFrames)))\n", "landmarks3D_all = np.zeros((68,3,int(totalNumberOfFrames)))\n", "\n", "shapeIndToVideoID = {} # dictionary for later useage\n", "endInd = 0\n", "for i, videoID in enumerate(videoDF['videoID']):\n", "    \n", "    # load video\n", "    videoFile = np.load(fullPaths[videoID])\n", "    landmarks2D = videoFile['landmarks2D']\n", "    landmarks3D = videoFile['landmarks3D']\n", "\n", "    startInd = endInd\n", "    endInd   = startInd + landmarks2D.shape[2]\n", "\n", "    # store in one big array\n", "    landmarks2D_all[:,:,startInd:endInd] = landmarks2D\n", "    landmarks3D_all[:,:,startInd:endInd] = landmarks3D\n", "    \n", "    # make sure we keep track of the mapping to the original video and frame\n", "    for videoFrameInd, shapeInd in enumerate(range(startInd,endInd)):\n", "        shapeIndToVideoID[shapeInd] = (videoID, videoFrameInd)\n", "\n", "# center the shapes around zero\n", "# i.e. such that for each frame the mean x,y,z coordinates will be zero\n", "# or in math terms: Xc = X - mean(X), Yc = Y - mean(Y), Zc = Z - mean(Z)\n", "landmarks2D_centered = np.zeros(landmarks2D_all.shape)\n", "landmarks2D_centered = landmarks2D_all - np.tile(landmarks2D_all.mean(axis=0),[68,1,1])\n", "\n", "landmarks3D_centered = np.zeros(landmarks3D_all.shape)\n", "landmarks3D_centered = landmarks3D_all - np.tile(landmarks3D_all.mean(axis=0),[68,1,1])\n", "\n", "# normalize the shapes such that they have the same scale\n", "# i.e. such that for each frame the mean euclidian distance from the shape center will be one\n", "# or in math terms: mean( sqrt(dX^2 + dY^2 + dZ^2) ) = 1 \n", "landmarks2D_normlized = np.zeros(landmarks2D_all.shape)\n", "landmarks2D_normlized = landmarks2D_centered / np.tile(np.sqrt((landmarks2D_centered**2).sum(axis=1)).mean(axis=0), [68,2,1])\n", "\n", "landmarks3D_normlized = np.zeros(landmarks3D_all.shape)\n", "landmarks3D_normlized = landmarks3D_centered / np.tile(np.sqrt((landmarks3D_centered**2).sum(axis=1)).mean(axis=0), [68,3,1])"], "execution_count": null, "metadata": {"_cell_guid": "ebb8d6cc-673e-42a7-a116-5500f81c4e33", "collapsed": true, "_uuid": "fd43c49eb3b55f2b2936515fbb898b3281794cd8"}, "cell_type": "code"}, {"source": ["## Show 2D Shape Normalization stages"], "metadata": {"_cell_guid": "1663b215-a392-4e1f-bd54-3636a9c98000", "_uuid": "3d72c8d052201f349177485960676d0f99d33e81"}, "cell_type": "markdown"}, {"outputs": [], "source": ["#%% check the 2D normalization and verify that everything is as expected\n", "# select random several frames to be used as test cases\n", "np.random.seed(2)\n", "\n", "listOfShapeColors = ['r','g','b','m','y','c','k']\n", "numShapesToPresent = len(listOfShapeColors)\n", "listOfShapeInds = np.random.choice(range(int(totalNumberOfFrames)),size=numShapesToPresent,replace=False)\n", "\n", "plt.close('all')\n", "plt.figure(figsize=(14,10))\n", "plt.suptitle('Shape Normalization Stages',fontsize=35)\n", "plt.subplot(1,3,1)\n", "for k,shapeInd in enumerate(listOfShapeInds):\n", "    plt.scatter(landmarks2D_all[:,0,shapeInd], -landmarks2D_all[:,1,shapeInd], s=15, c=listOfShapeColors[k])\n", "    for conPts in listOfAllConnectedPoints:\n", "        xPts =  landmarks2D_all[conPts[0]:conPts[-1],0,shapeInd]\n", "        yPts = -landmarks2D_all[conPts[0]:conPts[-1],1,shapeInd]\n", "        plt.plot(xPts,yPts,c=listOfShapeColors[k],lw=1)\n", "plt.axis('off'); plt.title('Original Shapes', fontsize=20)\n", "\n", "plt.subplot(1,3,2)\n", "for k,shapeInd in enumerate(listOfShapeInds):\n", "    plt.scatter(landmarks2D_centered[:,0,shapeInd], -landmarks2D_centered[:,1,shapeInd], s=15, c=listOfShapeColors[k])\n", "    for conPts in listOfAllConnectedPoints:\n", "        xPts =  landmarks2D_centered[conPts[0]:conPts[-1],0,shapeInd]\n", "        yPts = -landmarks2D_centered[conPts[0]:conPts[-1],1,shapeInd]\n", "        plt.plot(xPts,yPts,c=listOfShapeColors[k],lw=1)\n", "plt.axis('off'); plt.title('Centered Shapes', fontsize=20)\n", "\n", "plt.subplot(1,3,3)\n", "for k,shapeInd in enumerate(listOfShapeInds):\n", "    plt.scatter(landmarks2D_normlized[:,0,shapeInd], -landmarks2D_normlized[:,1,shapeInd], s=15, c=listOfShapeColors[k])\n", "    for conPts in listOfAllConnectedPoints:\n", "        xPts =  landmarks2D_normlized[conPts[0]:conPts[-1],0,shapeInd]\n", "        yPts = -landmarks2D_normlized[conPts[0]:conPts[-1],1,shapeInd]\n", "        plt.plot(xPts,yPts,c=listOfShapeColors[k],lw=1)\n", "plt.axis('off'); plt.title('Normlized Shapes', fontsize=20)"], "execution_count": null, "metadata": {"_cell_guid": "e24e0338-0289-4369-b138-5d4f24c8c480", "collapsed": true, "_uuid": "65830eee44442b3e75a6fd10a66b393e7ddb9f98"}, "cell_type": "code"}, {"source": ["On the left we see that the original faces can appear anywhere in the frame, and have several different sizes.  \n", "The centered images are at the same location, but with different sizes. The normlized faces on the right are approximatley same location and and same size. "], "metadata": {"_cell_guid": "624cd1b7-c76b-48d2-bb79-166bef52e2c5", "_uuid": "e8e33bf093bf9bd74970985f32b76eaf8c63661f"}, "cell_type": "markdown"}, {"source": ["## Cluster all 2D shapes using Kmeans and show the resulting clusters"], "metadata": {"_cell_guid": "fe0333fb-feba-45b2-9ca8-fcfb5b09c0f4", "_uuid": "f669eedec702efb7bfc6274dc70491b0eaaf6f35"}, "cell_type": "markdown"}, {"outputs": [], "source": ["#%% cluster normalized shapes and show the cluster centers\n", "numClusters = 16\n", "normalizedShapesTable = np.reshape(landmarks2D_normlized, [68*2, landmarks2D_normlized.shape[2]]).T\n", "\n", "shapesModel = cluster.KMeans(n_clusters=numClusters, n_init=5, random_state=1).fit(normalizedShapesTable[::2,:])\n", "clusterAssignment = shapesModel.predict(normalizedShapesTable)\n", "\n", "plt.figure(figsize=(14,14))\n", "numRowsAndCols = int(np.ceil(np.sqrt(numClusters)))\n", "for i in range(numClusters):\n", "    plt.subplot(numRowsAndCols,numRowsAndCols,i+1);\n", "    currClusterShape = np.reshape(shapesModel.cluster_centers_[i,:], [68,2])\n", "    plt.scatter(x=currClusterShape[:,0],y=-currClusterShape[:,1],s=20,c='r')\n", "    for conPts in listOfAllConnectedPoints:\n", "        xPts =  currClusterShape[conPts[0]:conPts[-1],0]\n", "        yPts = -currClusterShape[conPts[0]:conPts[-1],1]\n", "        plt.plot(xPts,yPts,c='g',lw=1)\n", "    plt.title('cluster %d' %(i),fontsize=15)\n", "    plt.axis('off')"], "execution_count": null, "metadata": {"_cell_guid": "577d0329-5ada-4693-8419-10a64986ed1f", "collapsed": true, "_uuid": "93b6f63141b6663e4acb93c0de73f213ed47073f"}, "cell_type": "code"}, {"source": ["Most clusters are various poses with a neutral expression. It could perhaps be interesting to have a look at the clusters if we increase the number of clusters. you are welcome to fork and have a look."], "metadata": {"_cell_guid": "062f3a09-c598-44e0-a31d-26616281afc8", "_uuid": "9d4310f0b5ab10bfa376e6385800f389d179b0ee"}, "cell_type": "markdown"}, {"source": ["# Show original images assigned to the same shape cluster\n", "## Look to the Left (Cluster 15)\n", "(their left, it's our right)"], "metadata": {"_cell_guid": "5ccfc6a7-91fe-4193-8009-000386ba5b21", "_uuid": "08f41dbc8af2f0891c23de13da1e3093fa0a5099"}, "cell_type": "markdown"}, {"outputs": [], "source": ["#%% show several original images that are assigned to a particular cluster\n", "selectedCluster = 15\n", "numRows = 4; numCols = 4;\n", "\n", "shapeIndsAssignedToCluster = np.nonzero(clusterAssignment == selectedCluster)[0]\n", "listOfShapeInds = np.random.choice(shapeIndsAssignedToCluster ,size=numRows*numCols,replace=False)\n", "\n", "plt.figure(figsize=(14,14))\n", "for i, shapeInd in enumerate(listOfShapeInds):\n", "    # load video and pickout the relevent frame\n", "    videoID  = shapeIndToVideoID[shapeInd][0]\n", "    frameInd = shapeIndToVideoID[shapeInd][1]    \n", "    videoFile = np.load(fullPaths[videoID])\n", "    image = videoFile['colorImages'][:,:,:,frameInd]\n", "    \n", "    # show the image\n", "    plt.subplot(numRows,numCols,i+1);\n", "    plt.imshow(image); plt.axis('off')"], "execution_count": null, "metadata": {"_cell_guid": "5a11742f-36f5-49d3-9e3f-4e63b2b98ad7", "collapsed": true, "_uuid": "219bdedbb9393fa23577435c3b8e3ac97d227236"}, "cell_type": "code"}, {"source": ["We can see that all of those images have similar face poses, which was exactly the goal."], "metadata": {"_cell_guid": "ccc36563-e35a-4ad7-8164-c968a05cd657", "_uuid": "2dcb708bab90f44ed1473a326c6be1cdf0ae41fa"}, "cell_type": "markdown"}, {"source": ["## Now look to the Right (Cluster 2)\n", "(their right, it's our left)"], "metadata": {"_cell_guid": "610ab800-a48d-4510-be78-dbd2ba1db1d2", "_uuid": "a020c309fe03b6292ef02e6b61d70864fe690eac"}, "cell_type": "markdown"}, {"outputs": [], "source": ["#%% show several original images that are assigned to a particular cluster\n", "selectedCluster = 2\n", "numRows = 4; numCols = 4;\n", "\n", "shapeIndsAssignedToCluster = np.nonzero(clusterAssignment == selectedCluster)[0]\n", "listOfShapeInds = np.random.choice(shapeIndsAssignedToCluster ,size=numRows*numCols,replace=False)\n", "\n", "plt.figure(figsize=(14,14))\n", "for i, shapeInd in enumerate(listOfShapeInds):\n", "    # load video and pickout the relevent frame\n", "    videoID  = shapeIndToVideoID[shapeInd][0]\n", "    frameInd = shapeIndToVideoID[shapeInd][1]    \n", "    videoFile = np.load(fullPaths[videoID])\n", "    image = videoFile['colorImages'][:,:,:,frameInd]\n", "    \n", "    # show the image\n", "    plt.subplot(numRows,numCols,i+1);\n", "    plt.imshow(image); plt.axis('off')"], "execution_count": null, "metadata": {"_cell_guid": "c55430c8-4dd5-44a9-be97-37b09aae1202", "collapsed": true, "_uuid": "fc56bf9b50135accf529fef2b5c27b241e8ccbe5"}, "cell_type": "code"}]}