---

title: 'Wiki Traffic Forecast Exploration - WTF EDA'
output:
  html_document:
    number_sections: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```



# Introduction

This is a comprehensive Exploratory Data Analysis for the [Web Traffic Time Series Forecasting](https://www.kaggle.com/c/web-traffic-time-series-forecasting) competition with tidy R.

This challenge is about predicting the future behaviour of time series' that describe the web traffic for Wikipedia articles. The [data](https://www.kaggle.com/c/web-traffic-time-series-forecasting/data) contains about 145k time series and comes in two separate files: *train_1.csv* holds the traffic data, where each column is a date and each row is an article, and *key_1.csv* contains a mapping between page names and a unique ID column (to be used in the submission file).


## Load libraries and data files

```{r, message = FALSE}
library('ggplot2') # visualization
library('ggthemes') # visualization
library('scales') # visualization
library('grid') # visualisation
library('gridExtra') # visualisation
library('corrplot') # visualisation
library('ggrepel') # visualisation
library('RColorBrewer') # visualisation
library('data.table') # data manipulation
library('dplyr') # data manipulation
library('readr') # data input
library('tibble') # data wrangling
library('tidyr') # data wrangling
library('lazyeval') # data wrangling
library('broom') # data wrangling
library('stringr') # string manipulation
library('purrr') # string manipulation
library('forcats') # factor manipulation
library('lubridate') # date and time
library('forecast') # time series analysis
library('prophet') # time series analysis
```



```{r, echo=FALSE}
# Define multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```


## Load data

Note, that the *key_1.csv* data is not small with about 700 MB and for the purpose of this exploration we only read a few rows to show its structure.

```{r, message=FALSE, warning=FALSE, echo = FALSE, results=FALSE}
train <- as.tibble(fread('../input/train_1.csv'))
key <- as.tibble(fread('../input/key_1.csv', nrows = 5))
```

## File structure and content

Those are the dimensions of the *train* data set:

```{r}
c(ncol(train),nrow(train))
```

The data is originally structured so that 550 dates refer to a column each:

```{r}
train %>% colnames() %>% head(5)
```

and the 145k article nanes are stored in the additional *Page* column:

```{r}
train %>% select(Page) %>% head(5)
```

The *key* data contains a unique alpha-numerical ID for each *Page* and *Date* combination, which is the reason for the relatively large file size.

```{r}
glimpse(key)
```


## Missing values

```{r}
sum(is.na(train))/(ncol(train)*nrow(train))
```

There are about 8% of missing values in this data set, which is not trivial. We will neeed to take them into account in our analysis.


# Data transformation and helper functions

## Article names and metadata

To make the training data easier to handle we split it into two part: the article information (from the *Page* column) and the time series data (*tdates*) from the date columns. We briefly separate the article information into data from *wikipedia*, *wikimedia*, and *mediawiki* due to the different formatting of the *Page* names. After that, we rejoin all article information into a common data set (*tpages*).

```{r}
tdates <- train %>% select(-Page)

foo <- train %>% select(Page) %>% rownames_to_column()
mediawiki <- foo %>% filter(str_detect(Page, "mediawiki"))
wikimedia <- foo %>% filter(str_detect(Page, "wikimedia"))
wikipedia <- foo %>% filter(str_detect(Page, "wikipedia")) %>% 
  filter(!str_detect(Page, "wikimedia")) %>%
  filter(!str_detect(Page, "mediawiki"))

wikipedia <- wikipedia %>%
  separate(Page, into = c("foo", "bar"), sep = ".wikipedia.org_") %>%
  separate(foo, into = c("article", "locale"), sep = -3) %>%
  separate(bar, into = c("access", "agent"), sep = "_") %>%
  mutate(locale = str_sub(locale,2,3))

wikimedia <- wikimedia %>%
  separate(Page, into = c("article", "bar"), sep = "_commons.wikimedia.org_") %>%
  separate(bar, into = c("access", "agent"), sep = "_") %>%
  add_column(locale = "wikmed")

mediawiki <- mediawiki %>%
  separate(Page, into = c("article", "bar"), sep = "_www.mediawiki.org_") %>%
  separate(bar, into = c("access", "agent"), sep = "_") %>%
  add_column(locale = "medwik")

tpages <- wikipedia %>%
  full_join(wikimedia, by = c("rowname", "article", "locale", "access", "agent")) %>%
  full_join(mediawiki, by = c("rowname", "article", "locale", "access", "agent"))

sample_n(tpages, size = 5)
```

Now we can search for certain *Page* subjects and filter their meta parameters:

```{r}
tpages %>% filter(str_detect(article, "The_Beatle")) %>%
  filter(access == "all-access") %>%
  filter(agent == "all-agents")
```


## Time series extraction

In order to plot the time series data we use a helper function that allows us to extract the time series for a specified row number. (The normalised version is to facilitate the coparision between multiple time series curves, to correct for large differences in view count.)

```{r}
extract_ts <- function(rownr){
  tdates %>%
    filter_((interp(~x == row_number(), .values = list(x = rownr)))) %>%
    rownames_to_column %>% 
    gather(dates, value, -rowname) %>% 
    spread(rowname, value) %>%
    mutate(dates = ymd(dates),
          views = as.integer(`1`)) %>%
    select(-`1`)
}

extract_ts_nrm <- function(rownr){
  tdates %>%
    filter_((interp(~x == row_number(), .values = list(x = rownr)))) %>%
    rownames_to_column %>% 
    gather(dates, value, -rowname) %>% 
    spread(rowname, value) %>%
    mutate(dates = ymd(dates),
          views = as.integer(`1`)) %>%
    select(-`1`) %>%
    mutate(views = views/mean(views))
}
```

A custom-made plotting function allows us to visualise each time series and extract its meta data:

```{r}
plot_rownr <- function(rownr){
  art <- tpages %>% filter(rowname == rownr) %>% .$article
  loc <- tpages %>% filter(rowname == rownr) %>% .$locale
  acc <- tpages %>% filter(rowname == rownr) %>% .$access
  extract_ts(rownr) %>%
    ggplot(aes(dates, views)) +
    geom_line() +
    geom_smooth(method = "loess", color = "blue", span = 1/5) +
    labs(title = str_c(art, " - ", loc, " - ", acc))
}

plot_rownr_log <- function(rownr){
  art <- tpages %>% filter(rowname == rownr) %>% .$article
  loc <- tpages %>% filter(rowname == rownr) %>% .$locale
  acc <- tpages %>% filter(rowname == rownr) %>% .$access
  extract_ts_nrm(rownr) %>%
    ggplot(aes(dates, views)) +
    geom_line() +
    geom_smooth(method = "loess", color = "blue", span = 1/5) +
    labs(title = str_c(art, " - ", loc, " - ", acc)) +
    scale_y_log10() + labs(y = "log views")
}

plot_rownr_zoom <- function(rownr, start, end){
  art <- tpages %>% filter(rowname == rownr) %>% .$article
  loc <- tpages %>% filter(rowname == rownr) %>% .$locale
  acc <- tpages %>% filter(rowname == rownr) %>% .$access
  extract_ts(rownr) %>%
    filter(dates > ymd(start) & dates <= ymd(end)) %>%
    ggplot(aes(dates, views)) +
    geom_line() +
    #geom_smooth(method = "loess", color = "blue", span = 1/5) +
    #coord_cartesian(xlim = ymd(c(start,end))) +  
    labs(title = str_c(art, " - ", loc, " - ", acc))
}
```

This is how it works (to visualise timey-wimey stuff):

```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 1", out.width="100%"}
plot_rownr(11214)
```

In addition, with the help of the extractor tool we define a function that re-connects the *Page* information to the corresponding time series and plots this curve according to our specification on *article* name, *access* type, and *agent* for all the available languages:

```{r}
plot_names <- function(art, acc, ag){

  pick <- tpages %>% filter(str_detect(article, art)) %>%
    filter(access == acc) %>%
    filter(agent == ag)
  pick_nr <- pick %>% .$rowname
  pick_loc <- pick %>% .$locale

  tdat <- extract_ts(pick_nr[1]) %>%
    mutate(loc = pick_loc[1])

  for (i in seq(2,length(pick))){
    foo <- extract_ts(pick_nr[i]) %>%
    mutate(loc = pick_loc[i])
    tdat <- bind_rows(tdat,foo)
  }

  plt <- tdat %>%
    ggplot(aes(dates, views, color = loc)) +
    geom_line() + 
    labs(title = str_c(art, "  -  ", acc, "  -  ", ag))

  print(plt)
}

plot_names_nrm <- function(art, acc, ag){

  pick <- tpages %>% filter(str_detect(article, art)) %>%
    filter(access == acc) %>%
    filter(agent == ag)
  pick_nr <- pick %>% .$rowname
  pick_loc <- pick %>% .$locale

  tdat <- extract_ts_nrm(pick_nr[1]) %>%
    mutate(loc = pick_loc[1])

  for (i in seq(2,length(pick))){
    foo <- extract_ts_nrm(pick_nr[i]) %>%
    mutate(loc = pick_loc[i])
    tdat <- bind_rows(tdat,foo)
  }

  plt <- tdat %>%
    ggplot(aes(dates, views, color = loc)) +
    geom_line() + 
    labs(title = str_c(art, "  -  ", acc, "  -  ", ag)) +
    scale_y_log10() + labs(y = "log views")

  print(plt)
}
```



Here is a classic example:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 2", out.width="100%"}

plot_names("The_Beatles", "all-access", "all-agents")

```



These are the tools we need for a visual examinination of arbitrary individual time series data. In the following, we will use them to illustrate specific observations that are of particular interest.



# Summary parameter extraction



In the next step we will have a more global look at the population parameters of our training time series data. Also here, we will start with the *wikipedia* data. The idea behind this approach is to probe the parameter space of the time series information along certain key metrics and to identify extreme observations that could break our forecasting strategies.



## Projects data overview



Before diving into the time series data let's have a look how the different meta-parameters are distributed:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 3", out.width="100%"}

p1 <- tpages %>% 

  ggplot(aes(agent)) + geom_bar(fill = "red")

p2 <- tpages %>% 

  ggplot(aes(access)) + geom_bar(fill = "red")

p3 <- tpages %>% 

  ggplot(aes(locale, fill = locale)) + geom_bar() + theme(legend.position = "none")



layout <- matrix(c(1,2,3,3),2,2,byrow=TRUE)

multiplot(p1, p2, p3, layout=layout)

```



We find that our *wikipedia* data includes 7 languages: German, English, Spanish, French, Japanese, Russian, and Chinese. All of those are more frequent than the *mediawiki* and *wikimedia* pages. Mobile sites are slightly more frequent than desktop ones.





## Basic time series parameters



We start with a basic set of parameters: mean, standard deviation, amplitude, and a the slope of a naive linear fit. This is our extraction function:



```{r}

params_ts1 <- function(rownr){

  foo <- tdates %>%

    filter_((interp(~x == row_number(), .values = list(x = rownr)))) %>%

    rownames_to_column %>% 

    gather(dates, value, -rowname) %>% 

    spread(rowname, value) %>%

    mutate(dates = ymd(dates),

          views = as.integer(`1`))

    

  slope <- ifelse(is.na(mean(foo$views)),0,summary(lm(views ~ dates, data = foo))$coef[2])

  slope_err <- ifelse(is.na(mean(foo$views)),0,summary(lm(views ~ dates, data = foo))$coef[4])



  bar <- tibble(

    rowname = rownr,

    min_view = min(foo$views),

    max_view = max(foo$views),

    mean_view = mean(foo$views),

    med_view = median(foo$views),

    sd_view = sd(foo$views),

    slope = slope/slope_err

  )

  

  return(bar)

}

```



And here we run it. (Note, that in this kernel version I'm currently using a sub-sample of the data for reasons of runtime. My extractor function is not very elegant, yet, and exceeds the kernel runtime for the complete data set.)



```{r}

set.seed(4321)

foo <- sample_n(tpages, 5500) #5500

#foo <- tpages

rows <- foo$rowname

pcols <- c("rowname", "min_view", "max_view", "mean_view", "med_view", "sd_view", "slope")



params <- params_ts1(rows[1])

for (i in seq(2,nrow(foo))){

  params <- full_join(params, params_ts1(rows[i]), by = pcols)

}



params <- params %>%

  filter(!is.na(mean_view)) %>%

  mutate(rowname = as.character(rowname))

```





## Overview visualisations



Let's explore the parameter space we've built. (The global shape of the distributions should not be affected by the sampling.) First we plot the histograms of our main parameters:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 4", out.width="100%"}

p1 <- params %>% 

  ggplot(aes(mean_view)) + geom_histogram(fill = "red", bins = 50) + scale_x_log10()

p2 <- params %>% 

  ggplot(aes(max_view)) + geom_histogram(fill = "red", bins = 50) + scale_x_log10()

p3 <- params %>% 

  ggplot(aes(sd_view/mean_view)) + geom_histogram(fill = "red", bins = 50) + scale_x_log10()

p4 <- params %>% 

  ggplot(aes(slope)) + geom_histogram(fill = "red", bins = 30) + 

  scale_x_continuous(limits = c(-25,25))



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



We find:



- The distribution of average views is clearly bimodal, with peaks around 10 and 200-300 views. Something similar is true for the number of maximum views, although here the first peak (around 200) is curiuosly narrow. The second peak is centred above 10,000.



- The distribution of standard deviations (divided by the mean) is skewed toward higher values with larger numbers of spikes or stronger variability trends. Those will be the observations that are more challenging to forecast.



- The slope distribution is resonably symmetric and centred notably above zero.



Let's split it up by *locale* and focus on the densities:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 5", out.width="100%"}

par_page <- left_join(params,tpages, by = "rowname")

p1 <- par_page %>% 

  ggplot(aes(mean_view, fill = locale)) +

  geom_density(position = "stack") +

  scale_x_log10(limits = c(1,1e4)) +

  theme(legend.position = "none")

p2 <- par_page %>% 

  ggplot(aes(max_view, fill = locale)) +

  geom_density(position = "stack") +

  scale_x_log10(limits = c(10,1e6)) +

  theme(legend.position = "none")

p3 <- par_page %>%

  ggplot(aes(sd_view, fill = locale)) +

  geom_density(position = "stack") +

  scale_x_log10(limits = c(1,1e5)) +

  theme(legend.position = "none")

p4 <- par_page %>% 

  ggplot(aes(slope, fill = locale)) +

  geom_density(position = "stack") + 

  scale_x_continuous(limits = c(-10,10))



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



We find:



- The chinese pages (zh, in pink) are slightly but notably different from the rest. The have lower mean and max views and also less variation. Their slope distribution is broader, but also shifted more towards positive values compared to the other curves.



- The peak in max views around 200-300 is most pronounced in the french pages (fr, in turquoise).



- The english pages (en, in mustard) have the highest mean and maximum views, which is not surprising.





Next, we will examine binned 2-d histograms.



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 6", out.width="100%"}

params %>%

  ggplot(aes(max_view-mean_view, mean_view)) +

  geom_bin2d(bins = c(50,50)) +

  scale_x_log10() +

  scale_y_log10() +

  labs(x = "maximum views above mean", y = "mean views")

```



We find:



- There is a clear correlation between mean views and maximum views. Also here we find again the two cluster peaks we had identified in the individual histograms. A couple of outliers and outlier groups are noticeable.



Let's zoom into the upper right corner (the numbers in parentheses are the row numbers):



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 7", out.width="100%"}

limx <- c(max(params$max_view)/50, max(params$max_view))

limy <- c(max(params$mean_view)/50, max(params$mean_view))

par_page %>%

  ggplot(aes(max_view-mean_view, mean_view)) +

  geom_point(size = 2, color = "red") +

  scale_x_log10(limits = limx) +

  scale_y_log10(limits = limy) +

  labs(x = "maximum views above mean", y = "mean views") +

  geom_label_repel(aes(label = str_c(article, " (",rowname,")")), alpha = 0.5)

```



Here we find a number of main pages and other meta pages (in the full data set).





Another question: Does the (assumed) linear change in views depend on the total number of views?



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 8", out.width="100%"}

params %>%

  ggplot(aes(slope, mean_view)) +

  geom_point(color = "red", alpha = 0.1) +

  scale_y_log10() +

  labs(x = "linear slope relative to slope error", y = "mean views")

```



We find that articles with higher average view-count have more variability in their linear trends. However, this might be due to our slope normalisation which will decrease the effective slope for low view counts. It should not, however, affect the observation that the slopes of low-view articles are on average slightly higher than those of high-view articles. Such an effect could be caused by viewing spikes, of course, but I would expect those to be randomly distributed.





# Individual observations with extreme parameters



Based on the overview parameters we can focus our attention on those articles for which the time series parameters are at the extremes of the parameter space.



## Large linear slope



Those are the observations with the highest slope values. (In the sample this will be different, but in the full *wikipedia* data set the top 10 have rownames 91728, 55587, 108341, 70772, 95367, 18357, 95229, 116150, 94975, 77292).



```{r}

params %>% arrange(desc(slope)) %>% head(5) %>% select(rowname, slope, everything())

```



Let's have a look at the time series data of the top 4 articles:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 9", out.width="100%"}

p1 <- plot_rownr(91728)

p2 <- plot_rownr(55587)

p3 <- plot_rownr(108341)

p4 <- plot_rownr(70772)



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



We find:



- Lot's of love for Twenty One Pilots in Spain. Those rapid rises and wibbly-wobbly bits are going to be difficult to predict, unless there's a periodic modulation on top of the large-scale trend. Certaintly worth figuring out. 



- We also see that our generic loess smoother is dealing rather well with most of the slower variability patterns and could be used to remove the low-frequency structures for further analysis.



Let's compare the interest in Twenty One Pilots for the different countries, to see whether a prediction for one of them could learn from the others:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 10", out.width="100%"}

plot_names_nrm("Twenty_One_Pilots", "all-access", "all-agents")

```



Note, that those curves are normalised to mean views (each) and have a logarithmic y-axis to mitigate the effect of large spikes. This chart is for relative trend comparison.



We find:



- Germany and France show quite similar viewing behaviour, while Russia and Spain are comparable too; especially in the early rise in interest. The English pages show less dramatic changes but end up 



- With a purely time-series-forecast approach I think that the large spikes are close to impossible to predict. However, external data could help a lot here.





Those viewing numbers were going up, but which articles were going down? (Top 10: 95856, 74115, 8388, 103659, 100213, 9633, 102481 38458, 30042, 74002).



```{r}

params %>% arrange(slope) %>% head(5) %>% select(rowname, slope, everything())

```





```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 11", out.width="100%"}

p1 <- plot_rownr(95856)

p2 <- plot_rownr(74115)

p3 <- plot_rownr(8388)

p4 <- plot_rownr(103659)



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



The main page itself on mobile, and review articles on 2015 were the biggest losers.





## High standard deviations



The top 10 *wikipedia* rows are 9775, 38574, 103124, 99323, 74115, 39181, 10404, 33645, 34258, and 26994. Bingo, anyone?



```{r}

params %>% arrange(desc(sd_view/mean_view)) %>% head(5) %>% 

  select(rowname, sd_view, mean_view, max_view, everything())

```



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 12", out.width="100%"}

p1 <- plot_rownr(9775)

p2 <- plot_rownr(38574)

p3 <- plot_rownr(103124)

p4 <- plot_rownr(99323)



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



Those are pretty strong spikes in the main page views, even if the baseline is around 1-10 million to begin with. They look consistent though over different languages. Any ideas what could cause this?



If we normalise standard deviation by mean we get a different set of results:



```{r}

params %>% arrange(desc(sd_view/mean_view)) %>% head(5) %>% 

  select(rowname, sd_view, mean_view, max_view, everything())

```





```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 13", out.width="100%"}

p1 <- plot_rownr(10032)

p2 <- plot_rownr(38812)

p3 <- plot_rownr(86905)

p4 <- plot_rownr(102521)



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



Those are very, very suspicious. They are essentially low baselines with single dates that have way higher view counts (e.g. around 20 vs 2 million for the upper left one). These have to be errors in the data which can be dangerous for predictions if they appear close to either end of the date window. In other cases, most smoothing methods should be able to deal with them.





## Large variability amplitudes



The top amplitudes are the same as the top standard deviations, due to the spikey nature of the variability:



```{r}

params %>% arrange(desc(max_view - mean_view)) %>% 

  head(5) %>% select(rowname, max_view, mean_view, everything())

```





## High average views



Those are the time series of the most popular pages, which we already identified as the main pages in the plots above:



```{r}

params %>% arrange(desc(mean_view)) %>% 

  head(5) %>% select(rowname, max_view, mean_view, everything())

```



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 14", out.width="100%"}

p1 <- plot_rownr(38574)

p2 <- plot_rownr(9775)

p3 <- plot_rownr(74115)

p4 <- plot_rownr(139120)



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



In addition to the spikes on the english main page there is a suprising amount of variability as exemplified by the long-term structure in the German main page.



What about other main pages, as identified in the zoom-in above?



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 15", out.width="100%"}

p1 <- plot_rownr_log(92206)

p2 <- plot_rownr(116197)

p3 <- plot_rownr_log(10404)

p4 <- plot_rownr_log(33645)



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



Here 3 of the 4 plots have a logarithmic y-axis to improve the clarity of visualising the time series' with strong spikes. We see that also those popular pages exhibit strong variability on various time scales.



**In summary: We have identified the time series' with the highest variability according to basic criteria. We also found a few time series sets with bogus values. These are the data sets that might pose the greatest challenge to our prediction algorithms.**





# Short-term variability



Before turning to forecasting methods, let's have a closer look at the characteristic short-term variability that has become evident in several of the plots already. Below, we plot a 2-months zoom into the "quiet" parts (i.e. no strong spikes) of different time series:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 16", out.width="100%"}

p1 <- plot_rownr_zoom(10404, "2016-10-01", "2016-12-01")

p2 <- plot_rownr_zoom(9775, "2015-09-01", "2015-11-01")

p3 <- plot_rownr_zoom(139120, "2016-10-01", "2016-12-01")

p4 <- plot_rownr_zoom(110658, "2016-07-01", "2016-09-01")



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



We see that the high-view-count time series on the left hand side show a very regular periodicity that is strikingly similar for both of them. A similar structure can be seen on the right hand side, although here it is partly distorted by a slight upward trend (upper right) and/or variance caused by lower viewing numbers (lower right).



These plots provide evidence that there is variability on a *weekly* scale. The next figure will visualise this weekly behaviour in a different way:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 17", out.width="100%"}

rownr <- 10404

start <- "2016-10-01"

end <- "2016-12-01"

foo1 <- extract_ts(rownr) %>%

  filter(dates > ymd(start) & dates < ymd(end)) %>%

  mutate(dates = wday(dates, label = TRUE)) %>%

  group_by(dates) %>%

  summarise(wday_views = mean(views)) %>%

  mutate(wday_views = wday_views/mean(wday_views)) %>%

  mutate(id = factor(rownr))



rownr <- 9775

start <- "2015-09-01"

end <- "2015-11-01"

foo2 <- extract_ts(rownr) %>%

  filter(dates > ymd(start) & dates < ymd(end)) %>%

  mutate(dates = wday(dates, label = TRUE)) %>%

  group_by(dates) %>%

  summarise(wday_views = mean(views)) %>%

  mutate(wday_views = wday_views/mean(wday_views)) %>%

  mutate(id = factor(rownr))



rownr <- 139120

start <- "2016-10-01"

end <- "2016-12-01"

foo3 <- extract_ts(rownr) %>%

  filter(dates > ymd(start) & dates < ymd(end)) %>%

  mutate(dates = wday(dates, label = TRUE)) %>%

  group_by(dates) %>%

  summarise(wday_views = mean(views)) %>%

  mutate(wday_views = wday_views/mean(wday_views)) %>%

  mutate(id = factor(rownr))



rownr <- 110658

start <- "2016-07-01"

end <- "2016-09-01"

foo4 <- extract_ts(rownr) %>%

  filter(dates > ymd(start) & dates < ymd(end)) %>%

  mutate(dates = wday(dates, label = TRUE)) %>%

  group_by(dates) %>%

  summarise(wday_views = mean(views)) %>%

  mutate(wday_views = wday_views/mean(wday_views)) %>%

  mutate(id = factor(rownr))



foo <- bind_rows(foo1,foo2,foo3,foo4)



foo %>%

  ggplot(aes(dates, wday_views, color = id)) +

  geom_jitter(size = 4, width = 0.1) +

  labs(x = "Day of the week", y = "Relative average views")

```



Here we average the variability in the previous plot over the day of the week and then overlay all four time series with different colours on a relative scale. We see the clear trend toward lower viewing numbers on the weekend (Fri/Sat/Sun), and also a declining trend from Monday through Thursday. This gives us valuable information on the general type of variability over the course of a week. In order to study this behaviour more in detail, we would need to average over a larger number of time series.



Of course, we can see the same variability pattern in a [power density spectrum](https://en.wikipedia.org/wiki/Spectral_density) which turns our time series into an analysis of characteristic periodicities via a [(Fast) Fourier Transformation](https://en.wikipedia.org/wiki/Fourier_transform). Here is a short helper function to plot the spectrum for an arbitrary row number and time range:



```{r}

plot_pds_rownr_zoom <- function(rownr,start,end){

  art <- tpages %>% filter(rowname == rownr) %>% .$article

  loc <- tpages %>% filter(rowname == rownr) %>% .$locale

  acc <- tpages %>% filter(rowname == rownr) %>% .$access

  

  pds <- extract_ts(rownr) %>%

  filter(dates > ymd(start) & dates < ymd(end)) %>%

  select(-dates) %>%

  ts() %>%

  spectrum(plot = FALSE)



  p <- tibble(period = 1./pds$freq,

              power = pds$spec**2) %>%

    ggplot(aes(period, power)) +

    geom_vline(xintercept = 7, colour="blue") +

    geom_line(color = "black", tsize = 2) +

    scale_x_log10() +

    labs(title = str_c("PDS of ", art, "-", loc, "-", acc))

  

  return(p)

}

```



And here we apply it to our four time series sets:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 18", out.width="100%"}

p1 <- plot_pds_rownr_zoom(10404, "2016-10-01", "2016-12-01")

p2 <- plot_pds_rownr_zoom(9775, "2015-09-01", "2015-11-01")

p3 <- plot_pds_rownr_zoom(139120, "2016-10-01", "2016-12-01")

p4 <- plot_pds_rownr_zoom(110658, "2016-07-01", "2016-09-01")



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



In this plot, a period of 7 days is indicated by the vertical blue line.



As expected, we find that all four data sets share a strong signal at a period of 1 week. This is particularly evident in the clean time series' with high median views, but also still in the article with lower views. **In our following analysis we can therefore reasonably assume that a period of 7 days is present in all our articles.**





# Forecast methods for selected examples



Now that we have identified a sample of time series' with extreme parameters we can use them to test different forecasting methods. For a sample of 145k articles we will most likely have to rely on an automatic mechanism to make our predictions (although a degree of fine-tuning might be possible). Therefore, our forecasting method will have to perform robustly for a range of different time series shapes and varibilities. Those methods that manage to deal with our extreme examples should be able to deal with any less variable time series as well.



For this competition our forecast period is 2 monts, i.e. about 60 days. In the following, we simulate this period and assess our prediction accuracy by keeping a hold-out sample of the last 60 days from our forecasting data. After making the prediction we can compare the actual view counts to the forecasted ones.





## ARIMA / auto.arima



A popular approach in time series forecasting is to use an *autoregressive integrated moving average* model; short [ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) model. This kind of model consists of three parts, parametrised by indeces *p, d, q* as ARIMA(p, d, q):



- **auto-regressive / p:** we are using past data to compute a regression model for future data. The parameter *p* indicates the range of *lags*; e.g. ARIMA(3,0,0) includes *t-1*, *t-2*, and *t-3* values in the regression to compute the value at *t*.



- **integrated / d:** this is a *differencing* parameter, which gives us the number of times we are subtracting the current and the previous values of a time series. Differencing removes the change in a time series in that it stabilises the mean and removes (seasonal) trends. This is necessary since computing the lags (e.g. difference between time *t* and time *t-1*) is most meaningful if large-scale trends are removed. A time series where the variance (or amount of variability) (and the autocovariance) are time-invariant (i.e. don't change from day to day) is called *stationary*.



- **moving average / q:** this parameter gives us the number of previous error terms to include in the regression error of the model.



Using our insight about the weekly periodicity, we can directly incorporate this frequency when turning our view counts into a time series object (using the *ts* function). Note, that we also perform some cleaning and outlier rejection using the *tsclean* tool. As usual, we wrap the modelling and plotting process into a function and then apply it to four time series sets that we know from our previous analysis:



```{r}

plot_auto_arima_rownr <- function(rownr){

  

  pageviews <- extract_ts(rownr) %>%

    rownames_to_column() %>%

    mutate(rowname = as.integer(rowname))

  pred_len <- 60

  pred_range <- c(nrow(pageviews)-pred_len+1, nrow(pageviews))

  pre_views <- pageviews %>% head(nrow(pageviews)-pred_len)

  post_views <- pageviews %>% tail(pred_len)



  arima.fit <- auto.arima(tsclean(ts(pre_views$views, frequency = 7)),

                          d = 1, D = 1, stepwise = FALSE, approximation = FALSE)

  fc_views <- arima.fit %>% forecast(h = pred_len, level = c(50,95))

  autoplot(fc_views) +

    geom_line(aes(rowname/7, views), data = post_views, color = "grey40") +

    labs(x = "Time [weeks]", y = "views vs auto.arima predictions")

}

```



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 19", out.width="100%"}

p1 <- plot_auto_arima_rownr(70772)

p2 <- plot_auto_arima_rownr(108341)

p3 <- plot_auto_arima_rownr(95856)

p4 <- plot_auto_arima_rownr(139120)



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



The results are not too bad, actually. Especially the lower left plot. We even got a downturn in the upper left plot. The upper right plot is a challenging problem, because the levelling of the viewer numbers at the end of the time range was not predictable from the previous behaviour. The same is true for the large spike in the lower right plot.



**Given that it's a fully automatic forecast (assuming only weekly periodicities) the auto.arima tool performs decently and provides us with a useful baseline to compare other methods to.**





## Prophet - *Section currently under maintenance*



[Prophet](https://facebookincubator.github.io/prophet/) is an open-source time series forecasting tool developed by [Facebook](https://research.fb.com/prophet-forecasting-at-scale/). It is implemented in an *R* library, and also a Python package (as [already](https://www.kaggle.com/attollos/time-series-forecast-example-with-prophet) [shown](https://www.kaggle.com/tunguz/forecast-example-w-prophet-median) in this competition).



Prophet works as an additive regression model which decomposes a time series into (i) a (piecewise) linear/logistic trend, (ii) a yearly seasonal component, (iii) a weekly seasonal component, and (iv) an optional list of important days (such as holidays, special events, ...). It claims to be "robust to missing data, shifts in the trend, and large outliers", which would make it well suited for this particular task.





### Basic performance



First, let's test the tool: 



```{r message=FALSE, error=FALSE}

rownr <- 139120

pageviews <- extract_ts(rownr) %>%

  rename(y = views,

         ds = dates)

pred_len <- 60

pred_range <- c(nrow(pageviews)-pred_len+1, nrow(pageviews))

pre_views <- pageviews %>% head(nrow(pageviews)-pred_len)

post_views <- pageviews %>% tail(pred_len)



proph <- prophet(pre_views, changepoint.prior.scale=0.5, yearly.seasonality=TRUE)

future <- make_future_dataframe(proph, periods = pred_len)

fcast <- predict(proph, future)

```



A few notes about the practical workings of prophet:



- *data format:* prophet expects a data frame with two columns: *ds, y*. The first one holds the dates, the second one the time series counts.



- *parameter changepoint.prior.scale* adjusts the trend flexibility. Increasing this parameter makes the fit more flexible, but also increases the forecast uncertainties and makes it more likely to overfit to noise. The changepoints in the data are automatically detected unless being specified by hand using the *changepoints* argument (which we don't do here).



- *parameter yearly.seasonality=TRUE* has to be enabled explicitely and allows prophet to notice large-scale cycles. The importance of this parameter is explored further below.



This is the standard prophet forecast plot:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 20", out.width="100%"}

plot(proph, fcast)

```



The observed data are plotted as black points and the fitted model, plus forecast, as a blue line. In light blue we see the corresponding uncertainties.



Prophet offers a decomposition plot, where we can inspect the additive components of the model: trend, yearly seasonality, and weekly cycles:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 21", out.width="100%"}

prophet_plot_components(proph, fcast)

```



We see that prophet recovers the weekly variation pattern we had extracted by hand in the previous section. This is a useful consistency check. The seasonal variability suggests an overall decline in views towards the middle of the year.



Being the ggplot2 freaks that we are, we decide to visualise our forecast in a different way that gives us more control over the output:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 22", out.width="100%"}

fcast %>%

  as.tibble() %>%

  mutate(ds = date(ds)) %>%

  ggplot(aes(ds, yhat)) + 

  geom_ribbon(aes(x = ds, ymin = yhat_lower, ymax = yhat_upper), fill = "light blue") +

  geom_line(colour = "blue") +

  geom_line(data = pre_views, aes(ds, y), colour = "black") +

  geom_line(data = post_views, aes(ds, y), colour = "grey50")

```



Here we plot the observed data as black line, our hold-out set as a grey line, and the forecast plus uncertainties in blue and light blue, again. This shows us immediately how our model is performing, and in this case it's not doing badly.



We turn this ggplot2 version into a plotting function and use it to predict a couple of sample time series. We also include the seasonality parameter (TRUE/FALSE) as a second input:



```{r}

plot_prophet_rownr_season <- function(rownr, season){

  art <- tpages %>% filter(rowname == rownr) %>% .$article

  loc <- tpages %>% filter(rowname == rownr) %>% .$locale

  acc <- tpages %>% filter(rowname == rownr) %>% .$access

  

  pageviews <- extract_ts(rownr) %>%

    rename(y = views,

         ds = dates)

  pred_len <- 60

  pred_range <- c(nrow(pageviews)-pred_len+1, nrow(pageviews))

  pre_views <- pageviews %>% head(nrow(pageviews)-pred_len)

  post_views <- pageviews %>% tail(pred_len)



  proph <- prophet(pre_views, changepoint.prior.scale=0.5, yearly.seasonality=season)

  future <- make_future_dataframe(proph, periods = pred_len)

  fcast <- predict(proph, future)

  

  p <- fcast %>%

    as.tibble() %>%

    mutate(ds = date(ds)) %>%

    ggplot(aes(ds, yhat)) +

    geom_ribbon(aes(x = ds, ymin = yhat_lower, ymax = yhat_upper), fill = "light blue") +

    geom_line(colour = "blue") +

    geom_line(data = pre_views, aes(ds, y), colour = "black") +

    geom_line(data = post_views, aes(ds, y), colour = "grey50") +

    labs(title = str_c("Prophet for ", art, "-", loc, "-", acc))

  

  return(p)

}  

```



```{r results="hide", fig.align = 'default', warning = FALSE, fig.cap ="Fig. 23", out.width="100%"}

p1 <- plot_prophet_rownr_season(70772, FALSE)

p2 <- plot_prophet_rownr_season(108341, TRUE)

p3 <- plot_prophet_rownr_season(95856, TRUE)

p4 <- plot_prophet_rownr_season(139120, TRUE)



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```





### The importance of seasonal variations



Enabling prophet to recognise long-term seasonal variations in the data is crucial for a successful forecasting of our time series data. To demonstrate this, below I'm plotting the following two sample curves: the German main page and the entry for *Oxygen* in the Spanish wikipedia (many thanks to [MuonNeutrino](https://www.kaggle.com/muonneutrino) for flagging this time series in their great [exploratory kernel](https://www.kaggle.com/muonneutrino/wikipedia-traffic-data-exploration)):



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 24", out.width="100%"}

p1 <- plot_prophet_rownr_season(72480, FALSE)

p2 <- plot_prophet_rownr_season(72480, TRUE)

p3 <- plot_prophet_rownr_season(139120, FALSE)

p4 <- plot_prophet_rownr_season(139120, TRUE)



layout <- matrix(c(1,2,3,4),2,2,byrow=FALSE)

multiplot(p1, p2, p3, p4, layout=layout)

```



The upper row of plot shows forecasts without a seasonal component vs the presence of this component in the lower row. We can clearly see that the seasonal forecasts predict the real time series evolution much better than the others. **A seasonal component should be included in a successful prophet model for this project.**





---



Thanks for reading this exploration! I'm grateful for all the upvotes and the great feedback.



Have fun!