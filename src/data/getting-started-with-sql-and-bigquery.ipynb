{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"# Introduction\n____\n\nSQL is the primary programming language used with databases, and it is an important skill for any data scientist.\n\n[BigQuery](https://cloud.google.com/bigquery/) is a database that lets you use SQL to work with **very** large datasets.\n\nThis series of tutorials and hands-on exercises will teach all the components you need to become effective with SQL and BigQuery.\n\nThis lesson describes basics about connecting to the database and running your first SQL query. Intricacies of SQL will come in the next steps.\n\n---\n\n# Set-Up\n\nThe first step is to start a kernel using one of the BigQuery datasets as the data source. In case you are curious, [here](https://www.kaggle.com/datasets?filetype=bigQuery) is a list of these datasets. \n\nFor now, you'll do an exercise using the \"Chicago Crime\" dataset. **Open [this link](https://www.kaggle.com/kernels/fork/1058477) in a new tab.** . As you read through the instructions on this page, translate each concept to run on the Chicago Crime dataset in the notebook you just opened to a new tab.\n\n# Your First BigQuery Commands\nWe'll use a Python package called `bq_helper`. It has functions for putting BigQuery results in Pandas DataFrames. \n\nYou can import`bq_helper` in your kernel with the command"},{"metadata":{"_cell_guid":"4ae835cb-5a14-4561-9a1e-8d9528cb610e","_uuid":"6bb23747a2eaf9c240d1d68b9d3f3e8c61466845","collapsed":true,"trusted":false},"cell_type":"code","source":"import bq_helper","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12974206-6f92-41d1-96d8-9780de4814d8","_uuid":"5f9a5a1f3e200c66b55071e252468a74e1fa119d"},"cell_type":"markdown","source":"After adding a BigQuery package to our kernel and importing the helper package, create a BigQueryHelper object pointing to a specific dataset. \n\nFind what the dataset is called by checking out the dataset listing for your dataset and then navigating to the \"Data\" tab. For example, [here's a link to the Data tab of the Hacker News dataset](https://www.kaggle.com/hacker-news/hacker-news/data), which is what we'll use in this example.\n\nIf you go to this link, you'll notice a blue rectangle with rounded edges near the top of the page that has the following text in it: \"bigquery-public-data.hacker_news.comments\". This tells you that you're looking of a summary of the \"comments\" table in the \"hacker_news\" dataset. The addresses of BigQuery datasets look like this:\n\n![](https://i.imgur.com/l11gdKx.png)\n\nWe will need to pass this information to BigQueryHelper in order to create our helper object. The active_project argument takes the BigQuery info, which is currently \"bigquery-public-data\" for all the BigQuery datasets on Kaggle. The dataset_name argument takes the name of the dataset we've added to our query. In this case it's \"hacker_news\". So we can create our BigQueryHelper object like so:"},{"metadata":{"_cell_guid":"a0616903-f432-45cb-9439-8052170fa6b6","_uuid":"c0380d78d0e9405e2fa40ed25ba5dcebe1b6b605","collapsed":true,"trusted":false},"cell_type":"code","source":"# create a helper object for our bigquery dataset\nhacker_news = bq_helper.BigQueryHelper(active_project= \"bigquery-public-data\", \n                                       dataset_name = \"hacker_news\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e5da1aa7-0760-40d6-a90e-0e889a6ba5e5","_uuid":"3c12fe9915a0a155a1213eaeb40375120dd5beca"},"cell_type":"markdown","source":"Now that we've created our helper object, we can get started actually interacting with the dataset!"},{"metadata":{"_cell_guid":"a8bb1533-eeaf-4665-b5d9-405ac6d74409","_uuid":"267c124e94b506db81ac7943a5a890b0480ebcd1"},"cell_type":"markdown","source":"---\n## Check Out The Structure of Your Dataset\n\nWe'll start by looking at the schema.\n\n> **Schema**: A description of how data is organized within a dataset.\n\nViewing the schema will be very helpful later on as we start to put together queries. We can use the `BigQueryHelper.list_tables()` method to list all the files in the hacker_news dataset."},{"metadata":{"_cell_guid":"26a24f20-d68b-41fc-ba8c-097b189de051","_uuid":"a0c63422a7f356a12372e8d08b6a46dedd26e7d3"},"cell_type":"markdown","source":"BigQuery datasets can be very large, and there are some restrictions on how much data you can access. \n\n**Each Kaggle user can scan 5TB every 30 days for free.  If you go over your quota you're going to have to wait for it to reset.**\n\nDon't worry, though: we'll teach you how to be careful when looking at BigQuery data to make sure you don't accidentally go over your quota."},{"metadata":{"_cell_guid":"f8877458-1930-4a4b-b6a4-37727132a1f7","_uuid":"11884b60927597dbe28aeed480cfc9f1f0d18750","collapsed":true,"trusted":false},"cell_type":"code","source":"# print a list of all the tables in the hacker_news dataset\nhacker_news.list_tables()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"Now that we know what tables are in this dataset, we can get information on the columns in a specific table. In this example, we're looking at the information on the table called \"full\". Note that other databases will have different table names, so you will not always use \"full.\" "},{"metadata":{"_cell_guid":"794e492c-7774-42be-9dad-8d54a1ff20d3","_uuid":"7c4f1fd91fcb1007ac1ca8b27fa0733d4ef067d1","collapsed":true,"trusted":false},"cell_type":"code","source":"# print information on all the columns in the \"full\" table\n# in the hacker_news dataset\nhacker_news.table_schema(\"full\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b052e663-3450-4177-9e3d-2e195f27f403","_uuid":"b74a023c17b9cfc8867d77629d977d3ac05ade16"},"cell_type":"markdown","source":"Each SchemaField tells us about a specific column. In order, the information is:\n\n* The name of the column\n* The datatype in the column\n* [The mode of the column](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#schema.fields.mode) (NULLABLE means that a column allows NULL values, and is the default)\n* A description of the data in that column\n\nSo, for the first column, we have the following schema field:\n\n`SchemaField('by', 'string', 'NULLABLE', \"The username of the item's author.\",())`\n\nThis tells us that the column is called \"by\", that is has strings in it but that NULL values are allowed, and that it contains the username of the item's author.\n\nWe can use the `BigQueryHelper.head()` method to check just the first couple of lines of of the \"full\" table to make sure this is right. (Sometimes you'll run into databases out there where the schema isn't an accurate description of the data anymore, so it's good to check. This shouldn't be a problem with any of the BigQuery databases on Kaggle, though!)"},{"metadata":{"_cell_guid":"0d433af6-903f-4725-ab10-06c757dab875","_uuid":"d3f69c3544eb06434f546857a4b3109d692266f7","collapsed":true,"trusted":false},"cell_type":"code","source":"# preview the first couple lines of the \"full\" table\nhacker_news.head(\"full\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c6ab4e1b-99ad-4fe8-ae04-a23acf738133","_uuid":"0f4261d9c37e0cc6add817a861a5b5bdd31bac63"},"cell_type":"markdown","source":"The `BigQueryHelper.head()` method will also let us look at just the information in a specific column. If we want to see the first ten entries in the \"by\" column, for example, we can do that!"},{"metadata":{"_cell_guid":"83cd9a08-3f7f-497f-a29d-7b82063534d4","_uuid":"b9b572af74be131a207b94e1ba01ec13514474c0","collapsed":true,"trusted":false},"cell_type":"code","source":"# preview the first ten entries in the by column of the full table\nhacker_news.head(\"full\", selected_columns=\"by\", num_rows=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"52b43784-627d-4891-9949-c420409d88e7","_uuid":"d2efb838faef47e6d081629c3f814d9c92dd380c"},"cell_type":"markdown","source":"Now that we know how to familiarize ourself with our datset, let's see how to check how big our queries are before we actually run them.\n\n--- \n\n## Check the size of your query before you run it\n\n>  Because the datasets on BigQuery can be very large, there are some restrictions on how much data you can access.  Remember that you can scan 5TB every 30 days for free, and after that you'll need to wait until the end of that 30-day period.\n\nThe [biggest dataset currently on Kaggle](https://www.kaggle.com/github/github-repos) is 3 terabytes, so you can easily go past your 30-day quota by running just a couple of queries!\n\n> **What's a query?** A query is small piece of SQL code that specifies what data would you like to scan from a databases, and how much of that data you would like returned. (Note that your quota is on data *scanned*, not the amount of data returned.)\n\nOne way to help avoid this is to estimate how big your query will be before you actually execute it. You can do this with the `BigQueryHelper.estimate_query_size()` method. For the rest of this notebook, I'll be using an example query that finding the scores for every Hacker News post of the type \"job\". Let's see how much data it will scan if we actually ran it."},{"metadata":{"_cell_guid":"6b4fb7c8-42d5-48bc-9c97-973459cb4e72","_uuid":"e3d6cfd9d0eadcc2e40e18e0cafd440d663bee92","collapsed":true,"trusted":false},"cell_type":"code","source":"# this query looks in the full table in the hacker_news\n# dataset, then gets the score column from every row where \n# the type column has \"job\" in it.\nquery = \"\"\"SELECT score\n            FROM `bigquery-public-data.hacker_news.full`\n            WHERE type = \"job\" \"\"\"\n\n# check how big this query will be\nhacker_news.estimate_query_size(query)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"df2d1516-7df0-4e57-a19b-7166c98411c3","_uuid":"202afef03bf6326b1cbdf891e36755457a3be3d1"},"cell_type":"markdown","source":"---\n## Run the Query\n\nNow that we know how to check the size of the query (and make sure we're not scanning several terabytes of data!) we're ready to run our first query. You have two methods available to help you do this:\n\n* *`BigQueryHelper.query_to_pandas(query)`*: This method takes a query and returns a Pandas dataframe.\n* *`BigQueryHelper.query_to_pandas_safe(query, max_gb_scanned=1)`*: This method takes a query and returns a Pandas dataframe only if the size of the query is less than the upperSizeLimit (1 gigabyte by default). \n\nHere's an example of a query that is larger than the specified upper limit."},{"metadata":{"_cell_guid":"5eecbccb-2f2c-495b-b189-5cf069f5ca5d","_uuid":"974f6bb8d4bb1227d60383fb523803b9ca6a61dc","collapsed":true,"trusted":false},"cell_type":"code","source":"# only run this query if it's less than 100 MB\nhacker_news.query_to_pandas_safe(query, max_gb_scanned=0.1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"116a5b85-56d0-4727-8d1a-cd23d79f2d37","_uuid":"e4142226c82063cc8672b4304c7d9124a2c3ad43"},"cell_type":"markdown","source":"And here's an example where the same query returns a dataframe. "},{"metadata":{"_cell_guid":"66c9cbe0-6253-451f-a049-07ccd4baf949","_uuid":"faf64b741a365eeada73ffc5c4016eec3db7eab9","collapsed":true,"trusted":false},"cell_type":"code","source":"# check out the scores of job postings (if the \n# query is smaller than 1 gig)\njob_post_scores = hacker_news.query_to_pandas_safe(query)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9a4fe8cc-b0b0-41c8-8846-879ecb3bdd45","_uuid":"1e896bdff33ffb8cb4514563f8ade9484534b1f4"},"cell_type":"markdown","source":"Since this has returned a dataframe, we can work with it as we would any other dataframe. For example, we can get the mean of the column:"},{"metadata":{"_cell_guid":"a2a02d1e-d98d-45e9-9c71-8792aec8eed8","_uuid":"67f9e2c7b95788a5e9beaec069facf14054ccc54","collapsed":true,"trusted":false},"cell_type":"code","source":"# average score for job posts\njob_post_scores.score.mean()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b2756eaa-60ee-429d-a276-9635697e6fbd","_uuid":"edcecca133f7419abc28a042a961bdc292960f57"},"cell_type":"markdown","source":"# Avoiding Common Mistakes when Querying Big Datasets\n____\n\nBig data is great! But working at a bigger scale makes your problems bigger too, like [this professor whose experiment racked up an unexpected $1000 bill](https://www.wired.com/2012/04/aws-bill-in-minutes/). Kaggle isn't charging for accessing BigQuery datasets, but these best practices can help you avoid trouble down the line.\n\n* *Avoid using the asterisk *(**) in your queries.* The asterisk means “everything”. This may be okay with smaller datasets, but getting everything from a 4 terabyte dataset takes a long time and eats into your monthly usage limit.\n* *For initial exploration, look at just part of the table instead of the whole thing.* If you're just curious to see what data's in a table, preview it instead of scanning the whole table. The `BigQueryHelper.head()` method in our helper package does this. Like `head()` in Pandas or R, it returns just the first few rows for you to look at.\n* *Double-check the size of complex queries.* If you're planning on running what might be a large query, either estimate the size first or run it using the `BigQueryHelper.query_to_pandas_safe()` method.\n* *Be cautious about joining tables.* In particular, avoid joining a table with itself (i.e. a self-join) and try to avoid joins that return a table that's larger than the ones you're joining together. (You can double-check yourself by joining just the heads of the tables.)\n* *Don't rely on LIMIT*: One of the things that can be confusing when working with BigQuery datasets is the difference between the data you *scan* and the data you actually *get back* especially since it's the first one that actually counts against your quota. When you do something like select a column with LIMIT = 10, you'll only get 10 results back, but you'll scan the whole column (and that counts against your monthly usage limit)."},{"metadata":{"_cell_guid":"fad2e4f7-1865-451a-94d4-b9c3ca310ddd","_uuid":"a933480e04c2e3aff1dd82539c8c60304d43293e"},"cell_type":"markdown","source":"# Keep Going\nGet started with your first SQL commands **[here](https://www.kaggle.com/dansbecker/select-from-where).**\n\n\n---\n\n*This tutorial is part of the [SQL Series](https://www.kaggle.com/learn/sql) on Kaggle Learn.*"}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}