{"cells": [{"source": ["# Identifying offensive words.\n", "\n", "This notebook is a simple exploratory analysis on the most common words associated to offensive comments, and a brief comparison between comments that have the word `mother` and `mothjer`."], "cell_type": "markdown", "metadata": {"_uuid": "22b1770fb32ab79bb964489e28fc459d34e8b299", "_cell_guid": "3057cacb-a552-4db1-999f-1604c76cff67"}}, {"source": ["# This R environment comes with all of CRAN preinstalled, as well as many other helpful packages\n", "# The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats\n", "# For example, here's several helpful packages to load in \n", "\n", "library(ggplot2) # Data visualization\n", "library(readr) # CSV file I/O, e.g. the read_csv function\n", "library(tidytext)\n", "library(tidyverse)\n", "library(magrittr)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "train <- read_csv('../input/train.csv')\n", "system(\"ls ../input\")\n", "\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "7afed82867136cee631429ad247c8d3efa874b73", "_cell_guid": "2fbfd72f-816b-4a76-a165-6552ee018bfb"}}, {"source": ["Once the data is loaded let's use `tidytext` to divide each comment into words, removing those words with no actual meaning (like above, and, etc)"], "cell_type": "markdown", "metadata": {"_uuid": "856fbd5b6fd136e6d795a7e9f11988029d6b348c", "_cell_guid": "09d2f506-edf6-4a73-a253-e27c74bb9201"}}, {"source": ["\n", "data(stop_words)\n", "\n", "words <- train %>% \n", "  unnest_tokens(word, comment_text) \n", "\n", "words %<>% anti_join(stop_words, by = 'word')\n"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "63482f351f1d951bcb66a66cc2fbf768a640976f", "_cell_guid": "d0f631da-7d21-456b-8d39-aeed5e4b34cf"}}, {"source": ["Now let's group each word to figure out what is their average score for each category.and remove those words that are not repeated at least 300 times."], "cell_type": "markdown", "metadata": {"_uuid": "09cf0b523a208c00fcb06e628eca13dbf68aea8f", "_cell_guid": "3f712dd7-9452-4311-8a0b-09e5e1ac657d"}}, {"source": ["counting <- words %>%\n", "  group_by(word) %>%\n", "  dplyr::summarise(\n", "    count = n(),\n", "    toxic_avg = mean(toxic),\n", "    severe_toxic = mean(severe_toxic),\n", "    obscene_avg = mean(obscene),\n", "    threat_avg = mean(threat),\n", "    insult_avg = mean(insult),\n", "    identity_hate_avg = mean(identity_hate),\n", "    bad_word_index = sum(toxic_avg, severe_toxic, obscene_avg, threat_avg,\n", "                        insult_avg, identity_hate_avg)  \n", "  ) %>% filter(count>300) %>%  arrange(desc(bad_word_index))\n", "\n"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "cd8d816f3c18110b18a33805e0a78e1f3d17eee5", "_cell_guid": "074fb0d9-2b41-4513-afbb-2d607aa75e8e"}}, {"source": ["Let's get only those words which are the most relevant for the analysis (the most offensive ones). Once we filter them, let's use principal component analysis to find out in which way each word is offensive and plot it using a biplot."], "cell_type": "markdown", "metadata": {"_uuid": "f7d71521e967d4db6085887ca9a9ac903e98eaa3", "_cell_guid": "e262cb73-1b3f-489e-844d-4869bb250571"}}, {"source": ["\n", "to_plot <- counting %>% filter(bad_word_index>=3) %>% data.frame\n", "\n", "rownames(to_plot) <- to_plot$word\n", "\n", "pc_words <- princomp(to_plot[,-c(1,2,9)], cor = T)\n", "\n", "pc_words %>% biplot()\n", "\n", "pc_words_sc <- pc_words$scores %>% data.frame\n", "pc_words_sc$words <- row.names(to_plot)\n"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "897843dc4c71a88dbbf0ff5ebff0cf32ba4e5a26", "_cell_guid": "a9ea2490-7bb5-4e6f-8e8e-fb50be2a5e7b"}}, {"source": ["From the biplot most of the words are organized as expected, with some exceptions, `fat` is associated to **identity hate**, which is surprissing because is the only non-race word on the bottom of the chart,  there are some generic offensive words in the middle of the chart, meaning that they can be used for any awful purposes, other ones as `die` are exclusively associated to **threat** which make total sense some others as  a$$ (sorry I feel uncomfortable writing it as it appear on the data) is associated with **threat**, on the middle left of the chart there are some unrecognizable words, which are shown using the code"], "cell_type": "markdown", "metadata": {"_uuid": "42e3f8d343034ed8caa57a977fe9272f59cd6561", "_cell_guid": "e5267ce9-e60c-4b26-8d7f-8fd484331910"}}, {"source": ["\n", "pc_words_sc %>% filter((Comp.1)<=-1) %>% select(words) "], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "9aa2ab48277442a144bdfed13a322b97ced7f4a1", "_cell_guid": "0ee68ee1-3c45-4e68-a3ad-0e78366145c5"}}, {"source": ["Out of this 4 words, which are heavilt related to toxic and obsene comments,  there are 3 that are misspelled and 1 that is notintrinsically offensive `mothjer`, is funny that it was misspelled exactly the same on different comments, maybe the obsene part is the typo, although I doubt it. \n", "\n", "# Mothjer vs Mother\n", "\n", "Is there actually a difference? Apparently there is, and surprisingly, mother when is misspelled is never related to hate or threat, but when it is properly spelled there are some hate and threat comments that have the word  `mother` in it."], "cell_type": "markdown", "metadata": {"_uuid": "63087bf02a7de1c853f99cddcad93bf43cf91eea", "_cell_guid": "f1423869-f68f-423e-84db-d199aae029e1"}}, {"source": ["counting %>% filter(word%in%c('mother', 'mothjer')) "], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "9ca63d5c328b6651ae5b152f88e8cccbe69cf513", "_cell_guid": "bcba716a-c9dd-4ef1-a14b-80d0bd2f60a1"}}, {"source": ["Is it that people tend to write more carefully when they are threating somebody or when they hate it? \n", "\n", "# Never copy paste typos\n", "\n", "Looking closer at the data I found a simple explanation to all this `mothjer` hypothesis. One particular user, or maybe 3 users, (most likely one) ,decided to type a lovely phrase full of wisdom and a typo in capital letters add an exclamation mark at the end of the sentence and copy-pasted it 304 in 3 different messages. This is how I found it \n", "\n", "\n"], "cell_type": "markdown", "metadata": {"_uuid": "af3787cdad1bd7ad5ddf8890b92d22cb25776224", "_cell_guid": "f1193ab3-6b22-48bd-9f17-d9c81b842c56"}}, {"source": ["Mothjer <- train %>% filter(grepl( 'mothjer', tolower(comment_text)))\n", "Mothjer$comment_text %>% head(1) %>% str_sub(1,300)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "d323bb393691130fa265b9423c4ff5871cfa2cd0", "_kg_hide-output": false, "_cell_guid": "741fcfef-9002-4757-8d6e-8774b4f23a35"}}, {"source": ["This explains why a loot of words were in the same spot on the biplot, why a typo was so many times repeated in the database and why I should check data before publishing a kernel claiming \"oustanding findings\".. \n", "\n", "# Counting words different\n", "\n", "Since there might be hateful comments with plenty of repeated words, let's delete those words that are count twice in the same comment and perform the same analisys to find out which words are associated to each type of toxic message.\n"], "cell_type": "markdown", "metadata": {"_uuid": "3c462563ce04e19dce1018bc7aea7db3bc67a37d", "_cell_guid": "d5e6c440-0829-466e-88ae-e12d90dbdd16"}}, {"source": ["words %<>% unique\n", "\n", "counting <- words %>%\n", "    group_by(word) %>%\n", "    dplyr::summarise(\n", "      count = n(),\n", "      toxic_avg = mean(toxic),\n", "      severe_toxic_avg = mean(severe_toxic),\n", "      obscene_avg = mean(obscene),\n", "      threat_avg = mean(threat),\n", "      insult_avg = mean(insult),\n", "      identity_hate_avg = mean(identity_hate),\n", "      bad_word_index = sum(toxic_avg, severe_toxic_avg, obscene_avg, threat_avg,\n", "                           insult_avg, identity_hate_avg)  \n", "    ) %>% filter(count>300) %>%  arrange(desc(bad_word_index))\n", "\n"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "cf635b46bfc6ad66ec109abda1642f50591362dc", "_cell_guid": "30a7a4c7-b503-4cb9-89ee-defedc111d1c"}}, {"source": ["\n", "to_plot <- counting %>% filter(bad_word_index>=1) %>% data.frame\n", "\n", "rownames(to_plot) <- to_plot$word\n", "\n", "pc_words <- princomp(to_plot[,-c(1,2,9)], cor = T)\n", "\n", "pc_words %>% biplot()\n", "\n", "pc_words_sc <- pc_words$scores %>% data.frame\n", "pc_words_sc$words <- row.names(to_plot)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "7f1556e7afb4a3eb6f60f9f4896bc7c6511767ae", "_cell_guid": "e808aa9e-4a49-43e1-95e1-c830ec76b043"}}, {"source": ["Here there are some new words the ones that can be highlited are **gay** used mainly on threat comments and hate. Some general mild words as **mother, hell, piece, stupid, idiot** and **shut** are used for any toxic general purpose, meantime any  derivative of the **f-word** is used in toxic and obscene comments. Also from the biplot is possible to realize that toxic and insult are similar and the least aggressive ones, while hate and threat are the most serious ones.\n", "\n", "# Conclusions\n", "\n", " DO NOT COPY PASTYE TYPOS! DO NOT COPY PASTYE TYPOS! DO NOT COPY PASTYE TYPOS! DO NOT COPY PASTYE TYPOS! DO NOT COPY PASTYE TYPOS! DO NOT COPY PASTYE TYPOS! DO NOT COPY PASTYE TYPOS! DO NOT COPY PASTYE TYPOS!\n", " \n", " # EXTRA : Offensive phrases with 3 words\n", " \n", " Let's explore the creativity of the toxic comments and find out which are the most toxic phrases with 3 words"], "cell_type": "markdown", "metadata": {"_uuid": "b49a92a4d092b3a92a6281c4529a904b9656aa8b", "_cell_guid": "d247311c-114e-4b6a-a7c5-8f79912445a5"}}, {"source": ["\n", "data(stop_words)\n", "\n", "words <- train %>% \n", "  unnest_tokens(word, comment_text, token = \"ngrams\", n = 3) \n"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "66939ca96bfd07e146d4c345c0c13af7f505d5be", "_cell_guid": "c31f8e26-ad02-4eb7-b6f6-59fcaab7a0ba"}}, {"source": ["words %<>% unique\n", "\n", "counting <- words %>%\n", "    group_by(word) %>%\n", "    dplyr::summarise(\n", "      count = n(),\n", "      toxic_avg = mean(toxic),\n", "      severe_toxic_avg = mean(severe_toxic),\n", "      obscene_avg = mean(obscene),\n", "      threat_avg = mean(threat),\n", "      insult_avg = mean(insult),\n", "      identity_hate_avg = mean(identity_hate),\n", "      bad_word_index = sum(toxic_avg, severe_toxic_avg, obscene_avg, threat_avg,\n", "                           insult_avg, identity_hate_avg)  \n", "    ) %>% filter(count>40) %>%  arrange(desc(bad_word_index))"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "c139fc861e58bee49e4d550a9528a76ea06e5b64", "_cell_guid": "d4fd55ee-cecb-4a9e-9957-4f16e16c1e12"}}, {"source": ["\n", "to_plot <- counting %>% filter(bad_word_index>=2) %>% data.frame\n", "\n", "rownames(to_plot) <- to_plot$word\n", "\n", "pc_words <- princomp(to_plot[,-c(1,2,9)], cor = T)\n", "\n", "pc_words %>% biplot()\n", "\n", "pc_words_sc <- pc_words$scores %>% data.frame\n", "pc_words_sc$words <- row.names(to_plot)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "8b9560d9998638ac426b9a32a7020840e8b5945c", "_cell_guid": "abb1ccfc-cca4-4ea5-b108-062cfd69f846"}}, {"source": ["\n", "\n"], "cell_type": "markdown", "metadata": {"_uuid": "bff5714e31caf5fc20ce54ed90d5b35075b39d27", "_cell_guid": "fef944fc-45cc-4cbb-ac78-f011cbb1dd2b"}}], "nbformat": 4, "metadata": {"language_info": {"pygments_lexer": "r", "name": "R", "mimetype": "text/x-r-source", "file_extension": ".r", "version": "3.4.2", "codemirror_mode": "r"}, "kernelspec": {"name": "ir", "language": "R", "display_name": "R"}}, "nbformat_minor": 1}