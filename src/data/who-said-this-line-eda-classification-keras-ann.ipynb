{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"cell_type":"markdown","source":"# About the dataset\nThis dataset is about the show about \"nothing\". Yeah, you guessed it right. I am talking about Seinfeld, one of the greatest sitcoms of all time. This dataset provides episodic analysis of the series including the entire script and significant amount of information about each episode."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"},"cell_type":"markdown","source":"# What have I done here?\nI have tried to create a classifier to predict the name of the speaker of a given dialogue. I have used the scripts database for this purpose. So, if a line is given to the predictor, it returns the person who said or might say that line."},{"metadata":{"_uuid":"2ccbc52ffbcd2fd3133867b3480042e1cfeef3f1","_cell_guid":"28754255-4350-483b-b1d4-fe30646b8165","trusted":true,"collapsed":true},"cell_type":"code","source":"# Importing libraries and \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# importing data\ndf = pd.read_csv(\"../input/scripts.csv\")\ndel df[\"Unnamed: 0\"]\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15b6c4273a825c57aed3be7d6006f7eee6e38e96","_cell_guid":"166e3523-a279-4d90-b0e8-7075650be300"},"cell_type":"markdown","source":"Now, we don't really require the EpisodeNo, SEID and Season columns so we remove them."},{"metadata":{"_uuid":"4f0e0466b74d9b11af7f8662925d05eeecee9ee3","_cell_guid":"383b05c8-ba69-4f38-8497-71af0964efbb","trusted":true,"collapsed":true},"cell_type":"code","source":"dial_df = df.drop([\"EpisodeNo\",\"SEID\",\"Season\"],axis=1)\ndial_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6e7cc8a74ecb2920ca21a846d9b6c170496f828","_cell_guid":"f3c2a00c-f765-4cdb-896f-85cd5ee1d449"},"cell_type":"markdown","source":"Time for some EDA"},{"metadata":{"_uuid":"3657df405d9acfc0bea43cb1bda8976d4d8d9bd6","_cell_guid":"6ae2e794-a833-42c9-8dfd-8e51d41868fc"},"cell_type":"markdown","source":"# Plot by number of dialogues spoken"},{"metadata":{"_uuid":"376bb4f62022e7f578ead32f41edba1d7f15d7a2","_cell_guid":"5189361f-5927-4353-a6f9-392cf1c0dbce","trusted":true,"collapsed":true},"cell_type":"code","source":"dial_df[\"Character\"].value_counts().head(12).plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f4f2f10792ec3103f40bddd8fa70873fdad237f","_cell_guid":"2880f8eb-bc40-4030-9699-9a2fe2f4b15b"},"cell_type":"markdown","source":"For creating a corpus out of the data, we will create a datframe concatenating all dialogues of a character. We are choosing 12 characters(by number of dialogues) "},{"metadata":{"_uuid":"4651d19e625d35d9c70695c4697d2e7a7a425475","_cell_guid":"0ed6e7e4-152b-4e90-afb5-42cdef5890ea","trusted":true,"collapsed":true},"cell_type":"code","source":"def corpus_creator(name):\n    st = \"\" \n    for i in dial_df[\"Dialogue\"][dial_df[\"Character\"]==name]:\n        st = st + i\n    return st\n\ncorpus_df = pd.DataFrame()\ncorpus_df[\"Character\"] = list(dial_df[\"Character\"].value_counts().head(12).index)\n\nli = []\nfor i in corpus_df[\"Character\"]:\n    li.append(corpus_creator(i))\n\ncorpus_df[\"Dialogues\"] = li\n\ncorpus_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f64aef38d0ecb7a9cd08efd1af024057125dbd7","_cell_guid":"5fd47394-c8ae-40a6-a66d-8678295df557"},"cell_type":"markdown","source":"# Preparing stopwords(words that are obsolete for NLP or words that hinder modelling). Very helpful in human speech but useless when trained for modelling"},{"metadata":{"_uuid":"91468de97026db0e33587ae22c7ca16924acc019","collapsed":true,"_cell_guid":"a4c77bbd-9b70-46f9-8f3a-72486c2b12f6","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction import text\npunc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\nstop_words = text.ENGLISH_STOP_WORDS.union(punc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44503b15dd63ff4aed59bfd3ad1a1ef04fa2d358","_cell_guid":"782583c9-c021-4c65-993a-62d73d47adfb"},"cell_type":"markdown","source":"Now, we create a text_processor function to tokenize concatenated dialogues and removing stop words"},{"metadata":{"_uuid":"2de975e77ab16410b7fc79010ee4b233dbd3048a","collapsed":true,"_cell_guid":"a8c13b22-875b-4f8a-b651-805d14e89f4f","trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\ndef text_processor(dialogue):\n    dialogue = word_tokenize(dialogue)\n    nopunc=[word.lower() for word in dialogue if word not in stop_words]\n    nopunc=' '.join(nopunc)\n    return [word for word in nopunc.split()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d312026289fc6efe7b28e8cbd5cbdc248467e243","_cell_guid":"617a350e-c6fc-4095-aa80-020ebb985b0b"},"cell_type":"markdown","source":"Now, we apply this method"},{"metadata":{"_uuid":"19a8738546383225df356481e9e110f3216e7314","_cell_guid":"2f841939-3b43-4854-add9-f160d7c6c6d5","trusted":true,"collapsed":true},"cell_type":"code","source":"corpus_df[\"Dialogues\"] = corpus_df[\"Dialogues\"].apply(lambda x: text_processor(x))\ncorpus_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffdc641c2d04229e874f0060fccc6f4aa58fb9a9","_cell_guid":"d9c1ed60-0dc5-4580-8b25-cee52f7d5c23"},"cell_type":"markdown","source":"Adding a length column to the new dataframe which contains length of the concatenated dialogues"},{"metadata":{"_uuid":"573951a4b9dd589edf17161dfac666614addcddf","_cell_guid":"b786be4b-c8cf-4401-8c98-9da076a52bcf","trusted":true,"collapsed":true},"cell_type":"code","source":"corpus_df[\"Length\"] = corpus_df[\"Dialogues\"].apply(lambda x: len(x))\ncorpus_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"428f1bcf017d9de10d3ca893ebd1560520b35ed0","_cell_guid":"df839bc8-fea2-46fb-9899-65cf27f3a805"},"cell_type":"markdown","source":"# Who has spoken the most in Seinfeld?"},{"metadata":{"_uuid":"f2db908857487f53d786018361df6199f509d04a","_cell_guid":"e1ddc419-73e7-40c8-ac9d-3637bab04613","trusted":true,"collapsed":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10))\nsns.barplot(ax=ax,y=\"Length\",x=\"Character\",data=corpus_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4086af1cb3512f1e0645051d4e3296b7ac68857","_cell_guid":"a91c213e-c56b-4ca0-9fc5-c2c63080ab06"},"cell_type":"markdown","source":"Obviously, Jerry speaks the most followed by George, Elaine and Kramer"},{"metadata":{"_uuid":"7d05a68aa0c6c9ee4d37243fc14354b838a705e4","_cell_guid":"586297ec-3850-403f-9a06-353c03c4dfb5"},"cell_type":"markdown","source":"Now, we do some **correlation analysis**  to find out how similar are the dialogues of different characters to each other.\n(For this, we will use a library called **gensim**. Next cell is the most important yet.)"},{"metadata":{"_uuid":"139e28190310bb6aef8100a9d9f1678ac7c80468","_cell_guid":"24b59f96-6fef-4e26-a405-f1193d65f536","trusted":true,"collapsed":true},"cell_type":"code","source":"import gensim\n# Creating a dictionary for mapping every word to a number\ndictionary = gensim.corpora.Dictionary(corpus_df[\"Dialogues\"])\nprint(dictionary[567])\nprint(dictionary.token2id['cereal'])\nprint(\"Number of words in dictionary: \",len(dictionary))\n\n# Now, we create a corpus which is a list of bags of words. A bag-of-words representation for a document just lists the number of times each word occurs in the document.\ncorpus = [dictionary.doc2bow(bw) for bw in corpus_df[\"Dialogues\"]]\n\n# Now, we use tf-idf model on our corpus\ntf_idf = gensim.models.TfidfModel(corpus)\n\n# Creating a Similarity objectr\nsims = gensim.similarities.Similarity('',tf_idf[corpus],num_features=len(dictionary))\n\n# Creating a dataframe out of similarities\nsim_list = []\nfor i in range(12):\n    query = dictionary.doc2bow(corpus_df[\"Dialogues\"][i])\n    query_tf_idf = tf_idf[query]\n    sim_list.append(sims[query_tf_idf])\n    \ncorr_df = pd.DataFrame()\nj=0\nfor i in corpus_df[\"Character\"]:\n    corr_df[i] = sim_list[j]\n    j = j + 1   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0f362db76ea587f8d81cc3df2c4eae5a63d63c6","_cell_guid":"4fe45e73-6bda-412a-af45-c1c2f731eaf0"},"cell_type":"markdown","source":"# Heatmap to detect similarity between characters' dialogues"},{"metadata":{"_uuid":"055d4ab85004629afddde377498e706badc2bea4","_cell_guid":"a8c38fc4-528a-4938-a4aa-d6286c02bcf2","trusted":true,"collapsed":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(corr_df,ax=ax,annot=True)\nax.set_yticklabels(corpus_df.Character)\nplt.savefig('similarity.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ff33556d0b420053e0e75d36520ed186ec07ad6","_cell_guid":"3ebb6797-68c6-42e0-b576-4fe23e811514"},"cell_type":"markdown","source":"This plot can actually depict how different the characters are from each other. Like **Jerry, George, Elaine and Kramer** speak highly similar lines. Maybe, that's why they are friends(This might have happened because they are usually talking to each other and also because their dialogues are mumore than others). **[Setting] has lowest correlation scores** well because it is the odd one out because it is not a person. **Jerry and George have highly similar dialogues with 81%  correlation.**"},{"metadata":{"_uuid":"a27b44987df492c693a7dd180c91ba67096443d2","_cell_guid":"f790bf77-307e-4465-9c21-1d4be8bcbb2d"},"cell_type":"markdown","source":"# An awesome way to use classification\nI am predicting the dialogues said by Elaine, George and Kramer only, so we will choose only their dialogues(I wanted to include Jerry, I mean what is Seinfeld without Jerry Seinfeld but I will later tell you why I didn't do that. Look for clues in markdown)"},{"metadata":{"_uuid":"667ebe9e0f395d37e71ed68e1a8802669d229c72","_cell_guid":"f40e592e-20d3-4452-8a31-353df5a5c4af","trusted":true,"collapsed":true},"cell_type":"code","source":"dial_df = dial_df[(dial_df[\"Character\"]==\"ELAINE\") | (dial_df[\"Character\"]==\"GEORGE\") | (dial_df[\"Character\"]==\"KRAMER\")]\ndial_df.head(8)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1044c2a9fd1a066be41cc98d6681246834e1704","_cell_guid":"b03c5487-ade5-43fc-8350-c6f76fdc99b6"},"cell_type":"markdown","source":"Way too many dialogues by george will certainly affect the classifier."},{"metadata":{"_uuid":"847e6ff85251ba7d4d710cdfdc4f8916b3d8507c","_cell_guid":"1f09b641-20c2-4bcf-a82d-5e8a38a84fb1"},"cell_type":"markdown","source":"# A text processor for processing dialogues"},{"metadata":{"_uuid":"36fc6aae8bd75781bb653658efd50d26a0104213","collapsed":true,"_cell_guid":"e61d92d2-59f1-42a5-841c-1d69a8860be1","trusted":true},"cell_type":"code","source":"def text_process(dialogue):\n    nopunc=[word.lower() for word in dialogue if word not in stop_words]\n    nopunc=''.join(nopunc)\n    return [word for word in nopunc.split()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ffc5bab1aaabd4d3753fe6e295d8a5f966d8856","_cell_guid":"648e1aa1-6dfb-4321-8639-3e997b408fac"},"cell_type":"markdown","source":"# Preparation for classifier"},{"metadata":{"_uuid":"591c53a10469f8d7bf64de51fb25bc0ac57c3c46","collapsed":true,"_cell_guid":"8c3878ee-1bc3-497b-b938-cd123893c893","trusted":true},"cell_type":"code","source":"X = dial_df[\"Dialogue\"]\ny = dial_df[\"Character\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e18c46b104169b963ebb45e13aed0ec212e59150","_cell_guid":"eb07c17a-a0f8-4840-aa8a-7fa50a87101f"},"cell_type":"markdown","source":"# TF-IDF Vectorizer\nConvert a collection of raw documents to a matrix of TF-IDF features. Equivalent to CountVectorizer followed by TfidfTransformer.\n\nIn information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf.\n"},{"metadata":{"_uuid":"4229761199b1b72be2dbdae49fd4cffc37612572","collapsed":true,"_cell_guid":"3268ea05-90ba-469d-9303-ee480bd5f58d","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(analyzer=text_process).fit(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb14b74c466d5e6d3b32a764625072c458fb15d7","_cell_guid":"184b27f4-791a-4d6e-a81e-5384891668a7","trusted":true,"collapsed":true},"cell_type":"code","source":"print(len(vectorizer.vocabulary_))\nX = vectorizer.transform(X)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cdfb7bc09c08b089109377042f67e8a541aab03","collapsed":true,"_cell_guid":"1f3d2440-d42b-43e5-8987-8aa1a566ed06","trusted":true},"cell_type":"code","source":"# Splitting the data into train and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b48f3cb33b0218ae2d9588025cad6f33667f859","_cell_guid":"81e21c93-5e1e-4328-baf8-ea963ce0f304"},"cell_type":"markdown","source":"# Creating a voting classifier with Multinomial Naive Bayes, logistic regression and random forest classifier\nThe EnsembleVoteClassifier is a meta-classifier for combining similar or conceptually different machine learning classifiers for classification via majority or plurality voting.\n\nThe EnsembleVoteClassifier implements \"hard\" and \"soft\" voting. In hard voting, we predict the final class label as the class label that has been predicted most frequently by the classification models. In soft voting, we predict the class labels by averaging the class-probabilities (only recommended if the classifiers are well-calibrated)."},{"metadata":{"_uuid":"f1df6906adb501151eb8a683a38695b71b79ce3e","collapsed":true,"_cell_guid":"ea4ddc26-d031-43ad-812d-d4e090241c31","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB as MNB\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.ensemble import VotingClassifier as VC\nmnb = MNB(alpha=10)\nlr = LR(random_state=101)\nrfc = RFC(n_estimators=80, criterion=\"entropy\", random_state=42, n_jobs=-1)\nclf = VC(estimators=[('mnb', mnb), ('lr', lr), ('rfc', rfc)], voting='hard')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe7ac3fda0a4d4bdc5cae5e808ffdfd479a2a397","_cell_guid":"5d5bf29f-a772-482f-8a36-9c5a1d8a9549","trusted":true,"collapsed":true},"cell_type":"code","source":"# Fitting and predicting\nclf.fit(X_train,y_train)\n\npredict = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8349274897c5b1f3fd3481e45af8a31baf41892","_cell_guid":"65f75730-e838-4147-8c28-17c834e60c28","trusted":true,"collapsed":true},"cell_type":"code","source":"# Classification report\nfrom sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, predict))\nprint('\\n')\nprint(classification_report(y_test, predict))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"209491d943f189541df90af6386ce6f1de4b385c","_cell_guid":"5f9cf7a9-5bd9-4214-9e84-6b8a2a509222"},"cell_type":"markdown","source":"So we get about 51% precision which is not bad considering the limited vocablury. George is dominant here due to high recall value of 0.80. So, unless a dialogue has words that don't exist at all in George's vocablury, there is a high chance George will the speaker of most lines. The situation was worse when Jerry was inclued. **This is why I decided to drop Jerry's dialogues from the dataset.** "},{"metadata":{"_uuid":"60a91fd31b86e7a3343726d52271a4dc08f5fd8b","_cell_guid":"adf485ce-bb7a-48d5-a426-77056bcbca5c"},"cell_type":"markdown","source":"# The Predictor"},{"metadata":{"_uuid":"4efe71a54a258eb1c9d9c9f33630fbc5db57c382","collapsed":true,"_cell_guid":"e13dcdf1-1fbb-4984-b18d-2fbb37b5785f","trusted":true},"cell_type":"code","source":"def predictor(s):\n    s = vectorizer.transform(s)\n    pre = clf.predict(s)\n    print(pre)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80d4b8fb28e14cf60a08a6fef2a0b374812bfaf1","_cell_guid":"a0d808b3-e991-41cc-9229-271434209d0d"},"cell_type":"markdown","source":"# Now, we predict..."},{"metadata":{"_uuid":"7a6331ec7c59df06f052d8e9c73c3c3719491431","_cell_guid":"6e6c662e-1153-4b9b-b4cd-56ef4860391c","trusted":true,"collapsed":true},"cell_type":"code","source":"# Answer should be Kramer\npredictor(['I\\'m on the Mexican, whoa oh oh, radio.'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9a85606236f54455da9917a22602bd8b8d0a64e","_cell_guid":"734c2bea-413f-4a39-acd5-2e450b137996","trusted":true,"collapsed":true},"cell_type":"code","source":"# Answer should be Elaine\npredictor(['Do you have any idea how much time I waste in this apartment?'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ece733e2a0843d4e8f8c3d8db97e046c1fdda24e","_cell_guid":"0bd85172-0040-4fbc-add3-8570f6ea3658","trusted":true,"collapsed":true},"cell_type":"code","source":"# Answer should be George \npredictor(['Yeah. I figured since I was lying about my income for a couple of years, I could afford a fake house in the Hamptons.'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ced7d5498b7997e5d9d873b3aaa21a0f1088f3be","_cell_guid":"94bc15e7-d53a-47f0-8c9d-f9a0e5274d59","trusted":true,"collapsed":true},"cell_type":"code","source":"# Now, a random sentence\npredictor(['Jerry, I\\'m gonna go join the circus.'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f91509a0a67746b39469fb77171cb9e49c1273c9","_cell_guid":"3fe969d5-9132-4f41-8eed-5ebfb9a172eb","trusted":true,"collapsed":true},"cell_type":"code","source":"# A random sentence\npredictor(['I wish we can find some way to move past this.'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c8f78b199ec49ce49d1033526195c7ea07ba2da","_cell_guid":"a7c1f7b3-64d7-45ed-95ec-55c5a46846e3","trusted":true,"collapsed":true},"cell_type":"code","source":"# Answer should be Kramer\npredictor(['You’re becoming one of the glitterati.'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"038682dc2832cab441f97887263c1bcdca6233e7","_cell_guid":"0207576a-7c9f-4340-bb38-e7eb94af4b31","trusted":true,"collapsed":true},"cell_type":"code","source":"# Answer should be Elaine\npredictor(['Jerry, we have to have sex to save the friendship.'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"431b4663b535e2a10b963225160803caf7b2a4eb","_cell_guid":"2ea795a5-32b0-44be-b06a-05fdc1a34018"},"cell_type":"markdown","source":"See, this \"george effect\" can lead to awkward results. That's why I am trying to find a way to get rid of that high recall value and also improve the precision of classifier. "},{"metadata":{"_uuid":"ac05daf5264310f3b1e10aebdaf2ec67f4d2c7ea","collapsed":true,"_cell_guid":"e40a2f28-6b8d-4638-a8da-167af416116e"},"cell_type":"markdown","source":"# Time to use keras for this purpose (Simple ANN approach)"},{"metadata":{"_uuid":"96758f7dd39ef943a48374dca2b3838eaefaabf8","_cell_guid":"9f04f88f-f917-4069-ba65-784a3990766b"},"cell_type":"markdown","source":"I have used Keras which is a high-level Neural Networks API built on top of low level neural networks APIs like Tensorflow and Theano. As it is high-level, many things are already taken care of therefore it is easy to work with and a great tool to start with. [Here's the documentation for keras](https://keras.io/).\n\nI have explained the working of keras for beginners in this kernel. [Click here.](https://www.kaggle.com/thebrownviking20/intro-to-keras-with-breast-cancer-data)\n\n**Refer to this kernel to learn in detail about the basic approach for a classification model using keras.** That kernel is based on numerical data and this one is based on textual data so there are bound to be some differences in both approaches.(If anything, this model requires more layers) "},{"metadata":{"_uuid":"b111241d8edab716aa805628ed1fca7ae99e65fc","_cell_guid":"d6f52355-36f7-4c85-a4e4-a83e4729e3ed"},"cell_type":"markdown","source":"First, we import libraries."},{"metadata":{"_uuid":"d1dbc62d7ab5d08e865906c5d9df5f9f2eb30161","_cell_guid":"d28c26b1-fffa-45e6-b8bf-002279019378","trusted":true,"collapsed":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.preprocessing.text import Tokenizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8585dd219e6421df038ee8ca7957972f195ab67d","collapsed":true,"_cell_guid":"95e6d1f5-f6dc-4981-86ed-fead203fd3de","trusted":true},"cell_type":"code","source":"# Encoding categorical data using label encoding and one-hot encoding \nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\ny = labelencoder_X_1.fit_transform(y)\ny = y.reshape(-1,1)\nonehotencoder = OneHotEncoder(categorical_features = [0])\ny = onehotencoder.fit_transform(y).toarray()\n# This would transform the dependent variable into a 3-column matrix, first for Elaine, second for George and third for Kramer ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ac42d9fcd522fba2f6d4b7d9015ddfd67dbbc39","_cell_guid":"2d724e1a-47bb-4183-a45a-e207e7c16c59"},"cell_type":"markdown","source":"# This would delete the 3rd column of the matrix to prevent the dummy variable trap\n[Click here](https://www.quora.com/When-do-I-fall-in-the-dummy-variable-trap) to know more about **Dummy Variable trap** "},{"metadata":{"_uuid":"64aeed0f7888201c5943ac92ab8880f78d04c007","collapsed":true,"_cell_guid":"63cb96c5-21e6-471b-ade4-2251362fcf7e","trusted":true},"cell_type":"code","source":"y = np.delete(y,2,1).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10b060cfc50212f62b8df6cd904f81de811f2295","collapsed":true,"_cell_guid":"1400800a-b70e-4c08-9c6e-fc42d93203cf","trusted":true},"cell_type":"code","source":"# Splitting the data again into train and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9071a574137f73c6a195fd759ac646a44074215","_cell_guid":"288abca0-5109-40f3-b56f-8e746b099cc1"},"cell_type":"markdown","source":"Creating the model."},{"metadata":{"_uuid":"7a0dcdaaa5ebe0e020afbebbb2bd2c0931f46cfe","collapsed":true,"_cell_guid":"1114d1b0-c5db-4e0f-9454-09411346034a","trusted":true},"cell_type":"code","source":"max_words = 14178\nnum_classes = 2\n\nmodel = Sequential()\nmodel.add(Dense(1024, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(512, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(256, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bba4fadc163e1a2281564e7d20455f813be2f9b","_cell_guid":"1c4b044e-7db2-4018-8be7-fdbcbebc6830"},"cell_type":"markdown","source":"Compiling the model."},{"metadata":{"_uuid":"b7bcd107cc068e8a3fd5d50327de1b3f9b19bbf2","collapsed":true,"_cell_guid":"b581f647-1989-44ce-93fe-af8c581a5fca","trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1efe2bb9b4786fe18f1f312dffe4909d8dddba1","_cell_guid":"ab9790a7-3f8a-4368-8427-8b012569af15"},"cell_type":"markdown","source":"Fitting and predicting the model"},{"metadata":{"_uuid":"9f58e3d835f94b61dd13bfdba4c6ae104a9ab1b5","_cell_guid":"d05dbc65-499c-485e-a234-5ef55264ccc2","scrolled":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"model.fit(X_train, y_train, epochs=100, batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa92e88afb97c9a1ff64f4d091ba5cbc09d585f5","collapsed":true,"_cell_guid":"c8cbddb4-90ad-4c5d-b0d1-4fe0263a7511"},"cell_type":"markdown","source":"# So the training set accuracy is 84% which is much better than the Voted Classifier we prepared previously."},{"metadata":{"_uuid":"779e2b55a6bde8791b09f076d412f51779f6c71f","_cell_guid":"3111a1bf-9c87-4d0b-b211-6df3c1f092c7","scrolled":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"model.fit(X_test, y_test, epochs=100, batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33518371b7cdc3d972a81af2cc80731e486202e4","_cell_guid":"df1e0ae6-ac0e-41e5-978e-22113ba356f5"},"cell_type":"markdown","source":"# So the test set accuracy is 85% which is even better than training set."},{"metadata":{"_uuid":"29f2a5475b7a0e386f80bc92ae94f59f3e830574","_cell_guid":"beca8fba-f7d5-4ad9-bc21-5d3c1dd5f5dc"},"cell_type":"markdown","source":"**This was a basic ANN approach which has shown great improvement over a simple voted classifier.**"},{"metadata":{"_uuid":"fb795fa1f5c93617c8c0e7576313a20a5301cc9d","_cell_guid":"e0a8a934-010f-4efb-a2e2-00abd38b17e8"},"cell_type":"markdown","source":"On a side note, this new GPU feature is very helpful. Thanks, Team Kaggle!"}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}