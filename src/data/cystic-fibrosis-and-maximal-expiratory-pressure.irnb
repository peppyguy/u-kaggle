{"cells":[{"metadata":{"_uuid":"b8acbadec7bfbd25f37ea78793079e6531982904"},"cell_type":"markdown","source":"# Introduction\n\nThe kernel will discuss model selection and regularization techniques with a small dataset from a study of 25 patients aged 7-23 years with cystic fibrosis. The data set will be used to determine the relationship between maximal expiratory pressure (pemax), a measure of the strength of the abdominal muscles and other expiratory muscles, and several other variables related largely to body size and lung function in these patients. \n\nThe data will be looked at in an exploratory manner. Then, a model search will be conducted to determine the best model at predicting pemax. For the model search, three shrinkage methods will be used: best subset selection, the lasso, and ridge regression along with two dimension reduction techniques: principal components regression and partial least squares. The results from each will be discussed. Then a final model will be selected that performed well on the dataset and the reasoning behind the choice will be justified. "},{"metadata":{"_uuid":"c1701f9a7ad2ed73a58d70457fd3a438a18d1fef"},"cell_type":"markdown","source":"# Data\n\nThe data contains lung function data for 25 cystic fibrosis patients (7-23 years old). It has been obtained from the ISwR library.\n\nAll the variables except sex are numeric vectors (continous variables). Sex is a discrete/categorical variable. With that being said, the data contains the following 10 variables:\n\n - age: age in years\n\n - sex: 0 for male; 1 for female\n\n - height: height (cm)\n\n - weight: weight (kg)\n\n - bmp: body mass (% of normal)\n\n - fev1: forced expiratory volume\n\n - rv: residual volume\n\n - frc: functional residual capacity\n\n - tlc: total lung capacity\n\n - pemax: maximum expiratory pressure"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"3e2b836101683d42911d05deda2b1b49aa811470"},"cell_type":"code","source":"library(ISwR) # Data source\nlibrary(GGally) # ggpairs plot\nlibrary(ggplot2) # Other plotting\nlibrary(leaps) # regsubsets\nlibrary(glmnet) # Ridge Regression and Lasso\nlibrary(plotmo) # Plotting glmnet\nlibrary(car) # vif among other things\nlibrary(knitr) # kable\nlibrary(htmltools) # Table Header Editing\nlibrary(pls) # PCR and PLS \n\ndata(cystfibr) # load data\ncystfibr$sex = as.factor(cystfibr$sex) # Change sex variable to categorical variable/factor\nlevels(cystfibr$sex) = c(\"M\",\"F\") # Convert 0 to 1 symbology to male and female\n\n# Read in and prep the data\nif (sum(is.na(cystfibr)) >0) { # to remove any rows with missing values\n  cystfibr = na.omit(cystfibr)\n}\n\n# Set global option for significant digits in output\noptions(digits=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f62928447676d66d829e9d8d9bb3cfbb2ff6eab"},"cell_type":"markdown","source":"# Analysis\n\nAs mentioned in the introduction, the cystfibr data will be explored. Then, a model search will be done to determine the relationship between maximal expiratory pressure (pemax) and the other variables in the data set. Three separate shrinkage methods will be used: best subset, ridge regression, and lasso. Then two dimension reduction techniques will be used: principal components regression and partial least squares. Once the model search is complete, a final model will be selected along with the justification for that model.\n\n## Exploratory Data Analysis\n\nLet us take a look at how the variables in the data are related to each other in the following figure. \n***\n<center>  \nFigure 1: First Visual Look at the Dataset\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"106976b309acd3c6805e63f5e080aae14f56c463"},"cell_type":"code","source":"p = ggpairs(cystfibr, aes(alpha=0.2),\n        diag = list(continuous = \"barDiag\"),\n        lower = list(continuous = \"smooth\"))\nsuppressMessages(print(p))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50ad573fcbe5e2176313eb82f2d00129fa4aa5d4"},"cell_type":"markdown","source":"***\nFrom Figure 1, one can see various aspects to the data. In the upper right portion of the figure, there are correlation coefficients along with boxplots for relating to sex (since sex is categorical). Here we can see that height, weight, and age have high correlation coefficients (>0.90) with each other along with along with rv (residual volume) and frc (functional residual capacity) having a high correlation coefficient (>0.90). \n\nLooking at the diagonals with the histograms, most seem to be relatively uniform. Bmp seems to be bimodal or have two peaks of data. \n\nThe correlation can also be seen visually in the bottom left portion of the figure with the scatterplots.\n\nLooking at the boxplots for the only categorical variable in the data set (sex) in Figure 1, there does not seem to be any obvious separation between the sexes for the variables, aside from fev1 (forced expiratory volume) that show two slightly distinct distributions by sex relative to the other variables. One can also see that females seem to have a more condensed distribution of height and weight compared to males. A similar plot is given below, but is now colored according to sex. This allows easier comparison between male and female.\n___\n<center>\nFigure 2: Dataset Relations colored by Sex (blue = female; red = male)\n</center>\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"8cb80dbd05d11cfebb7f1ed6be180fe108bdd026"},"cell_type":"code","source":"p = ggpairs(cystfibr, \n        aes(colour = sex, alpha=0.99), \n        lower = list(continuous = \"smooth\"),\n        upper = list(continuous = wrap(\"cor\", size = 3)))\nsuppressMessages(print(p))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71fe5a9d1754f4e914f73ab3ca49ffa9ccc06fcc"},"cell_type":"markdown","source":"___\nFigure 2 visually allows one to discern differences (and similarities) between male (red) and female (blue). In the bottom left portion with the scatterplots and trendlines, one can see that the two colored trendlines, one for each sex, mostly follow the same trend for each sex for each plot; however, there are some that have differences. For one, the fevl visually shows females (blue) having lower correlation with age, height, weight, bmp, tlc, and pemax than males (red). In fact, for pemax and fevl, females seem to have a slight negative correlation, while males have a noticeable positive correlation.  \n\nSpeaking of pemax, an interesting observation is to look at the pemax row and see how the two different sexes follow different trends for the various data set variables. For tlc, fevl, and bmp, the two sexes have a difference with regard to positive and negative correlation. This can also be seen numerically in the upper portion in the pemax column where one has negative correlation coefficient, while the other has positve correlation coefficient. For bmp and pemax in particular, the difference between the female (F: -0.307) and male (M: 0.424) groups causes the overall bmp correlation to be less in magnitude (Overall: 0.23). \n\nWith the data explored, let's look into how a model can be selected to predict maximal expiratory pressure (pemax) with the other variables in the data set.\n\n## Model Selection\n\nThis section will delve into three different shrinkage methods for model searching and two different dimension reduction methods. The first shrinkage method will be best subset selection. Followed by ridge regression. Then, lasso will be used. For dimension reduction, principal components regression and partial least squares will be used. For all model searches, a k-fold cross-validation (CV) method will be used in some fashion. The goal of this model selection will be to find the model that best predicts maximal expiratory pressure (pemax) with the other variables in the data set. This will be measured via a test mean-squared error.\n\n### Best Subset Selection\n\nTo perform best subset selection, we fit a separate least squares regression for each possible combination of the p predictors. That is, we fit all p models that contain exactly one predictor, all models that contain exactly two predictors, and so forth. We then look at all of the resulting models, with the goal of identifying the one that is the 'best' at determining the relationship between pemax and the other variables. \n\n'Best' in this paper will be based on using (5-fold) cross-validated prediction error. In other words, the model that minimizes the mean squared prediction error over the folds will be the 'best' model. A 'best' model will be chosen for each number of predictors from 1 to 9."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"dcdf0843291b2d3bf19145f1c4ca631f05576107"},"cell_type":"code","source":"# Create predict like function for regsubsets\npredict.regsubsets=function(object,newdata,id,...){\n  form=as.formula(object$call[[2]])\n  mat=model.matrix(form,newdata)\n  coefi=coef(object,id=id)\n  xvars=names(coefi)\n  mat[,xvars]%*%coefi\n} #predict.regsubsets\n\n# Set up Folds for k-fold cross validation\nk=5 # 5 folds since n is small (n-25)  \nset.seed(1)\nfolds=sample(1:k,nrow(cystfibr),replace=TRUE)\nmaxNumPred = ncol(cystfibr)-1\n# Initialize error matrix \ncv.errors=matrix(NA,k,maxNumPred, dimnames=list(NULL, paste(1:maxNumPred))) \n# Perform k-fold cross validation and get errors\nfor(j in 1:k){\n  best.fit=regsubsets(pemax~.,data=cystfibr[folds!=j,],nvmax=maxNumPred+1)\n  for(i in 1:maxNumPred){\n    pred=predict(best.fit,cystfibr[folds==j,],id=i)\n    cv.errors[j,i]=mean( (cystfibr$pemax[folds==j]-pred)^2)\n  }\n}\n# Get mean of errors across k-folds\nmean.cv.errors=apply(cv.errors,2,mean)\n# Find where minimum mean error is\nbestSubsetNumVars = which.min(mean.cv.errors)\nBS.cv.MSE = min(mean.cv.errors)\n# Get best subset coefficients using all the data (based on the number of variables found when minimizing mean.cv.error)\nreg.best = regsubsets(pemax~.,data=cystfibr, nvmax = maxNumPred+1)\nBS.cv.coef = coef(reg.best,bestSubsetNumVars)\n# Save off results for future use and display\nmodelResults = data.frame(as.list(BS.cv.coef))\nmodelResults$'Test MSE' = BS.cv.MSE\nrow.names(modelResults) = 'BestSubset'\n# Move 'Test MSE' to front\nmodelResults = modelResults[,c(which(colnames(modelResults)==\"Test MSE\"),which(colnames(modelResults)!=\"Test MSE\"))]\nmodelResults = t(modelResults)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a8d838504a20cd99ba965f97bec95ef8259754a"},"cell_type":"markdown","source":"In the following figure, we can see how the average mean squared error (MSE) over the different folds changes with the different number of variables in the best subset model.\n___\n<center>\nFigure 3: Best Subset Selection Errors vs Number of Variables\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"496948274b560b37666e78166c305a1dda348e56"},"cell_type":"code","source":"# Plot mean.cv.errors\npar(mfrow=c(1,1))\nplot(mean.cv.errors,type=\"b\", xlab= 'Number of Variables in Model', ylab='MSE over the folds')\nabline(v=bestSubsetNumVars, lty = 2)\ntitle('Best Subset Selection Model Errors vs Number of Variables')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27084b46aa7f6021717d19361593675901c5dac2"},"cell_type":"markdown","source":"___\n\nFrom Figure 3, one can see that the best subset model with 2 variables had the lowest average/mean squared error (MSE) over the folds during k-fold CV of around 800. The table below gives more details on what values the lowest or minimum MSE along with what two variables and their values are in the model.\n___\n<center>\nTable 1: Results for Best Subset Selection\n</center>"},{"metadata":{"trusted":true,"_uuid":"3bc176bdbdae4d2f10e6d66759c0a5c727a02ece"},"cell_type":"code","source":"# display results in table\nkable(modelResults)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ad8b449fced4de8144643afbbf3db99e929c4f3"},"cell_type":"markdown","source":"___\n\n\nFrom the Table 1, one can see the minimum MSE was 810, which aligns to where the minimum point is in Figure 3. Table 1 also shows the two variables chosen, weight and bmp, along with theirs values, 1.640 and -1.005 respectively. \n\n### Ridge Regression\n\nFrom reference [1], ridge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. In particular, the ridge regression coefficient estimates, $\\hat{\\beta}^R_\\lambda$, are the values that minimize:\n\n$$\\sum^n_{i=1} (y_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij} )^2 +\\lambda\\sum^p_{j=1}\\beta_j^2 = RSS + \\lambda\\sum^p_{j=1}\\beta_j^2$$\nwhere RSS is root sum squared and $\\lambda\\ge0$ is a tuning paramter, to be determined separately with k-fold cross validation. The 'best' model will have a $\\lambda$ that will minimize the mean cross-validated error across the k-folds.\n\nThe data will be split into a training set and testing set. The training set will be used to create a model and use 5-fold CV to determine the 'best' lambda. The 'best' lambda will be the one with the lowest average MSE. Once that is determined, the lambda value and model based on the training value will be used to predict values for the test data set to get a test mean square error. The lambda value will also be used to fit a model on all the data to determine the coefficients for said lambda.\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"27e859211ccabb1275fd00fb396df1e3b24d2350","_kg_hide-output":true},"cell_type":"code","source":"# Create our x matrix\nx = model.matrix(pemax~.,cystfibr)[,-1]\ny = cystfibr$pemax # Create our response vector (y)\n\nset.seed(8)\n# Get Training and Testing Data Set\ntrain = sample(1:nrow(x), 3*nrow(x)/5) # Use about 15 observations for training\ntest = -(train)\ny.test = y[test]\n\n# Fit ridge regression on the training set\ngrid=10^seq(10, -2, length=100)\nridge.mod = glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)\n\n# Perform k-fold cross validation on all the data to determine best lambda value\nridge.cv.out = cv.glmnet(x[train,], y[train], alpha=0, nfolds = 5) # 5-fold cv since small n\nridge.bestlam=ridge.cv.out$lambda.min\n\n# MSE associated with value of lambda that results in smallest cv-error\nridge.pred = predict(ridge.mod, s=ridge.bestlam, newx=x[test,])\nridge.MSE = mean((ridge.pred - y.test)^2)\n\n# Now refit ridge regression on full data set, using the value of lambda\n# selected by cross-validation\nridge.modF = glmnet(x, y, alpha=0)\nridge.coef=predict(ridge.modF,type=\"coefficients\",s=ridge.bestlam)[1:maxNumPred,]\n# Save off Results\nridgeModelResults = data.frame(as.list(ridge.coef))\nridgeModelResults$'Test MSE' = ridge.MSE\nridgeModelResults$BestLambda = ridge.bestlam\nrow.names(ridgeModelResults) = 'RidgeRegression'\n# Rearrange variables \nridgeModelResults = ridgeModelResults[,c(which(colnames(ridgeModelResults)==\"BestLambda\"), which(colnames(ridgeModelResults)!=\"BestLambda\"))]\nridgeModelResults = ridgeModelResults[,c(which(colnames(ridgeModelResults)==\"Test MSE\"),which(colnames(ridgeModelResults)!=\"Test MSE\"))]\nridgeModelResults = t(ridgeModelResults) # transpose for easier table display","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb1e8e980628a36d7204795abf0eaa68284ca051"},"cell_type":"markdown","source":"In the following figure, one can see how the error changes as lambda is changed for the ridge regression model (that is based on the training data).\n___\n<center>\nFigure 4: Ridge Regression Model Errors vs Lamda</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"479351a1e51f54ee99104680821a6dbaa48323ff"},"cell_type":"code","source":"plot(ridge.cv.out)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cfc54a081e751c6962c7927bf491be3e57d45a2"},"cell_type":"markdown","source":"___\n\nFrom the figure, one can see the lowest MSE occurs at a log(Lambda) value slightly more than 2 and has a MSE value around 600. The figure also indicates this model has all 9 variables in it from the x axis on top of the plot. In fact, ridge regression always includes all 9 variables for the lambda values shown. Remember, this figure is based on the model produced by the training set.\n\nHow the variable coefficient values change with lambda is given below in a visual manner.\n___\n<center>\nFigure 5: Ridge Regression Model Coefficients vs Lamda\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"54243e7661a930b57f6b28bb7be274de4613e0b4"},"cell_type":"code","source":"plot_glmnet(ridge.modF)\nabline(v=log(ridge.bestlam), lty = 2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2658b91cdc2877687edd7eb5c7dbda7a765caef5"},"cell_type":"markdown","source":"___\nFrom the figure, the dotted dash vertical line indicates the 'best' lambda value that results in the lowest training MSE. This figure is based on all the data (that includes both the training and testing set). One can see that the sex coefficient has the largest magnitude in the 'best' model selected. This is also shown in the table below that also displays the actual values along with the best lambda and test MSE values. \n___\n<center>\nTable 2: Results for Ridge Regression\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"48f8b8d58686f3b7601e97b90126c0ec36508792"},"cell_type":"code","source":"# display results in table\nkable(ridgeModelResults)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca768c0e6368f86e312ec03c19c99dcc63e8d3fa"},"cell_type":"markdown","source":"___\n\nFrom the Table 2, one can see the minimum MSE was 755, which aligns to about where the minimum point can visually be seen in Figure 4. Table 2 also shows that all variables were involed in the model. As seen in Figure 3, sex and age have the largest coefficient value magnitudes. \n\n### Lasso Model\n\nOne downside to ridge regression is that it will usually generate a model involving all the predictors even if some are irrelevant. It can create a challenge in model interpretation. The lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficient estimates, $\\hat{\\beta}^L_\\lambda$, are the values that minimize:\n\n$$\\sum^n_{i=1} (y_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij} )^2 +\\lambda\\sum^p_{j=1}|{\\beta_j}| = RSS + \\lambda\\sum^p_{j=1}|\\beta_j|$$\nSimiliar to ridge regression, $\\lambda\\ge0$ is a tuning paramter that will be determined separately with k-fold cross validation. The 'best' model will have a $\\lambda$ that will minimize the mean cross-validated error across the k-folds.\n\nAs with ridge regression, the data will be split into the same training set and testing set to provide a consistent data split and easier comparison. The training set will be used to create a model and use 5-fold CV to determine the 'best' lambda. The 'best' lambda will be the one with the lowest average MSE. Once that is determined, the lambda value and model based on the training value will be used to predict values for the test data set and get a test mean square error. The lambda value will also be used to fit a model on all the data to determine the coefficients for said lambda.\n"},{"metadata":{"trusted":true,"_uuid":"8185bf9251e916725f65160134a8184c4b7d72c5","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"set.seed(8)\n# Fit lasso on the training set (same one used as ridge)\ngrid=10^seq(10, -2, length=100) # using same grid as before (optional: could use new grid)\nlasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid, thresh=1e-12)\n\n# Create lasso model, perform k-fold CV, and get best lambda and lowest MSE\nlasso.cv.out= cv.glmnet(x[train,], y[train], alpha=1, nfolds = 5) # 5-fold cv since small n\nlasso.bestlam=lasso.cv.out$lambda.min\n\nlasso.pred = predict(lasso.mod, s=lasso.bestlam, newx=x[test,])\nlasso.MSE = mean((lasso.pred - y.test)^2) # test set MSE\n\n# Now refit lasso on full data set, using the value of lambda\n# selected by cross-validation\nlasso.modF = glmnet(x, y, alpha=1)\nlasso.coef = predict(lasso.modF, type=\"coefficients\", s=lasso.bestlam)[1:maxNumPred,]\n\n# Get coefficients and save off results\nlasso.coef = data.frame(as.list(lasso.coef))\nlassoModelResults = data.frame(as.list(lasso.coef))\nlassoModelResults$'Test MSE' = lasso.MSE\nlassoModelResults$BestLambda = lasso.bestlam\nrow.names(lassoModelResults) = 'Lasso'\n# Rearrange variables\nlassoModelResults = lassoModelResults[,c(which(colnames(lassoModelResults)==\"BestLambda\"), which(colnames(lassoModelResults)!=\"BestLambda\"))]\nlassoModelResults = lassoModelResults[,c(which(colnames(lassoModelResults)==\"Test MSE\"),which(colnames(lassoModelResults)!=\"Test MSE\"))]\nlassoModelResults = t(lassoModelResults) # transpose for easier table display","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72844ae8d33db7041a1b6664c9e639405042b8ba"},"cell_type":"markdown","source":"In the following figure, one can see how the error changes as lambda is changed for the lasso model (based on the training data).\n___\n<center>\nFigure 6: Lasso Model Errors vs Lamda</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c250522685062aa852d835f14629c99c028a9bec"},"cell_type":"code","source":"plot(lasso.cv.out)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8402aa9f6f84afac2811e9f5cf52ad0a5ae21762"},"cell_type":"markdown","source":"___\n\nFrom the figure, one can see the lowest MSE occurs at a log(Lambda) value near 0.5 and has a MSE value around 1000. It also indicates this model has around 6 variables in it from the x axis on top of the plot. Remember, this figure is based on the model produced by the training set. \n\nHow these variable coefficient values change with lambda (along with the number of variable coefficients) is given below in a visual manner.\n___\n<center>\nFigure 7: Lasso Model Coefficients vs Lamda\n</center>"},{"metadata":{"trusted":true,"_uuid":"b774c0201cc17159bc1598055e50496e129c01b3"},"cell_type":"code","source":"plot_glmnet(lasso.modF)\nabline(v=log(lasso.bestlam), lty = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2678dfe71cc6277e8d0ab2b86a219da1a876fc3c"},"cell_type":"markdown","source":"___\n\nFrom the figure, the dotted dash vertical line indicates the 'best' lambda value that results in the lowest MSE. This figure is based on all the data (that includes both the training and testing set). One can see that weight, bmp, and fevl coefficients have the largest magnitudes in the 'best' model selected. This is also shown in the table below that also displays the actual values along with the best lambda and test MSE values.\n___\n<center>\nTable 3: Results for Lasso\n</center>\n"},{"metadata":{"trusted":true,"_uuid":"0822e53a3dcf3b86a65fe1906d7e760662c2bfdc","_kg_hide-input":true},"cell_type":"code","source":"# display results in table\nkable(lassoModelResults)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20ee21b555a327d0e0deb6fb7bce754c0649e885"},"cell_type":"markdown","source":"___\n\nFrom the Table 3, one can see the minimum MSE was 821. Table 3 also shows which variables were involed in the model (weight, bmp, fevl, rv, and tlc) and their associated values. As seen in Figure 3, bmp, fevl, and weight have the largest coefficient value magnitudes. \n\n### Principle Components Regression (PCR)\n\nThe principal components regression (PCR) approach involves constructing the first M principal components, $Z_1,…,Z_M$ , and then using these components as the predictors in a linear regression model that is fit using least squares. The key idea is that often a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response. In other words, we assume that the directions in which $X_1,…,X_p$ show the most variation are the directions that are associated with Y. \n\nFor more details on PCR, please see reference [1].\n\nIn this paper, we fit a PCR model to the training set and use 5-fold CV to determine the best number of components to use (the number with the lowest error). Once that is determined, we predict values based on our test data set using the best number of components determined with the training data. With the original values and predicted values, the test MSE can be calculated. Afterwards, we use the full dataset to fit a model using that best number of components to get the resulting coefficients.\n\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f4e6f3bad8e9393e754fd1c619597e791fa8611a"},"cell_type":"code","source":"# Fit PCR model on the training set (same one used earlier) and Perform k-fold CV\nset.seed(8)\n# use only 5 folds since number of observations is small\npcr.fit = pcr(pemax~., data=cystfibr, subset=train, scale=TRUE, validation=\"CV\", segments = 5)\n\n# Get best number of components from lowest MSE (based on training data)\n# Get MSEP and where minimum MSEP is for best number of components\nMSEP_object = MSEP(pcr.fit)\npcrBestNumOfComponents = which.min(MSEP_object$val[1,1,])-1 # -1 to exclude intercept in number of components\npcr.pred=predict(pcr.fit, x[test,], ncomp=pcrBestNumOfComponents)\npcr.test.mse = mean((pcr.pred - y[test])^2) # Save off Test MSE\n\n# Now refit model on full data set, using the value of number of components\npcr.fitF = pcr(pemax~., data=cystfibr, scale=TRUE,ncomp=pcrBestNumOfComponents)\n\n# Get coefficients and save off results\npcrModelResults = data.frame(coef(pcr.fitF))\npcrModelResults['Test MSE',] = pcr.test.mse\npcrModelResults['BestNumOfComponents',] = pcrBestNumOfComponents\n# Change Row order\nrownameOrder = rownames(pcrModelResults) # To get row names, last two are Test MSE and BestNumOfCompoennts\nrownameOrder = c(tail(rownameOrder,2) , head(rownameOrder,-2))\npcrModelResults = t(t(pcrModelResults[rownameOrder,]))\nrownames(pcrModelResults) = rownameOrder\ncolnames(pcrModelResults) = paste(names(pcrBestNumOfComponents),\"PCR\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8a6381b735e6fbe0fe2e46467bdae5aaf4fe5ce"},"cell_type":"markdown","source":"In the following figure, one can see how the error changes as the number of components is changed for the PCR model (based on the training data).\n___\n<center>\nFigure 8: PCR Error vs Number of Components\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f32e9ade5622644057b4790bdbd85a2d4a0ab4a2"},"cell_type":"code","source":"validationplot(pcr.fit, val.type=\"MSEP\") # Visually see MSEP","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d144b811f2d3ac0362fdef1cb9144c2464110fac"},"cell_type":"markdown","source":"___\nFrom the figure, one can see the lowest cross validation MSE occurs at 4 components. The MSEP value is around 1000. Remember, this figure is based on the model produced by the training set.\n\nWe can now refit the model using 4 components and all the data to get the coefficients as mentioned earlier. The table below shows the results.\n___\n<center>\nTable 4: Results for PCR\n</center>"},{"metadata":{"trusted":true,"_uuid":"6789db53db721cb63efee5e97b2793f1fd5e989f"},"cell_type":"code","source":"# display results in table\nkable(pcrModelResults)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c26adc0aeadeadf90f916facd6b8f901fa616350"},"cell_type":"markdown","source":"\nFrom the Table 4, one can see the test MSE was 1070. Table 4 also shows the variables and their values. As seen in Figure 8, the best number of components was determined to be 4. \n\n### Partial Least Squares (PLS)\n\nLike PCR, PLS is a dimension reduction method, which first identifies a new set of features $Z_1$,…,$Z_M$ that are linear combinations of the original features, and then fits a linear model via least squares using these M new features. But unlike PCR, PLS identifies these new features in a supervised way—that is, it makes use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response. Roughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors.\n\nFor more details on PLS, please see reference [1].\n\nWe will now follow a similar procedure that was performed with PCR. That is, we fit a PLS model to the training set and use 5-fold CV to determine the best number of components to use (the number with the lowest error). Once that is determined, we predict values based on our test data set using the best number of components determined with the training data. With the original values and predicted values, the test MSE can be calculated. Afterwards, we use the full dataset to fit a model using that best number of components to get the resulting coefficients."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"02f98eaf9840e680e76d629e6f03d6c985a34c72"},"cell_type":"code","source":"# Fit PLS model on the training set (same one used earlier) and Perform k-fold CV\nset.seed(8)\n# use only 5 folds since number of observations is small\npls.fit=plsr(pemax~., data=cystfibr,subset=train,scale=TRUE, validation=\"CV\", segments = 5)\n# Get best number of components from lowest MSE (based on training data)\n# Get MSEP and where minimum MSEP is for best number of components\nplsMSEP_object = MSEP(pls.fit)\nplsBestNumOfComponents = which.min(plsMSEP_object$val[1,1,])-1 # -1 to exclude intercept in number of components\npls.pred=predict(pls.fit, x[test,], ncomp=plsBestNumOfComponents)\npls.test.mse = mean((pls.pred-y[test])^2) # Save off Test MSE\n# Now refit model on full data set, using the value of the best number of components\npls.fitF=plsr(pemax~., data=cystfibr,scale=TRUE,ncomp=plsBestNumOfComponents)\n# Get coefficients and save off results\nplsModelResults = data.frame(coef(pls.fitF))\nplsModelResults['Test MSE',] = pls.test.mse\nplsModelResults['BestNumOfComponents',] = plsBestNumOfComponents\n# Change Row order\nrownameOrder = rownames(plsModelResults) # To get row names, last two are Test MSE and BestNumOfCompoennts\nrownameOrder = c(tail(rownameOrder,2) , head(rownameOrder,-2))\nplsModelResults = t(t(plsModelResults[rownameOrder,]))\nrownames(plsModelResults) = rownameOrder\ncolnames(plsModelResults) = paste(names(plsBestNumOfComponents),\"PLS\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4657c24e30308f69e230d190ae5d245b20162b39"},"cell_type":"markdown","source":"In the following figure, one can see how the error changes as the number of components is changed for the PLS model (based on the training data).\n___\n<center>Figure 9: PLS Error vs Number of Components</center>"},{"metadata":{"trusted":true,"_uuid":"135272af408cb70f64f7addaf0f42003387aaf05"},"cell_type":"code","source":"validationplot(pls.fit,val.type=\"MSEP\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95ce08860e60df92d051526bfe65da10b578d32b"},"cell_type":"markdown","source":"___\nFrom the figure, one can see the lowest cross validation MSE occurs at 2 components. The MSE value near 1000. Remember, this figure is based on the model produced by the training set.\n\nWe can now refit the model using 2 components and all the data to get the coefficients as mentioned earlier. The table below shows the results.\n___\n<center>\nTable 5: Results for PLS\n</center>"},{"metadata":{"trusted":true,"_uuid":"67c473f97a53313b76abfbc705583c8a46cd1fe1"},"cell_type":"code","source":"# display results in table\nkable(plsModelResults)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc358c356e1e9489dab6d2dc21193b4d9e51f009"},"cell_type":"markdown","source":"___\nFrom the Table 5, one can see the test MSE was 990.044. Table 5 also shows the variables and their values. As seen in Figure 9, the best number of components was determined to be 2."},{"metadata":{"_uuid":"d1f3d8b86e5b7ead7ba990ce512f7093981acd14"},"cell_type":"markdown","source":"\n## Model Comparison and Final Model\n\nOut of all the models, ridge regression had the ‘best’ model since it produced the model with the lowest test MSE out of all the model search methods. This can easily be seen in the Test MSE row in the table below comparing the model selected from each method.\n____\n<center>\nTable 6: Model Comparison</center>"},{"metadata":{"trusted":true,"_uuid":"ee8419756d2fe06bc6d443a59d915ab3bc9020da","_kg_hide-input":true},"cell_type":"code","source":"# Merge/Combine Results\nrownameOrder = rownames(ridgeModelResults) # To save order of row names\nrownameOrder = c(head(rownameOrder,1),\"BestNumOfComponents\",tail(rownameOrder,-1))\ncomparisonResults = merge(cbind(ridgeModelResults, lassoModelResults),modelResults, by = 0, all = TRUE)\ncurrRowNames = comparisonResults$Row.names\ncomparisonResults = comparisonResults[-1] #Remove Row.Names column\nrownames(comparisonResults) = currRowNames # Add real row names\ncomparisonResults = merge( comparisonResults, cbind(plsModelResults, pcrModelResults), by = 0, all = TRUE)\ncurrRowNames = comparisonResults$Row.names\ncomparisonResults = comparisonResults[-1] #Remove Row.Names column\nrownames(comparisonResults) = currRowNames # Add real row names\ncomparisonResults=comparisonResults[rownameOrder,] # Change order of rows\ncomparisonResults[comparisonResults == 0] = NA # Change 0 to NA (shows up blank in table)# display results in table\nkable(comparisonResults)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fed1b9c2e9d4e2b4039491daed59cb99086defb"},"cell_type":"markdown","source":"___\n\nAs indicated by Table 6, ridge regression has the lowest test MSE of 754.65. Lasso has a MSE of 821.521 and Best Subset has a MSE of 809.717. For the dimension reduction techniques, PLS has a test MSE of 990.044 and PCR has a test MSE of 1069.965.\n\nRidge regression performed better than the lasso selection since ridge regression is able to ‘fine’ tune the parameters/predictors better than the lasso method that fixes certain coefficients to be zeroes for certain lambdas. The coefficients using ridge regression are generally not zero, so most, if not all, the predictors/variables are included in a ridge regression model.\n\nRidge regression performed better than best subset selection because best subset selection uses a least squares method along with selecting certain variables/variables over others (like lasso). As stated in reference [1], ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.\n\nRidge regression performed better than the dimension reduction methods, PCR and PLS, because it shrinks the coefficients of the principal components (unlike PCR and PLS that use transformations).\n\nThe selected ridge regression model does invoke all the paramters to varying levels. Ridge regression takes into account all the predictors and modifies their coefficients according to lamda, which was determined using k-fold CV. This is due to the value that ridge regression minimizes. Specifically, the $\\lambda\\sum^p_{j=1}\\beta_j^2$ term.\n\nIt should be noted that the ridge regression model, along with PCR and PLS, is harder to interpret since it includes all the variables in the model. Lasso and best subset are easier to interpret since they include less variables.\n\nIn addition, the small sample size of 25 patients is not an ideal situation and can affect the results shown here. With such a small sample size,  how you choose your training/validation/test datasets can have a larger impact on the resulting model.\n\n# Conclusion\n\nThe data set of 25 patients aged 7-23 years with cystic fibrosis was used to determine the relationship between maximal expiratory pressure (pemax), a measure of the strength of the abdominal muscles and other expiratory muscles, and several other variables related largely to body size and lung function in these patients.\n\nInitially, the data was explored with a matrix of various types of plots (scatter, box, density, correlation values, etc.). There were some interesting observations. There were also some observations with regard to how some of the variables had different trends for different sexes.\n\nOnce the exploratory analysis was complete, a model search was done with three shrinkage methods and two dimension reduction methods. The methods were best subset selection, the lasso, ridge regression, principal components regression, and partial least squares. These methods were used to search for the ‘best’ model to predict pemax with the other variables. The ‘best’ model in this paper was the one that produced the lowest test mean-squared error.\n\nAfter looking at each method’s best model, ridge regression produced the ‘best’ model to predict pemax. Ridge regression’s ‘best’ model had the lowest test mean-squared error and included all the other variables. Since it included all the variables, like PCR and PLS, it was a harder model to interpret. Lasso’s ‘best’ model and best subset ‘best’ model had less than all the predictors. This means the lasso and best subset ‘best’ models were easier to interpret than the ridge regression model; however, as was just mentioned, ridge regression produced the ‘best’ model in this paper with the lowest test mean-squared error.\n\nIn addition, the small sample size of 25 patients means how you choose your training/validation/test datasets can have a larger impact on the resulting model.\n\nAnyway, I hope you enjoyed going through my Kernel. Feel free to leave a comment/feedback or upvote.\n\n# References and Sources\n\n[1] James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning with applications in R, www.StatLearning.com, Springer-Verlag, New York\n\n[2] Dalgaard, Peter (2008) Introductory Statistics with R Edition 2, Springer-Verlag, New York"}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}