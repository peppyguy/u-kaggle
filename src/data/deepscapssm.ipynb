{"cells":[{"metadata":{"_uuid":"8757092062e2d995f3ba96d74e87c0e222d36b19"},"cell_type":"markdown","source":"**Scapulae features predictions with SSM parameters**\n\n*In this kernel we are going to try to predict anatomical features of the scapulae from the projections on the 10 first principal compornents of the Statistical Shape Model.  *"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport statsmodels.api as sm\nfrom sklearn import linear_model\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\nimport plotly.tools as tls\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\nimport warnings\n\n%matplotlib inline\n\n# figure size in inches\nrcParams['figure.figsize'] = 12,6\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91cd08a6e087317743a4fb58f7fb92a08d5e5fc8"},"cell_type":"markdown","source":"*We are going to first delete the ID column from our database. \nLet's check how the database looks like by loading the three first rows.*"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_features = pd.read_csv(\"../input/scapFeaturesData.csv\", encoding='ISO-8859-1' )\n#We delete the MeshID feature from our dataset\ndel df_features['MeshID']\ndf_features.head(n=3).transpose()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fe6b38dc093bbbc43769c0e4049964fd4b67cec"},"cell_type":"markdown","source":"**1. Some visualization of the features distributions**\n\nWe can notice  that the features seem to follow a gaussian distribution, intuitively it could let us think that a simple linear regression would be enough to obtain the predictions knowing that the parameters of the SSM also follow a gaussian distribution."},{"metadata":{"trusted":true,"_uuid":"4b0c6ae3f983533b218db44e409a94ac3b8b3d99"},"cell_type":"code","source":"trace1 = go.Histogram(\n    x=np.log(df_features['CSA']).sample(800), histnorm='percent', autobinx=True,\n    showlegend=True, name='CSA')\n    \ntrace2 = go.Histogram(\n    x=np.log(df_features['Version']).sample(800), histnorm='percent', autobinx=True,\n    showlegend=True, name='Version')\n\ntrace3 = go.Histogram(\n    x=np.log(df_features['Tilt']).sample(800), histnorm='percent', autobinx=True,\n    showlegend=True, name='Tilt')\n    \ntrace4 = go.Histogram(\n    x=np.log(df_features['Glene Width']).sample(800), histnorm='percent', autobinx=True,\n    showlegend=True, name='Glene Width')\n    \ntrace5 = go.Histogram(\n    x=np.log(df_features['Glene Length']).sample(800), histnorm='percent', autobinx=True,\n    showlegend=True, name='Glene Length')\n\n#Creating the grid\nfig = tls.make_subplots(rows=2, cols=3, specs=[[{'colspan': 2}, None, {}], [{}, {}, {}]],\n                          subplot_titles=(\"CSA\",\n                                          \"Version\", \n                                          \"Tilt\",\n                                          \"Glene Width\", \n                                          \"Glene Length\"))\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 3)\nfig.append_trace(trace3, 2, 1)\nfig.append_trace(trace4, 2, 2)\nfig.append_trace(trace5, 2, 3)\n\nfig['layout'].update(showlegend=True, title=\"Features Distribution\")\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c35ff3c89a6d7567d8484df30b00580ead087833"},"cell_type":"markdown","source":"**2. Baseline Model with sklearn linear regression**"},{"metadata":{"trusted":true,"_uuid":"dd2b740c4571d0b98b0fa5465005d2732d48700f"},"cell_type":"code","source":"\nprint('The standard deviation and the mean for the CSA are %(stdCS)f and %(meanCS)f .' %{'stdCS':df_features[\"CSA\"].std() , \"meanCS\": df_features[\"CSA\"].mean()})\nprint('The standard deviation and the mean for the version are %(stdV)f and %(meanV)f .' %{'stdV':df_features[\"Version\"].std() , \"meanV\": df_features[\"Version\"].mean()})\nprint('The standard deviation and the mean for the tilt are %(stdT)f and %(meanT)f .' %{'stdT':df_features[\"Tilt\"].std() , \"meanT\": df_features[\"Tilt\"].mean()})\nprint('The standard deviation and the mean for the glene width are %(stdW)f and %(meanW)f .' %{'stdW':df_features[\"Glene Width\"].std() , \"meanW\": df_features[\"Glene Width\"].mean()})\nprint('The standard deviation and the mean for the glene length are %(stdL)f and %(meanL)f .' %{'stdL':df_features[\"Glene Length\"].std() , \"meanL\": df_features[\"Glene Length\"].mean()})\n# We define the targets\ntarget = pd.DataFrame(df_features, columns=[\"CSA\",\"Version\",\"Tilt\",\"Glene Width\",\"Glene Length\"])\n\n# We define the predictors\ndf = pd.DataFrame(df_features, columns=[\"First PC\",\"Second PC\",\"Third PC\",\"Fourth PC\",\"Fifth PC\",\"Sixth PC\",\"Seventh PC\",\"Ninth PC\",\"Tenth PC\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e9779f01e117ab96b65f0b9f96225d56d40f8f3"},"cell_type":"code","source":"X = df\ny = target\n\n# I now fit a model\nlm = linear_model.LinearRegression()\nmodel = lm.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e90b2fff32c1b217fd3928acd3cf31fb253cbffd"},"cell_type":"code","source":"predictions = lm.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d5f33f18ae03ab284b8c7a5a6e480e1c1952fc6"},"cell_type":"code","source":"print(predictions[0:5].transpose())\nprint(lm.score(X,y))\ndf_features.head(n=5).transpose().head(n=5).transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfbf37216d95adf93ce32cdbe4c07d148eca3dc6"},"cell_type":"markdown","source":"**3. Using neural networks to improve the results**\n\n*We are going to use a more complex model to do our predictions.*"},{"metadata":{"trusted":true,"_uuid":"dd4981ef9bb2155af802fa986215f50056f89391"},"cell_type":"code","source":"# We do the necessary imports\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization, MaxPooling1D\nfrom keras import optimizers\nfrom keras.optimizers import RMSprop\nfrom keras.optimizers import Adam\nfrom keras.applications import vgg16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b11eb15f9b6caa12459de51e122e4adc5099c84"},"cell_type":"code","source":"model = Sequential()\n# Our input will be a 10 size vector containing the coefficients for each eigenvector\nmodel.add(Dense(100, input_dim=9))\nmodel.add(Activation('relu'))\nmodel.add(Dense(200))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(100))\nmodel.add(BatchNormalization())\nmodel.add(Dense(100))\n#model.add(MaxPooling1D(pool_size=3))\nmodel.add(Dense(5))\nmodel.compile(loss='mean_squared_error', optimizer=Adam(lr=0.01,decay=0.1), metrics=['accuracy'])\nmodel.summary()\n#sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4af9f9d4c678d9623c60bd12de29779ac9549248"},"cell_type":"code","source":"hist = model.fit(X, y, epochs=1000, verbose=1, validation_split=0.2)\ny_pred = model.predict(X) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7abf344b536a3d5eb5cdecb4c6dfd6d110db859c"},"cell_type":"code","source":"print(y_pred[0:5].transpose())\nscores = model.evaluate(X, y, verbose=1)\nprint('%(score)f percent accuracy.'%{'score':scores[1]*100})\ndf_features.head(n=5).transpose().head(n=5).transpose()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a8cad27f39b4bf2ea8db24587d1d11cc31aab41"},"cell_type":"markdown","source":"This is quite an improvement compared to the baseline model using simple linear regression.\nIt could be interesting to test the model with outliers such as pathological Scapulae or samples from our model with parameters far from the mean, we could then evaluate wether or not our model is overfitting on the data we have."},{"metadata":{"_uuid":"3500abb8eb2eedf21b5f5ba969eb15ccf86ecfed"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"8fd3c58547648fa41eddd3d1a2a38d48034199eb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}