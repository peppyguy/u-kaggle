{"cells":[{"metadata":{"_uuid":"8b1ab94702a571bec5c72d2b8c41289206930c7b"},"cell_type":"markdown","source":"### If you like this kernel greatly appreciate an UPVOTE \n\n# Simple Tutorial on How to Handle Missing Data ?\n\nReal-world data often has missing values. For example imagine buying a chocolate box with 60 chocolate samples where there are 15 different unique shapes of chocolates. Unfortunately, on    opening the chocolate box, you find two empty segments of chocolate. Now let us ask some questions to ourselves like\n\n   - Can you accurately find a way out off handling the missing chocolate segments?\n   - Should one just pretend as if the missing chocolate isn’t missing.? \n   - Should one return the chocolate box to the seller?  \n   - Should one go and buy two other chocolates   to fill the missing portion? \n   - Or can one just predict the shape of the missing chocolate based on previous experience of arrangement and shapes of chocolate in the box and then buy a chocolate of such predicted shape?             \n\nData can have missing values for a number of reasons such as observations that were not recorded and data corruption.Handling missing data is important as many machine learning algorithms do not support data with missing values.\n\nIn this kernel tutorial, you will discover how to handle missing data for machine learning with Python.\n\nSpecifically, after completing this tutorial you will know:\n\n- How to marking invalid or corrupt values as missing in your dataset.\n- How to remove rows with missing data from your dataset.\n- How to impute missing values with mean values in your dataset.\n\nLet’s get started.\n\n"},{"metadata":{"_uuid":"5ad4f534a6d60cce561ab431760fcf3571e99d05"},"cell_type":"markdown","source":"### Overview\n\n\nThis kernel tutorial is divided into 6 parts:\n\n**1. Pima Indians Diabetes Dataset:** where we look at a dataset that has known missing values.\n\n**2. Mark Missing Values:** where we learn how to mark missing values in a dataset.\n\n**3. Missing Values Causes Problems:** where we see how a machine learning algorithm can fail when it contains missing values.\n\n**4. Remove Rows With Missing Values:** where we see how to remove rows that contain missing values.\n\n**5. Impute Missing Values:** where we replace missing values with sensible values.\n\n**6. Algorithms that Support Missing Values:** where we learn about algorithms that support missing values.\n\nFirst, let’s take a look at our sample dataset with missing values.\n\n## 1. Pima Indians Diabetes Dataset\n\nThe Pima Indians Diabetes Dataset involves predicting the onset of diabetes within 5 years in Pima Indians given medical details.\n\nIt is a binary (2-class) classification problem. The number of observations for each class is not balanced. There are 768 observations with 8 input variables and 1 output variable. The variable names are as follows:\n\n0. Number of times pregnant.\n1. Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n2. Diastolic blood pressure (mm Hg).\n3. Triceps skinfold thickness (mm).\n4. 2-Hour serum insulin (mu U/ml).\n5. Body mass index (weight in kg/(height in m)^2).\n6. Diabetes pedigree function.\n7. Age (years).\n8. Class variable (0 or 1).\n\nWe can load the dataset as a Pandas DataFrame and print summary statistics on each attribute."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nprint(os.listdir(\"../input\"))\ndf = pd.read_csv('../input/pima-diabetes/pimaindians-diabetes.data.csv',header = None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"646449ba699ba30527bddc1af167e22e967732fd"},"cell_type":"markdown","source":"## 2. Mark Missing Values\n\nIn this section, we will look at how we can identify and mark values as missing.We can print summary statistics on each            attribute."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3e3c139485dd51acbd19b1bcd42aa2b49cacccf9"},"cell_type":"markdown","source":"This dataset is known to have missing values.\n\nSpecifically, there are missing observations for some columns that are marked as a zero value.\n\nThis is useful.\n\nWe can see that there are columns that have a minimum value of zero (0). On some columns, a value of zero does not make sense and indicates an invalid or missing value.\n\nSpecifically, the following columns have an invalid zero minimum value:\n\n1: Plasma glucose concentration\n\n2: Diastolic blood pressure\n\n3: Triceps skinfold thickness\n\n4: 2-Hour serum insulin\n\n5: Body mass index\n\nLet’ confirm this my looking at the raw data, the example prints the first 20 rows of data."},{"metadata":{"trusted":true,"_uuid":"a61ab9842c00be020561453fe5f776b63a8e2e38","_kg_hide-input":true},"cell_type":"code","source":"df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e797b42270f82abe39fbeb9e980d2ffa878fcb5e"},"cell_type":"markdown","source":"We can get a count of the number of missing values on each of these columns. We can do this my marking all of the values in the subset of the DataFrame we are interested in that have zero values as True. We can then count the number of true values in each column.\n\nWe can do this my marking all of the values in the subset of the DataFrame we are interested in that have zero values as True. We can then count the number of true values in each column."},{"metadata":{"trusted":true,"_uuid":"f64b182039bf1784ae0d5994da26c36a3c4d37d3","_kg_hide-input":true},"cell_type":"code","source":"print((df[[1,2,3,4,5]] == 0).sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a062eba7ab1b9b22f950c70d84f70a570c56e998"},"cell_type":"markdown","source":"We can see that columns 1,2 and 5 have just a few zero values, whereas columns 3 and 4 show a lot more, nearly half of the rows.\n\nThis highlights that different “missing value” strategies may be needed for different columns, e.g. to ensure that there are still a sufficient number of records left to train a predictive model.\n\nIn Python, specifically Pandas, NumPy and Scikit-Learn, we mark missing values as NaN.\n\nValues with a NaN value are ignored from operations like sum, count, etc.\n\nWe can mark values as NaN easily with the Pandas DataFrame by using the replace() function on a subset of the columns we are interested in.\n\nAfter we have marked the missing values, we can use the isnull() function to mark all of the NaN values in the dataset as True and get a count of the missing values for each column."},{"metadata":{"trusted":true,"_uuid":"b2256ec3195f41b5646dd89402a63a83a40c72d1","_kg_hide-input":true},"cell_type":"code","source":"# mark zero values as missing or NaN\ndf[[1,2,3,4,5]] = df[[1,2,3,4,5]].replace(0, np.NaN)\nprint(df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd6a3f1900488c8b7df1c96394a602f61a30e819"},"cell_type":"markdown","source":"Running   the example prints the number of missing values in each column. We can see that the columns 1:5 have the same number of missing values as zero values identified above. This is a sign that we have marked the identified missing values correctly.\n\nWe can see that the columns 1 to 5 have the same number of missing values as zero values identified above. This is a sign that we have marked the identified missing values correctly\n\nThis is a useful summary. I always like to look at the actual data though, to confirm that I have not fooled myself.\n\nBelow is the same example, except we print the first 20 rows of data.\n"},{"metadata":{"trusted":true,"_uuid":"41a7e3b227ad4f3ba93289f0b6f05d48cdcb2fdd","_kg_hide-input":true},"cell_type":"code","source":"# mark zero values as missing or NaN\ndf[[1,2,3,4,5]] = df[[1,2,3,4,5]].replace(0, np.NaN)\nprint(df.head(20))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60382b8646f0d5da271ebb52272f61d8262a2906"},"cell_type":"markdown","source":"Running the example, we can clearly see NaN values in the columns 2, 3, 4 and 5. There are only 5 missing values in column 1, so it is not surprising we did not see an example in the first 20 rows.\n\nIt is clear from the raw data that marking the missing values had the intended effect.\n\nBefore we look at handling missing values, let’s first demonstrate that having missing values in a dataset can cause problems.\n\n## 3. Missing Values Causes Problems\n\nHaving missing values in a dataset can cause errors with some machine learning algorithms.\n\nIn this section, we will try to evaluate a the Linear Discriminant Analysis (LDA) algorithm on the dataset with missing values.\n\nThis is an algorithm that does not work when there are missing values in the dataset.\n\nThe below example marks the missing values in the dataset, as we did in the previous section, then attempts to evaluate LDA using 3-fold cross validation and print the mean accuracy."},{"metadata":{"trusted":true,"_uuid":"ff2625a8e8599434558e12347ab63ab4205709ac","collapsed":true,"_kg_hide-input":true},"cell_type":"markdown","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.model_selection import cross_val_score\n\n# split dataset into inputs and outputs\n\nvalues = df.values\n\nX = values[:,0:8]\n\ny = values[:,8]\n\n# evaluate an LDA model on the dataset using k-fold cross validation\n\nmodel = LinearDiscriminantAnalysis()\n\nkfold = KFold(n_splits=3, random_state=7)\n\nresult = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n\nprint(result.mean())"},{"metadata":{"_uuid":"3b8390c931193baebee4092d2c99dce7595ea148"},"cell_type":"markdown","source":"**for the above code an  error is expected \n\nWe are prevented from evaluating an LDA algorithm (and other algorithms) on the dataset with missing values.\n\nNow, we can look at methods to handle the missing values.\n\n## 4. Remove Rows With Missing Values\n\nThe simplest strategy for handling missing data is to remove records that contain a missing value.\n\nWe can do this by creating a new Pandas DataFrame with the rows containing missing values removed.\n\nPandas provides the dropna() function that can be used to drop either columns or rows with missing data. We can use dropna() to remove all rows with missing data, as follows:"},{"metadata":{"trusted":true,"_uuid":"0d33093a6326668ee41ed96fe7f3a6c13e24956d","_kg_hide-input":true},"cell_type":"code","source":"# drop rows with missing values\ndf.dropna(inplace=True)\n# summarize the number of rows and columns in the dataset\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b96934d74131a16fe0bcce5625c6b61c468d26dd"},"cell_type":"markdown","source":"Running this example, we can see that the number of rows has been aggressively cut from 768 in the original dataset to 392 with all rows containing a NaN removed."},{"metadata":{"trusted":true,"_uuid":"3ee1d5b7afd466c09207a066bb93126368090774","_kg_hide-input":true},"cell_type":"code","source":"# split dataset into inputs and outputs\nvalues = df.values\nX = values[:,0:8]\ny = values[:,8]\n# evaluate an LDA model on the dataset using k-fold cross validation\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import KFold,cross_val_score\nmodel = LinearDiscriminantAnalysis()\nkfold = KFold(n_splits=3, random_state=7)\nresult = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\nprint(result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a74ef40fcca66164946abacda2f5d4df936b8bc"},"cell_type":"markdown","source":"The example runs successfully and prints the accuracy of the model.Removing rows with missing values can be too limiting on some predictive modeling problems, an alternative is to impute missing values.\n\n## 5. Impute Missing Values\n\nImputing refers to using a model to replace missing values.\n\nThere are many options we could consider when replacing a missing value, for example:\n\n- A constant value that has meaning within the domain, such as 0, distinct from all other values.\n- A value from another randomly selected record.\n- A mean, median or mode value for the column.\n- A value estimated by another predictive model.\n\nAny imputing performed on the training dataset will have to be performed on new data in the future when predictions are needed from the finalized model. This needs to be taken into consideration when choosing how to impute the missing values.\n\nFor example, if you choose to impute with mean column values, these mean column values will need to be stored to file for later use on new data that has missing values.\n\nPandas provides the fillna() function for replacing missing values with a specific value.\n\nFor example, we can use fillna() to replace missing values with the mean value for each column, as follows:"},{"metadata":{"trusted":true,"_uuid":"e686cfbd7b91cf825a4bd9561359bf34de317158","_kg_hide-input":true},"cell_type":"code","source":"# fill missing values with mean column values\ndf.fillna(df.mean(), inplace=True)\n# count the number of NaN values in each column\nprint(df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"feef1fef87fbc7b0f6c8fe6c28b544f543eb5d6e"},"cell_type":"markdown","source":"The scikit-learn library provides the Imputer() pre-processing class that can be used to replace missing values.\n\nIt is a flexible class that allows you to specify the value to replace (it can be something other than NaN) and the technique used to replace it (such as mean, median, or mode). The Imputer class operates directly on the NumPy array instead of the DataFrame.\n\nThe example below uses the Imputer class to replace missing values with the mean of each column then prints the number of NaN values in the transformed matrix."},{"metadata":{"trusted":true,"_uuid":"02914eaf5aae971307ebdfe39b6465c9a5f02644","_kg_hide-input":true},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\n# fill missing values with mean column values\nvalues = df.values\nimputer = Imputer()\ntransformed_values = imputer.fit_transform(values)\n# count the number of NaN values in each column\nprint(np.isnan(transformed_values).sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"795bb47ffe0239c25fff2135d2a115e7da6c171a"},"cell_type":"markdown","source":"Running the example shows that all NaN values were imputed successfully.\n\nIn either case, we can train algorithms sensitive to NaN values in the transformed dataset, such as LDA.\nThe example below shows the LDA algorithm trained in the Imputer transformed dataset."},{"metadata":{"trusted":true,"_uuid":"f2d3936da5f0451be3a5b5e7e92346ce8d2b5888"},"cell_type":"code","source":"# evaluate an LDA model on the dataset using k-fold cross validation\nmodel = LinearDiscriminantAnalysis()\nkfold = KFold(n_splits=3, random_state=7)\nresult = cross_val_score(model, transformed_values, y, cv=kfold, scoring='accuracy')\nprint(result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9b57596b457803835204fffa1502ed6b284c9a4"},"cell_type":"markdown","source":"Running the example prints the accuracy of LDA on the transformed dataset.\n\nTry replacing the missing values with other values and see if you can lift the performance of the model.\n\n**Maybe missing values have meaning in the data.**\n\nNext we will look at using algorithms that treat missing values as just another value when modeling.\n\n## 6. Algorithms that Support Missing Values\n\nNot all algorithms fail when there is missing data.\n\nThere are algorithms that can be made robust to missing data, such as k-Nearest Neighbors that can ignore a column from a distance measure when a value is missing.\n\nThere are also algorithms that can use the missing value as a unique and different value when building the predictive model, such as classification and regression trees.\n\nSadly, the scikit-learn implementations of decision trees and k-Nearest Neighbors are not robust to missing values. Although it is being considered.\n\nNevertheless, this remains as an option if you consider using another algorithm implementation (such as xgboost) or developing your own implementation."},{"metadata":{"_uuid":"a0f18b02e56b23e9708e874c9988386cc0e1090f"},"cell_type":"markdown","source":"## Summary\n\nIn this kernel, you discovered how to handle machine learning data that contains missing values.\n\nSpecifically, you learned:\n\n- How to mark missing values in a dataset as numpy.nan.\n- How to remove rows from the dataset that contain missing values.\n- How to replace missing values with sensible values.\n\n# If you like this kernel greatly appreciate an UPVOTE "},{"metadata":{"trusted":true,"_uuid":"99e46053a2fc25287948dce9dd4b876a36e21cd3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}