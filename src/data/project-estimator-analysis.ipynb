{"cells":[{"metadata":{"_uuid":"545af580ef3fa50bae581f702690b653cd1adc22"},"cell_type":"markdown","source":"Ever since I started off on my own as a freelance patent illustrator I've wanted to find a better way to provide my clients with accurate estimates for projects. A typical project starts off with the client sending me disclosure about a new patent idea. This disclosure can be in a variety of different formats: photos, prototypes, rough sketches, detailed sketches, written descriptions, and full-blown 3D models. And depending on the type of patent case it is, a project can consist of one single page, or it can consist of over 100 pages. Some pages will consist of only simple diagrams that don't take long at all compared, for example, to a cross section of an engine drawn out over multiple pages. In some instances, depending on the detail level of the disclosure, I will actually build a 3D model of the invention in order to have all the detail required.\n\nSo as you can imagine, a lot goes into estimating a project. It starts out with me getting an understanding of the invention and what we're trying to communicate with the drawings. So when a new project comes in, work that I'm doing on other projects has to be halted so that I can send out an estimate on this new project. This is where Data Science and specifically Machine Learning comes in. \n\nAfter discovering the world of Data Science I started to think about my business differently. I started collecting data on anything that I could think of that might be of value. I ended up finding a few good things that I could record about my various projects: the type of case (it's always either a **design** or **utility** patent), the **number of pages** in the final package, and whether or not I had to **build a 3D model**. I did this, collecting the data, just to be able to analyze things when I became curious about a particular aspect of my business. \n\nThen I got into machine learning and realized that I could potentially leverage this data to not only look at the past, but to predict the future of my projects. I got excited about the idea of building a model that would allow me to provide clients with accurate estimates that would take a fraction of the time. So that's what this first post is about. I thought it might be interesting to document my journey with this project as I collect more data and improve the model. As more data comes in, I will update with a new post. Thanks for checking it out."},{"metadata":{"_uuid":"89a550d37df2209810fdf7a98a9f32fe4e299890"},"cell_type":"markdown","source":"# Loading the Libraries for EDA"},{"metadata":{"trusted":true,"_uuid":"0e06661b679385c6524eae98a42326cc2acca845"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom plotnine import *\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae83cfc4a90efd6fe3e3d18f7779565ead38f957"},"cell_type":"markdown","source":"# Importing the Data"},{"metadata":{"trusted":true,"_uuid":"d558f2fcec7fe05e5ef926aa6fd0d3a96b4d4b0f"},"cell_type":"code","source":"df = pd.read_csv('../input/projects.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64ed0ab18f5665e1c0650b0a688b48806c9ab299"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75a16892f3b0822123d19f266d0e2ae7d3b541cf"},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"_uuid":"beabfbfe010774771ce32a04f35f8e3f6b5dc429"},"cell_type":"markdown","source":"## Variable Identification"},{"metadata":{"trusted":true,"_uuid":"0c2a05a62170e3d012662b5bde963c1fb60bdb93"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4aad6287f6040a5a7bf12c18a94d14ac2abe8349"},"cell_type":"markdown","source":"Here we can see the variables of the dataset. Our dependent variable will be hours as this is what we're trying to predict. Let's take a look at the datatypes for each variable."},{"metadata":{"trusted":true,"_uuid":"e893f8a2921aa3b07fe89af76e401f4096c07cfc"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d59aab4c37eadda71377df57e33295b8ba4581a"},"cell_type":"markdown","source":"We have 3 categorical independent variables, and 1 continuous independent variable."},{"metadata":{"_uuid":"3f92aa98b0ceff94a410df3115bb55344444e091"},"cell_type":"markdown","source":"## Univariate Analysis"},{"metadata":{"_uuid":"ad894255b7ddfa0e36c42dbee510cc04368dcc3f"},"cell_type":"markdown","source":"#### Case Type Variable"},{"metadata":{"trusted":true,"_uuid":"8be9fca192180bbcd5582a650d0b79a51d92ca30"},"cell_type":"code","source":"df['case_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad58377fa3fcb8a8fa49c7bff5fab532bd4688ab"},"cell_type":"code","source":"(df['case_type'].value_counts() / len(df)).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb2a9ad5488e0235fc07d8f7bfe009d83b55e36d"},"cell_type":"markdown","source":"Here we can see that utility cases make up about 70% of this dataset."},{"metadata":{"_uuid":"6eb84f3405562eee2ee0966f940bbcb95d5c7fab"},"cell_type":"markdown","source":"#### Number of Pages"},{"metadata":{"trusted":true,"_uuid":"0f8094db5df4d97634b1bc1fcfe34c4e093be33d"},"cell_type":"code","source":"df['number_pages'].value_counts().sort_index().plot.line()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1edb1c5e4b674fe1276eb19f7711490274c6f6e0"},"cell_type":"markdown","source":"Here we can see that the majority of the projects in this dataset have under 10 pages."},{"metadata":{"_uuid":"73acc6c7c5732b36da9b2ff07a641323b6e2e8a7"},"cell_type":"markdown","source":"#### 3D Modeling"},{"metadata":{"trusted":true,"_uuid":"ae5e5dcd29777d106e5fd6cfe540120c1efc6c18"},"cell_type":"code","source":"df['3d_modeling'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86e71ceb3a64b2622b3868a74aebde8073ca0a8f"},"cell_type":"code","source":"(df['3d_modeling'].value_counts() / len(df)).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e360960b6e995d70159b920e27dceaae431dc295"},"cell_type":"markdown","source":"Here we see that for this dataset only about 20% percent of the projects require me to create a 3d model."},{"metadata":{"_uuid":"ee1716e6f97232c1a89b85582667b0199e6098d0"},"cell_type":"markdown","source":"#### Target Variable (hours)"},{"metadata":{"trusted":true,"_uuid":"faed6046dabd7968bc9949bd062a639c5955675b"},"cell_type":"code","source":"df['hours'].value_counts().sort_index().plot.line()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c43cffdb05365f123fe9c39b6dc22fbac16439ec"},"cell_type":"markdown","source":"Here we can see that most of the projects in this dataset take around 7 hours to complete. It is pretty rare to have a case that goes over 7 hours."},{"metadata":{"_uuid":"191dd7ce4c748cee7bef7db05a22c37cf03c9408"},"cell_type":"markdown","source":"## Bi-variate Analysis"},{"metadata":{"_uuid":"2998a9377a15144c8c49028391d41b5c040ca409"},"cell_type":"markdown","source":"#### Case Type vs Number of Pages (Categorical vs. Categorical)"},{"metadata":{"trusted":true,"_uuid":"0d57b0f148b5a48ec006480559798046f91b06f1"},"cell_type":"code","source":"type_pages = pd.crosstab(index = df['number_pages'],\n                        columns = df['case_type'])\ntype_pages","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"662e5605f03caf4abb4e781d6bef0706be2c60c1"},"cell_type":"code","source":"type_pages.plot(kind = 'bar',\n               figsize = (8,8),\n               stacked=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dabe019f09b4b2b55ea4382f8b2b77a49a3f99a"},"cell_type":"markdown","source":"Here we can see that for both utility and design cases the majority seem to be in the 5 - 8 page range. Also, utility projects appear to be more spread out, whereas design projects tent to mainly fall within that 5 - 8 range. This makes sense to me as someone with domain knowledge about this kind of work.\n\nFor example, I know that for design projects the end goal is to have a package that describes the design of a new invention from every angle. Typically there will be an orthographic (isometric) perspective drawing, and then a front view elevation drawing, rear, left and right side, and top and bottom. Each view tends to get it's own page -- with occasional exceptions -- and so I'll usually end up with 7 pages. \n\nUtility projects on the other had can really be any length of pages. I was actually interested to see the distribution of the number of pages for utility cases as without this visual I would not have been able to guess it's shape."},{"metadata":{"_uuid":"1f0fbc2373a4aaabbf54d757bd98af07d66d256f"},"cell_type":"markdown","source":"#### Case Type vs 3D Modeling (Categorical vs. Categorical)"},{"metadata":{"trusted":true,"_uuid":"62f700f8fb10bf434fda08c40d0b7d9bddd97447"},"cell_type":"code","source":"type_modeling = pd.crosstab(index = df['3d_modeling'],\n                        columns = df['case_type'])\ntype_modeling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1a9e29b7aa7acc5fd289a3b8604c27d90f04d9d"},"cell_type":"code","source":"type_modeling.plot(kind = 'bar',\n               figsize = (8,8),\n               stacked=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"407cd63b04574054b29e27d5108320246ff24827"},"cell_type":"markdown","source":"Here we can see that 3D modeling is more often used for design projects. Again, because of my domain knowledge this makes complete sense to me. As I mentioned above design projects are all about seeing a new invention from all angles and sometimes the disclosure I receive from the client is lacking in it's descriptiveness. This is where modeling things in 3D first will allow me to work much faster, and more accurately, than if I tried to wing it.\n\nAs utility matters really only have to work on a 2D plane, it's pretty rare that I need to incorporate any 3D modeling in those projects. It is interesting to see just how much (proportionally within this sample) I'm actually using this option."},{"metadata":{"_uuid":"128cf3e987aa3a6e96c7eab2f4389d1344928685"},"cell_type":"markdown","source":"#### Case Type vs Hours (Categorical vs. Continuous)"},{"metadata":{"trusted":true,"_uuid":"e10598cdce0ec4d0b42abbe95a6f8653be51c68e"},"cell_type":"code","source":"df.boxplot(column=\"hours\",\n           by= \"case_type\",\n           figsize= (8,8))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d14a6f465ef66bace961f1d376053d4de0f3217"},"cell_type":"markdown","source":"Here we get a sense of the time required to finish each type of project. We also see that there are some outliers, but given my knowledge of those matters I know that they were simply ramdom projects that turned out to be quite lengthly. Because I want to be able to estimate these larger projects, it makes sense to keep them in the set. Coming up with a way to accurately and quickly estimate projects is actually even more important when it comes to large cases as it can take quite a while to go through each page and try to guess how long it will take to complete."},{"metadata":{"_uuid":"549472710408b3a56888027f38f254dbc9b515fa"},"cell_type":"markdown","source":"#### Number of Pages vs 3D Modeling (Cat vs Cat))"},{"metadata":{"trusted":true,"_uuid":"07714f3227d3c35a0377803ca2ed4ff89b3ba917"},"cell_type":"code","source":"pages_modeling = pd.crosstab(index = df['number_pages'],\n                        columns = df['3d_modeling'])\npages_modeling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb4d8958597b7b166f429d6c7240b64dd1453def"},"cell_type":"code","source":"pages_modeling.plot(kind = 'bar',\n               figsize = (8,8),\n               stacked=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57f00c8d7a92330ac375795aaebb1703e57df244"},"cell_type":"markdown","source":"Everything here is generally skewed to the right, but we see a high number of projects requiring 3D modeling to have around 5 to 7 pages. This can be explained by what I mentioned about about design matters being the main type of project to require 3D modeling. And we know that design projects tend to fall in the page count range."},{"metadata":{"_uuid":"8695a9df0296a0645c027fcd5417d601a33526b8"},"cell_type":"markdown","source":"#### Number of Pages vs Hours ( Cat vs Cont)"},{"metadata":{"trusted":true,"_uuid":"957ff8b6af25a6616a8094a3d62f463f8dd12fe1"},"cell_type":"code","source":"df.boxplot(column=\"hours\",\n           by= \"number_pages\",\n           figsize= (8,8))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"136660a69bcfe5bc02b0afe95a6a93f48628af92"},"cell_type":"markdown","source":"There seems to be an obvious positive relationship between the **number_pages** and **hours** variables, but there is quite a bit of variance. This illustrates the fact that it isn't necessarily the length of a project, in terms of the number of pages, that dictates the amount of time required to finish because some drawings can be much more complex than others."},{"metadata":{"_uuid":"c5630f7d419a5d19ed855dea87e2fc2dd3b31be2"},"cell_type":"markdown","source":"#### 3D Modeling vs Hours ( Cat vs Cont)"},{"metadata":{"trusted":true,"_uuid":"c3946886580f709f966e4859f293ff9528dbc24d"},"cell_type":"code","source":"df.boxplot(column=\"hours\",\n           by= \"3d_modeling\",\n           figsize= (8,8))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59f553cfdeb079e87ac46dd0f40217da22366003"},"cell_type":"markdown","source":"This looks like another effect of the utility versus design relationship I've been discussing. Design projects tend to be more predicable in terms of hours to complete."},{"metadata":{"_uuid":"3638687cfdd648e9e98571224a870014d473c065"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"_uuid":"eb0e452dd99c5c7e04a2f238980c93c7e9e082c0"},"cell_type":"markdown","source":"Because there is so much variance in the length of time needed to complete a project, I'd like to try some feature engineering. Hopefully this will not only help our model's accuracy, it will also make it easier on me when I'm entering information about a new project that needs estimating. \n\nGiven the fact that the drawing complexity will vary from project to project, it will take longer to complete pages for more complex projects. I'd like to create a new variable called **difficulty** that will assign levels of difficulty based on how long it takes to finish a page for each project. I'll start by creating a temporary vector to hold the values of a project's **hours** divided by it's **number_pages**."},{"metadata":{"trusted":true,"_uuid":"aaa96f8f905e4aa2fd8195c88534e65678925429"},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3d688f544e68f429ae84b9f886340d20399b252"},"cell_type":"code","source":"# Create a new variable that records 'hours' / 'number_pages'\ndf['hour_page'] = df['hours'] / df['number_pages']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e19ad0f200d7538f4b299d99d82a05be9114f280"},"cell_type":"code","source":"df.hour_page.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af368ce0a444dd9e4424d6c3f626a49036bf6c60"},"cell_type":"markdown","source":"Here we can take a look at the spead again. I'd like to divide this into four equal groups that will represent the four difficulty levels."},{"metadata":{"trusted":true,"_uuid":"75b5893c81e479334f6c227069f476aaf24538fc"},"cell_type":"code","source":"# Create variables to store location of bin boundaries\nhp_min = df.hour_page.min()\nhp_max = df.hour_page.max()\nhp_range = hp_max - hp_min\nhp_bin = hp_range / 4\n\n\n# Create variables to store location of difficulty bins\nlevel_one = hp_min + hp_bin\nlevel_two = level_one + hp_bin\nlevel_three = level_two + hp_bin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73daab5f3e21516394708f4b0cd5912b6946ccbc"},"cell_type":"code","source":"# Create a function that will assign a difficulty to each project\n# based on 'hour_page'\n\ndef get_difficulty(row):\n    difficulty = 0\n    if row.hour_page < level_one:\n        difficulty = 1\n    elif (row.hour_page >= level_one) & (row.hour_page < level_two):\n        difficulty = 2\n    elif (row.hour_page >= level_two) & (row.hour_page < level_three):\n        difficulty = 3\n    elif (row.hour_page >= level_three):\n        difficulty = 4\n    else:\n        return difficulty\n    \n    return difficulty","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9321485bcf5a43f8a71b68e1eeb5a4eddf8a834a"},"cell_type":"code","source":"df['difficulty'] = df.apply(get_difficulty, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eae1f0d4a0aeb8a0a7c24d1ac276f5514d125896"},"cell_type":"code","source":"df.difficulty.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f50783f7abc71b438e357ae0953ec1a424f6b31b"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49bc06eab461b44c79ef27deea49ffd064a4855c"},"cell_type":"markdown","source":"Having the four options for difficulty will make it easier for me to take a better guess when estimating a new project. Now instead of trying to guess and exact amount of time-per-page, I simply have to narrow my guess to four options. After creating this variable I think I'm ready to get into the modeling stage."},{"metadata":{"_uuid":"4576f999a01b2c2fdfebcb081755c782c292727f"},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"_uuid":"f4a6a642c2fc51d6c0e3d6627e2325c604d502f7"},"cell_type":"markdown","source":"Since I'm working with such a small amount of data, I know it's important to get the right model. I'm referencing scikit learn's cheat sheet here http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html to find a good starting point. Based on my data, I'm deciding to go with an Support Vector Regression."},{"metadata":{"trusted":true,"_uuid":"6b9382a3c0c433f7469c3ec624646d2d0b4103cb"},"cell_type":"code","source":"# First I'll import some libraries I know I'll need \nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb9096168ca96873d536a66db64fdc7745032973"},"cell_type":"code","source":"# I'll make a copy of the dataset so I can refer back to it\ndf_train = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2107c1beee3e43538b648aa0db4cc07d72900d69"},"cell_type":"markdown","source":"Let's remove some variables that we won't need. **ProjectID** is simply an identification so it won't be helping the model. **hour_page** can be removed because we already have this information accounted for in the fact that we have both the **number_pages** and the **hours** variables."},{"metadata":{"trusted":true,"_uuid":"805a3fd2872c9724ed65298689ac99b66ac810e0"},"cell_type":"code","source":"# Delete unneeded variables\ndel df_train['project_id']\ndel df_train['hour_page']\n\nprint('Data shape:', df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f25ba14a5625482243bd3483de2b3cec1cd09a16"},"cell_type":"markdown","source":"### Data Preprocessing"},{"metadata":{"_uuid":"f70ee630217f1f1302a6fa00c759a08ed4df39b5"},"cell_type":"markdown","source":"We have two categorical variables that need to be encoded. I'll use LabelEncoder to take care of this."},{"metadata":{"trusted":true,"_uuid":"f5aad4efd0d3281847fb564039362f6870246143"},"cell_type":"code","source":"# Use label encoder on the 'case_type' and '3d_modeling' variables\nlabelencoder_X = LabelEncoder()\n\ndf_train['case_type'] = labelencoder_X.fit_transform(df_train['case_type'])\ndf_train['3d_modeling'] = labelencoder_X.fit_transform(df_train['3d_modeling'])\n# utility = 1, yes = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d32340e06ca19164537ae111be08fc329e6f50d"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa1ea716192a089f573b7f8cda9a38f6c10c3e8a"},"cell_type":"markdown","source":"Here we can confirm that the label encoding correctly converted the categorical variables. Here a **case_type** of 1 = **utility** (0 = design), and a 1 for **3d_modeling** = **yes**. Next we'll separate our *Independent* variables and our *dependent* variable."},{"metadata":{"trusted":true,"_uuid":"4cefdc8c6695b29259e88a3e9a01402b45a5a777"},"cell_type":"code","source":"# Create X and y arrays for the dataset\nX = df_train[['case_type', 'number_pages', '3d_modeling', 'difficulty']].copy()\ny = df_train['hours'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8af231991466b656ae4fb2ba7055aaecd1b21e87"},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"859f713168b18cf6bc06047a3ceb26b88b064c69"},"cell_type":"markdown","source":"We'll have to reshape the *y* vector in order for our model to work correctly."},{"metadata":{"trusted":true,"_uuid":"d3113efd46b6f4f89e0012a61479905a7475e98c"},"cell_type":"code","source":"y = y.reshape(-1, 1)\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b386824c2fec02ef3e40f551012c7fe53851972"},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab1cfcd9d30cd3914399c13ed5aa778dd4aab031"},"cell_type":"markdown","source":"Now we have the dataset split into training and test sets."},{"metadata":{"trusted":true,"_uuid":"606afe47b0803beb2b9932497132a1c9285ce810"},"cell_type":"code","source":"print('Training data shape: {}'. format(len(X_train)))\nprint('Test data shape: {}'. format(len(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c01ef440aefd2e686ab4949c0e30f4a11a9b283"},"cell_type":"markdown","source":"### Fitting the Model"},{"metadata":{"trusted":true,"_uuid":"2945fd618753b6516db148644bd04b4d38208530"},"cell_type":"code","source":"# PIPELINE\nmy_pipeline = make_pipeline(StandardScaler(), SVR(kernel = 'linear'))\n\nmy_pipeline.fit(X_train, y_train)\ny_pred = my_pipeline.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eedb8fe527e67cce880712bcd16d01db9fa0d9b9"},"cell_type":"code","source":"svr_pipeline_score = mean_absolute_error(y_test, y_pred)\nprint('Mean Absolute Error for SVR: {}'.format(svr_pipeline_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8f98aca40bb8f796b80f8f6446f49ef31bbfae7"},"cell_type":"markdown","source":"Let's see if we can improve on the model by using **Grid Search** to find the best hyperparameters to use."},{"metadata":{"_uuid":"481165b488937b0a69b1057cbe3e2f09fe685e1b"},"cell_type":"markdown","source":"### Grid Search"},{"metadata":{"trusted":true,"_uuid":"2d41b640f02fb41888750e0f75fc8a3d4c6b992c"},"cell_type":"code","source":"# Feature Scaling\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\ny_train = sc_y.fit_transform(y_train)\n\n\nregressor = SVR(kernel = 'linear')\n\n\n\n# Applying Grid Search to find the best model and the best parameters\nparameters = [{'C': [1, 5, 10, 15, 20], 'kernel': ['linear']},\n              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\ngrid_search = GridSearchCV(estimator = regressor,\n                           param_grid = parameters,\n                           scoring = 'neg_mean_absolute_error',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\n\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint('Best Accuracy: {}'.format(best_accuracy))\nprint('Best Parameters: {}'.format(best_parameters))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9880f94386080f165849a1264f2edb90a5c706cc"},"cell_type":"markdown","source":"### Applying New Hyperparameters"},{"metadata":{"trusted":true,"_uuid":"4c85a9dd38d9810deefe3fc499769c49222b9a23"},"cell_type":"code","source":"# Create X and y arrays for the dataset\nX = df_train[['case_type', 'number_pages', '3d_modeling', 'difficulty']].copy()\ny = df_train['hours'].values\ny = y.reshape(-1, 1)\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n\n# PIPELINE\nmy_pipeline = make_pipeline(StandardScaler(), SVR(kernel = 'linear', C = 10))\n\nmy_pipeline.fit(X_train, y_train)\ny_pred = my_pipeline.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7908ad9046053ce5debcd848434be6ff5475df3"},"cell_type":"code","source":"# Print out the accuracy score\nsvr_updated_pipeline_score = mean_absolute_error(y_test, y_pred)\nprint('Mean Absolute Error for SVR: {}'.format(svr_pipeline_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"600c3a29ff7f7d210eace5d13c8e34ccd962874b"},"cell_type":"markdown","source":"Selecting these new hyperparameters has improved our model by reducing the **mean absolute error** from around **0.97** to **0.68**. Let's see if we can improve the model even more by using the popular **XGBoost** method."},{"metadata":{"_uuid":"bcef5b2381a028b2b2d32902c1e4339ce75e23a8"},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true,"_uuid":"5b1633fa9a1db7f48d20a1033df5acbc9305ba5c"},"cell_type":"code","source":"# Fitting XGBoost to the Training set\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# get predicted 'hours' on validation data\nxg_score = mean_absolute_error(y_test, y_pred)\nprint('Mean Absolute Error for XGBoost: {}'.format(xg_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c980b5659d729f412ed8bd714536a0a0c481813b"},"cell_type":"markdown","source":"The **XGBoost** was not as effective as our updated **SVR** model."},{"metadata":{"_uuid":"8f1745122542e5fdf3f7eda3bce30a8b16a652a2"},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{"_uuid":"99d350f764aa80e78eb7e3408bdf74f32134f5da"},"cell_type":"markdown","source":"That was a lot of fun! It is extremely satisfying to use some data that I've collected about my business to actually improve efficiency. Based on the scores of the model, I feel confident enough to begin using this to assit me on future project estimations. However, it is important to note that I'm dealing with quite a small dataset here. Even though I've tested the model with some hypothecical new project parameters and haven't really gotten any estimates I don't agree to be helpful, I will always take the model's prediction with a grain of salt. I will continue to tweak and improve my model as I collect more data. Hopefully after I've collected enough data, I can begin to use **k-Fold Cross Validation** to get a better sense of my model's performance.\n\n#### Next Steps\n\nAt the time of this writing I'm using the model to estimate by a simple python script that prompts me to enter the four categories of information about a new project. Going forward I'd like to learn about using something like Flash to create an application based on the model.\n\nThanks for taking the time to check out this kernel and dataset. Please feel free to leave a comment if you have any feedback about anything I could do differently or of ways to improve. Cheers!"},{"metadata":{"trusted":true,"_uuid":"11030419944d1ed151001ba64aec8a49a02de383"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}