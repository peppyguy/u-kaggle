---
title: 'Treemap House of Horror - Spooky EDA/LDA/Features'
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---

<center><img src="http://moheban-ahlebeit.com/images/Free-Wallpaper-For-Halloween/Free-Wallpaper-For-Halloween-24.jpg"></center>

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, error=FALSE)
```


# Introduction - the road to madness



*Welcome, weary traveller. You have come to a weird and wonderful place indeed.*

Before you lies a fear-inducing exploration of the [Spooky Author Identification](https://www.kaggle.com/c/spooky-author-identification/) data set with the magical powers of [tidy R](http://tidyverse.org/) and [ggplot2](http://ggplot2.tidyverse.org/).

The aim of this hair-raising Halloween-themed challenge is to predict the author of horror stories passages based on their writing. In particular, we will be analysing texts from Edgar Allan Poe, Mary Shelley, and HP Lovecraft with the goal to predict the probability for each excerpt to be written by one of those towering figures of harrowing horror.

The [data](https://www.kaggle.com/c/spooky-author-identification/data) comes in the shape of the traditional two files (`../input/train.csv`) and (`../input/test.csv`). Each row contains a piece of horrible history from a specific author.


Let's explore the depth of their insane imagination.

*And better not get lost, or else there's no telling which horrors could await you ...*


# Preparations {.tabset .tabset-fade .tabset-pills}

## Load libraries

On our journey we will take a range of libraries for general data wrangling and general visualisation together with more specialised language-processing tools.

```{r, message = FALSE}
# general visualisation
library('ggplot2') # visualisation
library('scales') # visualisation
library('grid') # visualisation
library('gridExtra') # visualisation
library('RColorBrewer') # visualisation
library('corrplot') # visualisation

# general data manipulation
library('dplyr') # data manipulation
library('readr') # input/output
library('data.table') # data manipulation
library('tibble') # data wrangling
library('tidyr') # data wrangling
library('stringr') # string manipulation
library('forcats') # factor manipulation

# specific visualisation
library('alluvial') # visualisation
library('ggrepel') # visualisation
library('ggridges') # visualisation
library('gganimate') # visualisation
library('ggExtra') # visualisation

# specific data manipulation
library('lazyeval') # data wrangling
library('broom') # data wrangling
library('purrr') # string manipulation
library('reshape2') # data wrangling

# Text / NLP
library('tidytext') # text analysis
library('tm') # text analysis
library('SnowballC') # text analysis
library('topicmodels') # text analysis
library('wordcloud') # test visualisation
library('igraph') # visualisation
library('ggraph') # visualisation
library('babynames') # names

# Models
library('Matrix')
library('xgboost')
library('caret')

library('treemapify') #visualisation
```

## Helper functions

We will conjure up the *multiplot* function, courtesy of [R Cookbooks](http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/) to create multi-panel plots. We also prepare make a brief helper function to compute binomial confidence intervals.

```{r}
# Define multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

```{r echo=FALSE}
# function to extract binomial confidence levels
get_binCI <- function(x,n) as.list(setNames(binom.test(x,n)$conf.int, c("lwr", "upr")))
```

## Load data

We use *data.table's* fread spell to speed up reading in the data:

```{r warning=FALSE, results=FALSE}
train <- as.tibble(fread('../input/train.csv'))
test <- as.tibble(fread('../input/test.csv'))
sample_submit <- as.tibble(fread('../input/sample_submission.csv'))
```

# Overview: File structure and content {.tabset .tabset-fade .tabset-pills}

Let's first take a careful peak at the two dusty grimoires that we have been handed. *In faded large, letters, we can read the words *train* and *test* on their covers. Around the letters we can see carvings that we cannot quite identify, but which appear to change their shape when we don't look at them.* We shrug off a shiver and proceed to use our trusty *summary* and *glimpse* tools.


## Training data

```{r}
summary(train)
```

```{r}
glimpse(train)
```

We find:

- Our *training* data contains a scary sentence per row, together with a unique ID and an abbreviated author name. "HPL" is Lovecraft, "MWS" is Shelly, and "EAP" is Poe.

Here are a few example sentences:

```{r}
train$text[1]
train$text[4321]
train$text[666]
```


## Test data:

```{r}
summary(test)
```

```{r}
glimpse(test)
```

We find:

- The *test* data follows the same format and obviously does not contain the author name. Not because they shall not be named, but because that's what we are tasked to predict.


## Missing values



```{r}

sum(is.na(train))

sum(is.na(test))

```



There are no missing values. *Hopefully, nobody else will go missing either ...*





## Reformating features



We change the author name to a factor to make things easier. We might need all the help we can get on this dangerous exploration:



```{r}

train <- train %>%

  mutate(author = as.factor(author))

```





# Overview visualisations + tidytext magic



*As we dive deeper into the dusty grimoires in front of us we are starting to forget the world around us. Our candle is steadily burning as we turn page after creaky page. Slowly, very slowly, a picture emerges from the back of our mind that shows us the potential of the journey we have only just embarked on.*



With us on this dangerous expedition we have our trusty [`tidytext` *magic book*](http://tidytextmining.com/). It teaches us the fundamentals of dealing with the strange and powerful world of natural language data and *should belong in the travelling inventory of every aspiring adventurer.*



The tidy text format is being defined as a table with one token per row; with a token being a word or another meaningful unit of text (paraphrased). Through tidy text we can unleash the power of the `tidyverse` at our study subject.





## Wordclouds



We start by carefully cutting out the scary words using our `tidytext` scissors. The first pair of magic scissors at our disposal carry the `unnest_tokens` spell, which drops punctuation and transforms all words into lower case. In addition, `tidytext` contains a dictionary of `stop words`, like "and" or "next". We will ignore all the non-frightening words because *who has time for those when their very life is possibly in jeopardy?*





```{r}

t1 <- train %>% unnest_tokens(word, text)

t1 <- t1 %>%

  anti_join(stop_words, by = "word")

```





Then we pick the most common words and arrange them in a neat little wordcloud, because a pentagram would have been too dangerous:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 1", out.width="100%"}

t1 %>%

  count(word) %>%

  with(wordcloud(word, n, max.words = 50, color = c("purple4", "red4", "black")))

```



Those are the 50 most common words in our entire `training` set. We see that "time" is essential. Perhaps a warning not to spend too long gazing into the abyss of unspeakable horrors? And there is the prospect of something being "found", but whether it's "death" or "life" is being left disconcertingly vague.



We can do the same for our three authors. First Mary Shelley:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 2", out.width="100%"}

t1 %>%

  filter(author == "MWS") %>%

  count(word) %>%

  with(wordcloud(word, n, max.words = 30, color = "purple4"))

```



Then Edgar Allan Poe in a bloody red:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 3", out.width="100%"}

t1 %>%

  filter(author == "EAP") %>%

  count(word) %>%

  with(wordcloud(word, n, max.words = 30, color = "red4"))

```



Finally HP Lovecraft:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 4", out.width="100%"}

t1 %>%

  filter(author == "HPL") %>%

  count(word) %>%

  with(wordcloud(word, n, max.words = 30, color = "blue4"))

```





## Author count and sentence/word length



Sometimes, numbers can be scarier than words; so let's look at those too. Here is an overview of how many sentences we have from each author and what the distribution of the `length` of these sentences (in terms of character count) and the `word lengths` are. This is some light feature engineering to warm up our analytic dexterity:



```{r message=FALSE, split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 4", out.width="100%"}

p1 <- train %>%

  ggplot(aes(author, fill = author)) +

  geom_bar() +

  theme(legend.position = "none")



p2 <- train %>%

  mutate(sen_len = str_length(text)) %>%

  ggplot(aes(sen_len, author, fill = author)) +

  geom_density_ridges() +

  scale_x_log10() +

  theme(legend.position = "none") +

  labs(x = "Sentence length [# characters]")



p3 <- t1 %>%

  mutate(word_len = str_length(word)) %>%

  ggplot(aes(word_len, fill = author)) +

  geom_density(bw = 0.05, alpha = 0.3) +

  scale_x_log10() +

  theme(legend.position = "none") +

  labs(x = "Word length [# characters]")





layout <- matrix(c(1,2,1,3),2,2,byrow=TRUE)

multiplot(p1, p2, p3, layout=layout)

p1 <- 1; p2 <- 1; p3 <- 1; p4 <- 1; p5 <- 1

```



We find:



- A large fraction of `training` sentences are by Edgar Allen Poe, with Lovecraft and Shelley close together on 2nd and 3rd place.



- Poe's sentences appear to be on average a bit shorter than the others. Probably because ravens are notoriously succinct in their expressions. Lovecraft's distribution is narrower and peaks towards slightly longer sentences. Tentacles need space, after all.



- In terms of word length, Shelley and Lovecraft are very similar but Poe appears to use slightly longer words in overall. Here we use an overlapping density plot but the overlay between blue (MWS) and green (HPL) is almost perfect, so that you can only see the red of Poe in a different shape





# Characteristic words



*Our fingers tremble in an almost imperceptible way as our mind slowly decents deeper into the realm of the frighteningly fantastic with every turn of page in the heavy books that lie at our feet. We did not notice that through the course of our intial analysis we had sat down on the cool marble floor and put our candle next to us. The candle is shorter now. But we don't really care. We want to know more about this intriguing mystery. Recklessness is slowly taking over.*



*We are anxious to find out which words could point us to which author. The overview wordclouds were a good start, but now we have tasted blood and want to know more.*





## Frequency comparison



This plot shows the frequencies for the overall most popular words split into usage by author:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 4", out.width="100%"}

foo <- t1 %>%

  group_by(word, author) %>%

  count()



bar <- t1 %>%

  group_by(word) %>%

  count() %>%

  rename(all = n)



foo %>%

  left_join(bar, by = "word") %>%

  arrange(desc(all)) %>%

  head(80) %>%

  ungroup() %>%

  ggplot(aes(reorder(word, all, FUN = min), n, fill = author)) +

  #ggplot(aes(word, n)) +

  geom_col() +

  xlab(NULL) +

  coord_flip() +

  facet_wrap(~ author) +

  theme(legend.position = "none")

```



We can already see that some words are almost equally frequent for all authors, such as "time". In contrast, "love" is clearly more used by Shelley than by Lovecraft. The word "half" is only found in Poe and Lovecraft, but not in Shelley's work at a notable frequency.





## Author-dependent word frequencies



We can dive deeper into this fundamental question by directly comparing the relative frequency of word use between authors. In this example, we will compare MWS to EAP and HPL. We also require at least 10 occurences per author for a word to be included:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 5", out.width="100%"}

frequency <-t1 %>%

  count(author, word) %>%

  filter(n > 1e1) %>%

  group_by(author) %>%

  mutate(freq = n / sum(n)) %>% 

  select(-n) %>% 

  spread(author, freq) %>% 

  gather(author, freq, EAP:HPL) %>%

  filter(!is.na(freq) & !is.na(author))



ggplot(frequency, aes(freq, MWS , color = abs(MWS - freq))) +

  geom_abline(color = "gray40", lty = 2) +

  geom_jitter(alpha = 0.1, size = 2.5, width = 0.1, height = 0.1) +

  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +

  scale_x_log10(labels = percent_format()) +

  scale_y_log10(labels = percent_format()) +

  #scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray95") +

  facet_wrap(~author, ncol = 2) +

  theme(legend.position="none") +

  labs(y = "MWS", x = NULL)

```



In these plots, words that are close to the dashed line (of equal frequency) have similar frequencies in the corresponding *authors*. Words that are further along a particular *author* axis (such as “father” for MWS vs EAP) are more frequent for that *author*. The blue-gray scale indicates how different the MWS frequency is from the overall frequency (with higher relative frequencies being lighter). The (slightly jittered) points in the background represent the complete set of (high-frequency) words, whereas the displayed words have been chosen to avoid overlap.



The plots give us another useful overview angle. For instance, they suggest that Mary Shelley uses "heart" more than either of the other two authors, but she uses "beautiful" as more often than Lovecraft but equally often as Poe.





In the same way, we can directly compare Poe vs Lovecraft, if we dare:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 6", fig.height=3.5, out.width="100%"}

frequency <-t1 %>%

  count(author, word) %>%

  filter(n > 1e1) %>%

  group_by(author) %>%

  mutate(freq = n / sum(n)) %>% 

  select(-n) %>% 

  spread(author, freq) %>% 

  gather(author, freq, HPL:MWS) %>%

  filter(!is.na(freq) & !is.na(author) & author == "HPL")



ggplot(frequency, aes(freq, EAP , color = abs(EAP - freq))) +

  geom_abline(color = "gray40", lty = 2) +

  geom_jitter(alpha = 0.1, size = 2.5, width = 0.1, height = 0.1) +

  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +

  scale_x_log10(labels = percent_format()) +

  scale_y_log10(labels = percent_format()) +

  #scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray95") +

  facet_wrap(~author, ncol = 2) +

  theme(legend.position="none") +

  labs(y = "EAP", x = NULL)

```



Looks like Lovecraft is all about "strange", "ancient", "streets"; while Poe is "altogether" a "character" of "ideas".



For a more systematic approach we compute the correlation coefficients for each frequency set, this time without any limitations on the number of occurences:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 6", out.width="100%"}

frequency <- t1 %>%

  count(author, word) %>%

  filter(n > 1e1) %>%

  group_by(author) %>%

  mutate(freq = n / sum(n)) %>% 

  select(-n) %>% 

  spread(author, freq)



frequency %>%

  select(-word) %>%

  cor(use="complete.obs", method="spearman") %>%

  corrplot(type="lower", method="number", diag=FALSE)

```



This plot makes use of the *corrplot* package to visualise the correlation coefficients for each combination of two features. What our burning eyes see here, in the light of the flickering candle, is whether two features are connected so that one changes with a predictable trend if you change the other. The closer this coefficient is to zero the weaker is the correlation. Both 1 and -1 are the ideal cases of perfect correlation and anti-correlation.



We find that our author's vocabularies are mildly correlated with each other. This is not surprising, given that we are looking at the same genre. The differences in correlation coefficients are small, so while we could say that Poe is more similar to Shelley than to Lovecraft, there is not much that separates them from each other.





## A note on word stems



*Many feelings of fear and uneasiness can be reduced to a fundamental existential dread hidden deep in our unconcious mind. Even though we like to live the notion that we are in control, underneath the surface some part of our minds remember that the threads of fate are woven by forces entirely beyond our understanding. For them, we are merely ants without purpose. And their boots won't show mercy if we cross their path.*



In a similar way, we will notice in our analysis that certain words are essentially just variants of basic terms (e.g. "fear", "feared", and "fears"). For some purposes these word variants might obfuscate the meaning we are interested in. Fortunately, we can reduce them to their basic meaning, their *word stem*, using a stemming tool.



As far as I can see, tidytext has currently no native stemming function. Therefore, we will use the `SnowballC` package and its `wordStem` tool. Here we compare the top stemmed words in a similar way as above:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 7", out.width="100%"}

foo <- t1 %>%

  mutate(word = wordStem(word)) %>%

  group_by(word, author) %>%

  count()



bar <- t1 %>%

  mutate(word = wordStem(word)) %>%

  group_by(word) %>%

  count() %>%

  rename(all = n)



foo %>%

  left_join(bar, by = "word") %>%

  arrange(desc(all)) %>%

  head(60) %>%

  ungroup() %>%

  ggplot(aes(reorder(word, all, FUN = min), n, fill = author)) +

  #ggplot(aes(word, n)) +

  geom_col() +

  xlab(NULL) +

  coord_flip() +

  facet_wrap(~ author) +

  theme(legend.position = "none")



```



We see that "love" remains a theme predominantly used by Shelley, whereas "house" and its relatives find greater use in Lovecraft's work. And Poe has things "appear" more frequently. Here we will continue our analysis on the original words since some of the sublety in writing styles might be washed out by a stemming approach.





## The Day of the TF-IDFs



*Suddenly, we feel the hairs on your neck rising. Although we are still staring at the mysterious pages in front of us we know that something has changed. The room feels colder. Our candle is flickering in a draft that wasn't there before. Our heart quickens. We have to fight the urge to jump up and run away. Very slowly we rise our gaze, not knowing what to expect. And we find ourselves eye to eye with one of the most powerful and fabled creatures in all of tidyverse lore: a TF-IDF.*



*We are suprised at how calm we are on the outside, as we frantically try to remember what our old master told us ages ago. Words have power through their meaning. If we can recall what the meaning behind the TF-IDF is, then it might even help us. If not ...; well, better not think about that. As the panic slowly ebbs away we suddenly know with great clarity:*



- TF stands for *term frequency*; essentially how often a word appears in the text. This is what we measured above. A list of stop-words can be used to filter out frequent words that likely have no impact on the question we want to answer (e.g. "and" or "the"). However, using stop words might not always be an elegant approach. IDF to the rescue.



- IDF means *inverse document frequency*. Here, we give more emphasis to words that are rare within a collection of documents (which in our case means the entire text data.)



- Both measures can be combined into `TF-IDF`, a heuristic index telling us how frequent a word is in a certain context (here: a certain `author`) within the context of a larger document (here: all `authors`). You can understand it as a normalisation of the relativ text frequency by the overall document frequency. This will lead to words standing out that are characteristic for a specific `author`, which is pretty much what we want to achieve in order build a prediction model.



Our `tidytext` repertoire includes the magical tool `bind_tf_idf` to extract these metrics from a tidy data set that contains words and their counts per `author`:



```{r}

frequency <-t1 %>%

  count(author, word)



tf_idf <- frequency %>%

  bind_tf_idf(word, author, n)

```





And from one moment to the other we have all the might of the TF-IDF at our fingertips. Through its eyes, we see a strange new world. These are the most characteristic words for each author:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 7", out.width="100%"}

tf_idf %>%

  arrange(desc(tf_idf)) %>%

  mutate(word = factor(word, levels = rev(unique(word)))) %>%

  top_n(30, tf_idf) %>%

  ggplot(aes(word, tf_idf, fill = author)) +

  geom_col() +

  labs(x = NULL, y = "TF-IDF values") +

  theme(legend.position = "top", axis.text.x  = element_text(angle=45, hjust=1, vjust=0.9))

  

```



We find:



- There are a lot of names, like "Adrian" or "Raymond", especially for Mary Shelley. Spotting a mention of "Perdita" anywhere in our data is almost a surefire way to identify her writing. MWS appears to be the most characteristic writer among our famous trio.



- Lovecraft loves to craft stories about "bearded" "attics", wherease Poe has us pardon his French "monsieur" on the way to "Jupiter". 





*We are starting to have a good feeling about this, but the TF-IDF seems worried. It is a moody beast and we better treat it well.*



Maybe it would want to give us an overview of the most characteristic terms for each `author`. Even if we don't like the idea, we once again align our mind with the bottomless concience of the TF-IDF to see the world through its eyes:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 8", out.width="100%"}

tf_idf %>%

  arrange(desc(tf_idf)) %>%

  mutate(word = factor(word, levels = rev(unique(word)))) %>%

  group_by(author) %>%

  top_n(20, tf_idf) %>%

  ungroup() %>%  

  ggplot(aes(word, tf_idf, fill = author)) +

  geom_col() +

  labs(x = NULL, y = "tf-idf") +

  theme(legend.position = "none") +

  facet_wrap(~ author, ncol = 3, scales = "free") +

  coord_flip() +

  labs(y = "TF-IDF values")



```



We find:



- Naturally, we recover our top ranking words such as "Perdita" or "Arkham". But here we also see that Mary Shelley liked the word "until" while H P Lovecraft wrote about "legends".



- Edgar Allan Poe's work contains some interesting technical terms such as "diameter" or "velocity". And "lollipop", which is, admittedly, somewhat less scary than expected. Unless it is a lollipop made from ... blood!



*We try to keep this (somewhat) happy thought in our mind to comfort us, because we suddenly realise that the TF-IDF has left as quickly and quietly as it had arrived. We had found an unlikely companion for our journey, but now the beast is gone. Could it be at all possible that it was scared by something?*



*At least we had the whereabouts to sketch down what we saw through the eyes of the TF-IDF, so that our insights won't be lost. But as we sort through the ink-blotted sheets of paper we realise that the TF-IDF had left a cryptic note behind before it vanished. We hold it close to our candle to read the pale words and suddenly a shiver runs down our spine. The message reads*



    Single words alone won't save you here





# Word pair frequencies - let bigrams be bigrams



*The thought echos through our mind: what did "single words" mean? And why won't they be enough ... ? Then the realisation: it's about groups of words! Not only the words themselves matter but also their context. Despite the sudden feeling of loneliness we brace ourselves against the darkness and continue our analysis from a new point of view.*



In a similar way as measuring the frequencies of individual words we can also study the properties of groups of words that occur together. This gives us an idea about the (typical) relationships between words in a certain document.



Tidytext, and other tools, use the concept of the *n-gram*, which *n* being the number of adjacent words we want to study as a group. For instance, a *bigram* is a pair of two words.





## Bigrams



Let's start with those bigrams. We can extract all of those pairs in a very similar way as the individual words using our magical *tidytext* scissors. Here are a few random examples that will change every time we run this part:



```{r}

t2 <- train %>% select(author, text) %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)

sample_n(t2, 5)

```



In order to filter out the stop words we need to *separate* the bigrams first, and then later *unite* them back together after the filtering. *Separate/unite* are also the names of the corresponding *dplyr* functions:



```{r}

bi_sep <- t2 %>%

  separate(bigram, c("word1", "word2"), sep = " ")



bi_filt <- bi_sep %>%

  filter(!word1 %in% stop_words$word) %>%

  filter(!word2 %in% stop_words$word)



# for later

bigram_counts <- bi_filt %>%

  count(word1, word2, sort = TRUE)



t2 <- bi_filt %>%

  unite(bigram, word1, word2, sep = " ")

```





Now we can extract the TF-IDF values with the skills we have learned from the beast:



```{r}

t2_tf_idf <- t2 %>%

  count(author, bigram) %>%

  bind_tf_idf(bigram, author, n) %>%

  arrange(desc(tf_idf))

```





And then we plot the bigrams with the highest TF-IDF values per *author* and we see that ...



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 9", out.width="100%"}

t2_tf_idf %>%

  arrange(desc(tf_idf)) %>%

  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%

  group_by(author) %>%

  top_n(10, tf_idf) %>%

  ungroup() %>%  

  ggplot(aes(bigram, tf_idf, fill = author)) +

  geom_col() +

  labs(x = NULL, y = "TF-IDF values") +

  theme(legend.position = "none") +

  facet_wrap(~ author, ncol = 3, scales = "free") +

  coord_flip()

```



Um... I have the indistinct feeling that both Poe and Lovecraft are laughing at us. If there is only one thing in the world that should make you feel uneasy, it's probably laughter from those two.



We also find:



- Besides cruel humour, for Poe it's all about "chess players" and "tea pots". We've also got a few more names and, apparently, a fair share of "Orang Utan" appearances.



- Lovecraft sets the scence with "ancient houses" and "shunned houses" during the "seventeenth century". Also he has a couple of characteristic character names.



- So has Mary Shelly, who seems to really like "Lord Raymond". Well, everybody loves Raymond, don't they? We also find a few turns of phrase that are typical for her, such as "fellow creatures", "hours passed", or "ill fated".  *Let's hope that the latter is not an omen for our own expedition into the heart of the darkness ...*





## Like the trap of a spider - Networks of bigrams



Using these bigrams we can explore the way that word are inter-related within a text. Step by step, we can visualise their connections with other words by building a *network*. A network of words is a combination of connected nodes. Much like a hunting spider constructs its web to trap careless prey, so are we being led deeper and deeper into the bleak wilderness of our author's imagination.



Here we use the *igraph* package to build the network and the *ggraph* package to visualise it within the context of the *tidyverse*:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 10", out.width="100%"}

bigram_graph <- bigram_counts %>%

  filter(n > 6) %>%

  graph_from_data_frame()



set.seed(1234)



a <- grid::arrow(type = "closed", length = unit(.1, "inches"))



ggraph(bigram_graph, layout = "fr") +

  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,

                 arrow = a, end_cap = circle(.07, 'inches')) +

  geom_node_point(color = "lightblue", size = 3) +

  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +

  theme_void()

```



We find, that many bigrams stand alone. For example "shrill voice". But there are also cases where one word connects to multiple others, such as the term "human" in "human race", "human agency", and "human nature".



Here the arrows show the direction of the word relation (e.g. "native country" rather than "country native"). Transparency is applied to these linking arrows according to the frequency of their occurence (rarer ones are more transparent).





## Spinning their webs - Author-specific networks



We can make the same kind of network plots for each individual *author* to investigate their specific terms of importances. In order for this to work, we need to extract the bigram counts separately. For this, we build a short helper function, to which we also assign the flexibility to extract how many bigram combinations to display in the plot. Here, the first parameter of the function is the name of the *author* and the second is the lower limit for the bigram word combinations.



```{r}

# input parameters: author, minimum count for bigram graph

plot_bigram_net_author <- function(aname, bimin){



  foo <- t2 %>%

    filter(author == aname)



  bar <- foo %>%

    separate(bigram, c("word1", "word2"), sep = " ")

  

  bi_filt <- bar %>%

    filter(!word1 %in% stop_words$word) %>%

    filter(!word2 %in% stop_words$word)

  

  bigram_graph <- bi_filt %>%

    count(word1, word2, sort = TRUE) %>%

    filter(n > bimin) %>%

    graph_from_data_frame()

  

  set.seed(1234)



  a <- grid::arrow(type = "closed", length = unit(.1, "inches"))



  ggraph(bigram_graph, layout = "fr") +

    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,

                 arrow = a, end_cap = circle(.07, 'inches')) +

    geom_node_point(color = "lightblue", size = 3) +

    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +

    theme_void()

}

  

```





*We now have the power to conjour up network plots at will; even if their shape reminds us of a vast and cold universe in which we are but a speck of dust. Helpless against the every-changing tides of fate and tragedy. Merely a pawn on an hyper-dimensional chess board governed by rules we can't even hope to understand.*



We try to banish these thoughts from our mind and focus on sketching out the plots. Here they are. Carefully laid out; just like the plans of mice and men ... . (Sorry, wrong genre)



We try to keep the network plots relatively sparse, so that we can see the important connections more clearly. Feel free to experiment with larger numbers of bigrams here.



Edgar Allan Poe first. He's the original one who knocks:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 11", out.width="100%"}

plot_bigram_net_author("EAP",4)

```



We see a characteristic French angle together with a tendency for technical terms and measurements.





Now it's Mary Shelley's turn of phrase:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 12", out.width="100%"}

plot_bigram_net_author("MWS",3)

```



Here we find a focus on communities ("country people", "native country", "fellow creatures") as well as the "heart" and the "eyes".





And finally Mr Howard Phillips Lovecraft:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 13", out.width="100%"}

plot_bigram_net_author("HPL",4)

```



There are not many multi-node connections here, but we find an emphasis on "door"s, "house"s, and "street"s.



*And could it be merely coincidence that at the very centre of his net there is the term "lurking fear"? What new horrors could possibly lurk in the next chapter of our analysis?*



*By now, our candle should have been close to burning down; flickering with its last dying breaths. Instead, it is burning brighter than before - almost as if it is drawing its light from a different source than the inaminate wax. The room around us might as well have disappeared, so little attention are we paying to it. And the marble floor, once cool to the touch, now seems more like the infinitesimal grains of sand in an endless, scorched desert.*



*We can't keep our eyes off the ancient pages in our quest to decifer the secret meaning behind all this madness. The suspense is terrible. I hope it will last.*





## Trigrams



*Three is a magical number. A terrible number. There were 3 witches to foretell Macbeth his blood-drenched destiny. The devil hound Cerberus has 3 heads. The number of the beast is 3 times the number 3+3. All these warning signs try to reach our concience as we prepare to repeat the same analysis we had done for bigrams on their cousins thrice removed: trigrams.*



*Blind for knowledge, yielding to the call of power just like the sorcerer's apprentice, we continue our study. We crave to know more. A little spark of reason and self-preservation is trying to make itself heard against the raging thirst in our brains, but it burns ever weaker as the candle, is it still a candle?, shines brighter and brighter.*



Extracting trigrams follows the same procedure as for bigrams. Again we filter out stop words and include a few random examples:



```{r}

t3 <- train %>% select(author, text) %>% unnest_tokens(trigram, text, token = "ngrams", n = 3)



tri_sep <- t3 %>%

  separate(trigram, c("word1", "word2", "word3"), sep = " ")



tri_filt <- tri_sep %>%

  filter(!word1 %in% stop_words$word) %>%

  filter(!word2 %in% stop_words$word) %>%

  filter(!word3 %in% stop_words$word)



# for later

trigram_counts <- tri_filt %>%

  count(word1, word2, word3, sort = TRUE)



t3 <- tri_filt %>%

  unite(trigram, word1, word2, word3, sep = " ")



sample_n(t3, 5)

```



And here is the corresponding TF-IDF plot for the most characteristic terms:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 14", out.width="100%"}

t3_tf_idf <- t3 %>%

  count(author, trigram) %>%

  bind_tf_idf(trigram, author, n) %>%

  arrange(desc(tf_idf))



t3_tf_idf %>%

  arrange(desc(tf_idf)) %>%

  mutate(trigram = factor(trigram, levels = rev(unique(trigram)))) %>%

  group_by(author) %>%

  top_n(5, tf_idf) %>%

  ungroup() %>%  

  ggplot(aes(trigram, tf_idf, fill = author)) +

  geom_col() +

  labs(x = NULL, y = "TF-IDF values") +

  theme(legend.position = "none") +

  facet_wrap(~ author, ncol = 3, scales = "free") +

  coord_flip()

```





We find:



- More scary laughter and characteristic names from Poe and Lovecraft. Feel free to admit that you also read "Eric Moreland Clap**ton**" at first glance in HPL's list. I like the imagery of a "horned waning moon".



- Curiously, Mary Shelley does not seem to have particularly typical phrases she repeats more often than others. The ones she does use suggest a penchant for body language, especially the eyes.



- Most importantly, though, we find out that Raymond was from [Galifrey](https://en.wikipedia.org/wiki/Time_Lord). That might explain why he's so popular and why he manages to exert such a strong influence on Shelley's writing.





# Shocking sentiments



*We pause to relax our strained eyes for a moment. Lifting our gaze from the curved letters on the old paper takes a surprising amount of will power, but we still manage to do it. We find ourselves in an aura of diffuse light, created by our candle that appears to be hovering slightly above the floor. Outside the light, not much seems to exist. It is as if a thick mist has decended upon us to cut all ties to the outside world.*



*As we close our eyes, we suddenly see the letters from the book appear as burning shapes behind our eyelids. Our eyes snap open again. A strange wisper fills our head. It seems to come from within our ears and tells tales of fear and decay. Other voices chime in with conflicting messages that only compound our confusion. Some seem hostile, some downright insane. But some others could be almost friendly? We have all but forgotten that we are still holding the book in our hands as we franctically search for a way to distinguish the eery messengers.*



`Sentiment analysis` allows us to capture the `tone` of a statement, sentence, or book by analysing the frequency of `positive vs negative` words within it. This is done through the use of a pre-compiled `sentiment` lexicon, the entries of which are categorised into various degrees of positive or negative meaning. `Tidytext` lets us make use of several sentiment data bases. The analysis in this chapter mostly applies to individual words.





## Positive vs negative words



First, we are joining the `sentiment lexicon` to our tidy words data to assign each term its positive or negative sentiment. Then we plot the result for the different authors:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 14", out.width="100%"}

t1 %>%

  filter(author == "MWS") %>%

  inner_join(get_sentiments("bing"), by = "word") %>%

  count(word, sentiment, sort = TRUE) %>%

  ungroup() %>%

  group_by(sentiment) %>%

  top_n(10, n) %>%

  ungroup() %>%

  mutate(word = reorder(word, n)) %>%

  ggplot(aes(word, n, fill = sentiment)) +

  geom_col(show.legend = FALSE) +

  facet_wrap(~sentiment, scales = "free_y") +

  labs(y = "Contribution to negative/positive sentiment", x = NULL) +

  coord_flip() +

  ggtitle("Mary Shelley - Sentiment analysis")

```



We find:



- MWS used positive words almost as much as negative ones. There's much "love", "happiness", and "joy" to counteract all the "death", "fear", and "misery" in the world.



- Even derivations of the word "love", such as "loved" and "beloved" make it into the top 10.



- "Death" is clearly her most common negative word, but the unholy trinity of "fear", "misery", and "despair" underlines a strong emotional focus.





```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 15", out.width="100%"}

t1 %>%

  filter(author == "HPL") %>%

  inner_join(get_sentiments("bing"), by = "word") %>%

  count(word, sentiment, sort = TRUE) %>%

  ungroup() %>%

  group_by(sentiment) %>%

  top_n(10, n) %>%

  ungroup() %>%

  mutate(word = reorder(word, n)) %>%

  ggplot(aes(word, n, fill = sentiment)) +

  geom_col(show.legend = FALSE) +

  facet_wrap(~sentiment, scales = "free_y") +

  labs(y = "Contribution to negative/positive sentiment", x = NULL) +

  coord_flip() +

  ggtitle("H P Lovecraft - Sentiment analysis")

```



We find:



- Not much of a positive mood in Lovecraft's works; that's for sure.



- The difference to Shelley is striking. There's no "love" or "happiness" among the top positive words and instead we get "fresh", "led", and "silent" as in "he was led in silence to a freshly dug grave".



- Lovecraft's most frequent negative word is "strange"; which by all accounts is pretty much what we would expect from Cthulhu's creator.





```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 16", out.width="100%"}

t1 %>%

  filter(author == "EAP") %>%

  inner_join(get_sentiments("bing"), by = "word") %>%

  count(word, sentiment, sort = TRUE) %>%

  ungroup() %>%

  group_by(sentiment) %>%

  top_n(10, n) %>%

  ungroup() %>%

  mutate(word = reorder(word, n)) %>%

  ggplot(aes(word, n, fill = sentiment)) +

  geom_col(show.legend = FALSE) +

  facet_wrap(~sentiment, scales = "free_y") +

  labs(y = "Contribution to negative/positive sentiment", x = NULL) +

  coord_flip() +

  ggtitle("E A Poe - Sentiment analysis")

```



We find:



- Also Poe uses positive words sparingly, but at least there is "love" in his world. Again, there are several technical-sounding words such as "precisely" or "sufficient(ly)".



- It is interesting that his most-used negative word is "doubt" - pointing to a different kind of horror of the imagination.





## Comparison wordcloud



An alternative way of portraying the most important words for each sentiment is through our favourite word-clouds. This uses the `reshape` library. Here are all the authors combined as an example:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 17", out.width="100%"}

t1 %>%

  inner_join(get_sentiments("bing"), by = "word") %>%

  count(word, sentiment, sort = TRUE) %>%

  acast(word ~ sentiment, value.var = "n", fill = 0) %>%

  comparison.cloud(colors = c("#F8766D", "#00BFC4"), max.words = 50)

```



We can see "love" and "death" squaring up against one another; locked in an eternal battle in which "beauty" counters "fear" and "evil" rages against "joy".



*In our own mind, feverish in its quest for understanding, the sentiments echo in an distorted and disfigured way.*





## The negativity index



*From within the ancient texts a wave of negativity is washing over us, drowning our mind in sentiments of fear and terror. Instinctively, we feel that we don't have the power to withstand the bombardement. All we can do to prevent being consumed by madness is to focus on the feelings. Give our brains a defined task and hope that the negativity will eventually subside.*



Here we assign an index of negativity to each author. First, we collect all the negative words and count their fraction among the sample of negative plus positive terms. We then go a step further and assign a "negativity fraction" to each sentence; defined in the same way as the other index: `# negative / (# negative + # positive)`. We plot the distribution of these negativity indeces for the three authors:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 18", fig.height=3.5, out.width="100%"}

p1 <- t1 %>%

  inner_join(get_sentiments("bing"), by = "word") %>%

  ggplot(aes(author, fill = sentiment)) +

  geom_bar(position = "fill")



p2 <- t1 %>%

  inner_join(get_sentiments("bing"), by = "word") %>%

  group_by(author, id, sentiment) %>%

  count() %>%

  spread(sentiment, n, fill = 0) %>%

  group_by(author, id) %>%

  summarise(neg = sum(negative),

            pos = sum(positive)) %>%

  arrange(id) %>%

  mutate(frac_neg = neg/(neg + pos)) %>%

  ggplot(aes(frac_neg, fill = author)) +

  geom_density(bw = .2, alpha = 0.3) +

  theme(legend.position = "right") +

  labs(x = "Fraction of negative words per sentence")



layout <- matrix(c(1,2),1,2,byrow=TRUE)

multiplot(p1, p2, layout=layout)

```



We find:



- H P Lovecraft's texts are on average notably more negative than the author's works: Only about 25% positive words vs around 40% for Poe and Shelley.



- And also the distribution of the fraction of negative words per sentence is clearly skewed towards larger values for HPL (green) than in the case of MWS and EAP. The difference between Shelley and Poe is more subtle. The fraction of negative words in Mary Shelley's work rises gradually toward larger values, whereas for Edgar Allan Poe is goes more into a direction of "all or nothing": his texts show an almost bimodal distribution with peak at low and high negativity, respectively.



*This last observation shows Poe's understanding of despair being enhanced by brief periods of positivity that are inevitably crushed by the relentless darkness of life. As we feel our mind being overtaken by fear and misery all we can do is to focus our intellect on analysing this transition. Is it already too late for anything else?*





## Negated negativity



As so often in life, context is important. So far, our study of negative terms did not take into account the other words that preceded them. This is important, because "not bad" means "good", or at least good-ish, and therefore a positive sentiment instead of a negative one.



Here we show the impact of this negated negativity by extracting the most popular words that are preceded by the word "not" (i.e. "not alone" or "not pleasant"). We analyse the entire `training` data in this way, as well as every author on their own. As above, positive words are shown in blue, negative ones in red:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 19", out.width="100%"}

bi_sep <- train %>%

  select(author, text) %>%

  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%

  separate(bigram, c("word1", "word2"), sep = " ")



p1 <- bi_sep %>%

  filter(word1 == "not") %>%

  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%

  count(word1, word2, score, sort = TRUE) %>%

  ungroup() %>%

  mutate(contribution = n * score) %>%

  arrange(desc(abs(contribution))) %>%

  head(15) %>%

  mutate(word2 = reorder(word2, contribution)) %>%

  ggplot(aes(word2, n * score, fill = n * score > 0)) +

  geom_col(show.legend = FALSE) +

  xlab("") +

  ylab("Sentiment score * number of occurrences") +

  coord_flip() +

  theme(plot.title = element_text(size=11)) +

  ggtitle("All authors - Words preceded by the term 'not'")



p2 <- bi_sep %>%

  filter(author == "HPL") %>%

  filter(word1 == "not") %>%

  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%

  count(word1, word2, score, sort = TRUE) %>%

  ungroup() %>%

  mutate(contribution = n * score) %>%

  arrange(desc(abs(contribution))) %>%

  head(15) %>%

  mutate(word2 = reorder(word2, contribution)) %>%

  ggplot(aes(word2, n * score, fill = n * score > 0)) +

  geom_col(show.legend = FALSE) +

  xlab("") +

  ylab("Sentiment score * number of occurrences") +

  coord_flip() +

  theme(plot.title = element_text(size=11)) +

  ggtitle("HPL - Words preceded by the term 'not'")



p3 <- bi_sep %>%

  filter(author == "MWS") %>%

  filter(word1 == "not") %>%

  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%

  count(word1, word2, score, sort = TRUE) %>%

  ungroup() %>%

  mutate(contribution = n * score) %>%

  arrange(desc(abs(contribution))) %>%

  head(15) %>%

  mutate(word2 = reorder(word2, contribution)) %>%

  ggplot(aes(word2, n * score, fill = n * score > 0)) +

  geom_col(show.legend = FALSE) +

  xlab("") +

  ylab("Sentiment score * number of occurrences") +

  coord_flip() +

  theme(plot.title = element_text(size=11)) +

  ggtitle("MWS - Words preceded by the term 'not'")



p4 <- bi_sep %>%

  filter(author == "EAP") %>%

  filter(word1 == "not") %>%

  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%

  count(word1, word2, score, sort = TRUE) %>%

  ungroup() %>%

  mutate(contribution = n * score) %>%

  arrange(desc(abs(contribution))) %>%

  head(15) %>%

  mutate(word2 = reorder(word2, contribution)) %>%

  ggplot(aes(word2, n * score, fill = n * score > 0)) +

  geom_col(show.legend = FALSE) +

  xlab("") +

  ylab("Sentiment score * number of occurrences") +

  coord_flip() +

  theme(plot.title = element_text(size=11)) +

  ggtitle("EAP - Words preceded by the term 'not'")



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



We find:



- Once more, Lovecraft is the most negative of our superb but small group of authors. His work contains many more occurences of negating positive words such as "help" or "pleasant".



- Mary Shelley, on the other hand, appears to negate negative words more often; most prominently "not die". The appearance of "hope" in the list of the positive negated words is characteristic for her.



- Also Edgar Allan Poe applies "not" more often in connection with negative words. His strong usage of "not fail" is notable. Fear of failure is a formidable personal demon that brings with it a whole army of real and imaginary terrors.



*The stream of sentient sentiments slowly subsides. Our own concience resurfaces from the tsunami of feelings. We are able to hear our own thoughts again, and we take a deep breath. The mist around us has cleared and we are able to see the wooden shelfs of a library surrouding us. Next to us, a candle has burned down and left only a small ceramic plate behind.*



*We have the strange feeling that were supposed to be holding something in our hands, but nothing is there. We slowly stand up and look around. The long corridors are empty. Nothing unusual is to be seen. We feel a little bit dizy and use our hand to steady us against a nearby shelf. Just below our skin, we can see the familiar stream of words pumping through our veins. The back and forth of the glowing letters is as reassuring as always. We steady ourselves and walk along the empty shelfs towards the centre of the library.*



*Back in the real world, our body is slumped over the ancient book in our motionless hands. All the pages are blank now.*





# Feature engineering idea: nomen est omen



*The wood of the shelfs feels old and dry under our fingers as we absentmindedly touch them on our way through the empty cooridors. Everything seems to have been long disbandend, but no dust has settled on any of the boards as if they were only brought into existence a blink of an eye ago. The stone tiles of the floor are cracked. Our boots make no sounds as we walk. The air tastes drained of vitality.*



*Our swirling thoughts come to an abrupt stop as we notice a lone book sitting at the edge of a shelf a few steps in front. It is leather bound and sends a brief electric shock through our arm as we reach out to grab it. After a short hesitation our curiousity takes over and we touch it again. No shock this time, only a warm, tingling sensation that makes the hairs on our arm stand up. The title of the book is "Nameless Evil".*



We have already noticed that our three authors can be identified by the names of their most prominent characters; with Mary Shelley writing about "Raymond" or Lovecraft about "Herbert West". But what about names in general? Are some authors more likely to use names under certain circumstances? After sentence or character length this is one of our first feature-engineering ideas on our quest for knowledge



In order to determine whether a token is a name we will use the `babynames` [package](https://cran.r-project.org/web/packages/babynames/index.html) which includes given names from the years 1880 to 2015, provided by the US Social Security Administration. Here we plot the general name-usage statistics and those for more specific circumstances. The error bars indicate the 95% confidence level from binomial statistics:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 20", out.width="100%"}

babynames <- babynames %>%

  filter(n > 500)



t1 <- t1 %>%

  mutate(name = (word %in% str_to_lower(babynames$name)))



p1 <- t1 %>%

  group_by(author, name) %>%

  count() %>%

  spread(name, n) %>%

  mutate(frac_name = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  ggplot(aes(author, frac_name, col = author)) +

  geom_point() +

  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.5, size = 0.7) +

  theme(legend.position = "none") +

  labs(y = "First Names [%]")



p2 <- t1 %>%

  filter(name) %>%

  distinct(word, author) %>%

  ggplot(aes(author, fill = author)) +

  geom_bar() +

  theme(legend.position = "none") +

  labs(y = "# distinct first names")





# setup for beginning/end of sentence tokens

foo <- train %>%

  unnest_tokens(word, text) %>%

  rownames_to_column() %>%

  mutate(rowname = as.integer(rowname)) %>%

  anti_join(stop_words, by = "word") %>%

  arrange(rowname)



not_start_id <- foo %>%

  select(id) %>%

  duplicated()



not_end_id <- foo %>%

  arrange(desc(rowname)) %>%

  select(id) %>%

  duplicated()



p3 <- foo %>%

  mutate(start_id = !not_start_id) %>%

  filter(start_id) %>%

  mutate(name = (word %in% str_to_lower(babynames$name)) &

           word != "content" & word != "art" & word != "lovely") %>%

  group_by(author, name) %>%

  count() %>%

  spread(name, n) %>%

  mutate(frac_name = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  ggplot(aes(author, frac_name, col = author)) +

  geom_point() +

  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.5, size = 0.7) +

  theme(legend.position = "none") +

  labs(y = "Name at start of sentence [%]")



p4 <- foo %>%

  arrange(desc(rowname)) %>%

  mutate(end_id = !not_end_id) %>%

  filter(end_id) %>%

  mutate(name = (word %in% str_to_lower(babynames$name))) %>%

  group_by(author, name) %>%

  count() %>%

  spread(name, n) %>%

  mutate(frac_name = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  ggplot(aes(author, frac_name, col = author)) +

  geom_point() +

  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.5, size = 0.7) +

  theme(legend.position = "none") +

  labs(y = "Name at end of sentence [%]")





layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



We find:



- Mary Shelley uses names significantly more often than the other authors (top left plot). Almost 3% of the tokens in her text (excluding stop words) are common names. And even between Poe (1.5%) and Lovecraft (2.1%) there is a smaller, but still signficant difference.



- However, there is less diversity (top right plot) in Shelley's character cast then for example for Lovecraft who uses nearly 250 distinct names compared to her 150. I'm guessing "Raymond" is to blame here. Poe's cast list is closer in length to Lovecraft.



- The two bottom plots explore the percentage of names that are used at the beginning (left plot) or the the end (right plot) of a sentence. This excludes stop words in determining the first/last token of a sentence. Here the uncertainties are larger since we're dealing with as many words as sentences. For the start token, there is again a pretty decent progression from Poe through Lovecraft to Shelley. In terms of names at the end of the sentence, Lovecraft and Poe show practically identical usage while Shelley is clearly ahead.



- In the process of this analysis I also learnt that "Content" was used as a female baby first name five times in 1962. And "The" was used, according to this data set, five times to name male babies in 1991. If accuraten, then this can easily count as horror if you are the person growing up with such a name. To shake your head some more, feel free to explore the babynames data set for additional gems. Here, we only look at the more common names (> 500 occurences in at least 1 year).



*We have absorbed the information in the "Nameless Evil" book into our brain. It shall be useful in accomplishing our mission. We are briefly trying to remember the origin of this mission, but brush the thought aside as we spot further books on nearby shelfs ahead. Those books could prove equally as helpful as the data about names. And who really cares about the reason for this mission? As long as we are successful everything else matters little. As long as we reach our goal. As long as we bring back the Dark One.*





# Topic models - LDA, DTM



*The time moves different in this place. For every new book that we discover and devoure within the blink of an eye, the moments between books stretch into infinity. Impatience is burning brighter and hotter with every hour that is wasted before the summoning. We know what we have to do - seem to have known it from the moment we were born. Only the path to our destiny remains shrouded in obscurity.*



*All this time we continue to gain knowledge about the dark arts. Reading books on the supernatural promises a cure for the wretched human condition, but so far we are confused about the conflicting messages these books are sending. Different tales of ancient history are buried in different manuscripts. We need to find a way to discover their common spirit unless we want to wander this maze for centuries.*





## LDA and DTM



LDA stands for `Latent Dirichlet allocation`; a method that allows us to perform `topic modelling`. This general term describes an unsupervised classification of documents. LDA treats documents as mixtures of topics, which are in turn defined as mixtures of words. By finding the corresponding clusters, or topics, within these words we can find underlying structure in the data.



`tidytext` has no native LDA module, and we will be using the `topicmodels` package for this analysis. Most text analysis packages in R don't use the tidy text format, but rely on other data structures like document term matrices (`DTM`). Using the `cast` spell of `tidytext` we can convert a tidy data frame into such a sparse document matrix as it is used by the `topicmodels`, `tm`, and other text mining package.



Here, we begin by casting the tidy words data frame (t1) into a DTM:



```{r}

freq <-t1 %>%

  count(id, word)



t1_tm <- cast_dtm(freq, id, word, n)

t1_tm

```



We see that the matrix is largely empty. In a DTM every row is a document (which here correponds to a specific sentence `id`) and every column to a term (i.e. a word). The value of the matrix at a certain position is the number of occurences of that word in this specific document. Normally, this leads to very large matrices which are mostly filled with zeros, since not every word will appear in every document. This is an excerpt of our DTM:



```{r}

inspect(t1_tm[1:5,1:10])

```





We will start to perform LDA for a model with 15 topics. This number is chosen relatively arbitrarily. We can assume that each of our authors did contribute more than one story to our text. We don't know how many different stories there are, but it might still be useful to have the number of topics be divisible by the our number of 3 authors: 



```{r}

t1_lda <- LDA(t1_tm, k = 15, control = list(seed = 1234))

```





Now, we are turning the result into tidy data:



```{r}

t1_topics <- tidy(t1_lda, matrix = "beta")

t1_topics %>% sample_n(5)

```



Here, the *beta* parameter describes the probability that this particular word (or 'term' here) belongs to that topic. Now we are back to a tidy format of one row per observation (= word).



Let's visualise the top terms per topic:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 21", out.width="100%"}

t1_topics %>%

  group_by(topic) %>%

  top_n(5, beta) %>%

  ungroup() %>%

  arrange(topic, -beta) %>%

  mutate(term = reorder(term, beta)) %>%

  ggplot(aes(term, beta, fill = factor(topic))) +

  geom_col(show.legend = FALSE) +

  facet_wrap(~ topic, scales = "free", ncol = 5) +

  coord_flip()

```



Here we see that for instance topic 1 contains words such as "love" and "earth", while topic 3 prominently features "thousand" and topic 5 has the word "beauty". We also see several overlaps between topics such as "eyes" or "time".





To have a better view of the differences between the topics we can look at the terms for which the difference in $\beta$ is largest. This way we can ignore the words that are shared with similar frequency between topics. Here we choose the first 5 topics as examples. Feel free to examine additional differences by forking this script.



We visualise these difference by plotting their log ratios: `log10(topic x / topic y)`. In this way, if a word is 10 times more frequent in topic x the log ratio will be 1, whereas it will be -1 if the word is 10 times more frequent in topic y. This makes for barplots that are nicely symmetric around zero:  



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 22", out.width="100%"}

p1 <- t1_topics %>%

  mutate(topic = paste0("topic", topic)) %>%

  spread(topic, beta) %>%

  filter(topic1 > .001 | topic2 > .001) %>%

  mutate(log_ratio = log10(topic2 / topic1)) %>%

  group_by(direction = log_ratio > 0) %>%

  top_n(5, abs(log_ratio)) %>%

  ungroup() %>%

  mutate(term = reorder(term, log_ratio)) %>%

  ggplot(aes(term, log_ratio, fill = log_ratio > 0)) +

  geom_col() +

  theme(legend.position = "none") +

  labs(y = "Log ratio of beta in topic 2 / topic 1") +

  coord_flip()



p2 <- t1_topics %>%

  mutate(topic = paste0("topic", topic)) %>%

  spread(topic, beta) %>%

  filter(topic2 > .001 | topic3 > .001) %>%

  mutate(log_ratio = log10(topic3 / topic2)) %>%

  group_by(direction = log_ratio > 0) %>%

  top_n(5, abs(log_ratio)) %>%

  ungroup() %>%

  mutate(term = reorder(term, log_ratio)) %>%

  ggplot(aes(term, log_ratio, fill = log_ratio > 0)) +

  geom_col() +

  theme(legend.position = "none") +

  labs(y = "Log ratio of beta in topic 3 / topic 2") +

  coord_flip()



p3 <- t1_topics %>%

  mutate(topic = paste0("topic", topic)) %>%

  spread(topic, beta) %>%

  filter(topic4 > .001 | topic3 > .001) %>%

  mutate(log_ratio = log10(topic4 / topic3)) %>%

  group_by(direction = log_ratio > 0) %>%

  top_n(5, abs(log_ratio)) %>%

  ungroup() %>%

  mutate(term = reorder(term, log_ratio)) %>%

  ggplot(aes(term, log_ratio, fill = log_ratio > 0)) +

  geom_col() +

  theme(legend.position = "none") +

  labs(y = "Log ratio of beta in topic 4 / topic 3") +

  coord_flip()



p4 <- t1_topics %>%

  mutate(topic = paste0("topic", topic)) %>%

  spread(topic, beta) %>%

  filter(topic4 > .001 | topic5 > .001) %>%

  mutate(log_ratio = log10(topic5 / topic4)) %>%

  group_by(direction = log_ratio > 0) %>%

  top_n(5, abs(log_ratio)) %>%

  ungroup() %>%

  mutate(term = reorder(term, log_ratio)) %>%

  ggplot(aes(term, log_ratio, fill = log_ratio > 0)) +

  geom_col() +

  theme(legend.position = "none") +

  labs(y = "Log ratio of beta in topic 5 / topic 4") +

  coord_flip()



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



We find that "Cthulhu" and "pit" are characteristic parts of topic 4. While "beauty" is prominent in topic 5, topic 1 reminds us of our existence in a "dreadful" "universe".



*With our new topic skills, the contents of the occult books we read start to slowly aggregate into clusters. We find ourselves surveying the different aspects of the summoning ritual: The preparations and ingredients. The sacrifices. The spells and incantations. And the surpreme range of suffering and despair that will be unleashed onto the world once the ritual is finished.*



*The Dark One is promising us an eternity of riches and rewards in this life and all others. We are eager to see this vision become reality. Mankind is doomed, but something superior is about to take its place.*





## Sentence topics - a tree(map) house of horror



*Our powers grow by the minute. New facets and intricacies of the summoning ritual fall into place in a sophisticated flourish of pure knowledge. The exillaration of understanding is euphoric.*



In a similar way to the topic probabilities per word we can also assign each sentence (i.e. `id`) to a certain topic and compute the corresponding probabilities. These values, called $\gamma$ here, are derived from the LDA model via tidytext in a similar way as above:



```{r}

t1_tdocs <- tidy(t1_lda, matrix = "gamma")

t1_tdocs %>% arrange(document) %>% dplyr::slice(seq(1234,1239))

```



The probabilities here can also be interpreted as the number of words per sentence id that belong to this topic.



We can create a comprehensive overview of how these topics relate to our three `authors` by building a `treemap` using the `treemapify` [package](https://cran.r-project.org/web/packages/treemapify/index.html).



Here, we can see the frequency of groups of `topics` (coloured boxes labeled in white) for each `author` (labeled in a large black font and separated by grey borders). Topic-group box sizes decrease from the bottom left to the top right of the plot and each subgroup (i.e. `author`) box. The colours of the white-labeled neighbour boxes are identical troughout the plot (e.g. "1" is always red):



```{r  split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 23", fig.height=3.5, out.width="100%"}

t1_tdocs %>%

  left_join(train, by = c("document" = "id")) %>%

  select(-text) %>%

  mutate(topic = as.factor(topic)) %>%

  group_by(document) %>%

  top_n(1, gamma) %>%

  ungroup() %>%

  group_by(author, topic) %>%

  count() %>%

  ungroup() %>%

  ggplot(aes(area = n, fill = topic, label = topic, subgroup = author)) +

  geom_treemap() +

  geom_treemap_subgroup_border() +

  geom_treemap_subgroup_text(place = "centre", grow = T, alpha = 0.5, colour =

                             "black", fontface = "italic", min.size = 0) +

  geom_treemap_text(colour = "white", place = "topleft", reflow = T) +

  theme(legend.position = "null") +

  ggtitle("LDA topics per author")

```



We find:



- Topic 1 is much more important for Shelley than for any other author. For Lovecraft it is one of the least frequent topics.



- Poe's most frequent topic is 8, which is found very late in the ranking of the other authors.



- For Lovecraft, we see that his most frequent topics 7, 4, 11, and 10 find much less use in the works of the others.





*In our deranged mind we can see the words swirling around and assembling into topics that scream of strength and destruction. The path to the summoning becomes clearer. We see a future that will burn in the fires of hell for eternity. Empires will crumble and mayhem will reign. Flames will consume the blind works of mankind and the doors of the underworld will remain open forever. This is the new glorious age of fear and we will be richly rewarded for assisting its birth.*



*Back in our concience, a thought is stirring but we ignore it. We know that the Dark One promises a future without the hubris and decadence of mortal existence. Using our new-found power, we turn our mind to harnessing its creative potential in the service of our master.*





# Feature engineering



*Patterns and concepts that have been hidden under the surface of the ancient library books are suddenly revealed to us. Our unblinking eyes disect every sentence into its atomic constituents and reassemble them in ever new shapes and forms. Whatever terrible secrets the decaying paper holds, we are determined to reveal it.*



After studying the existing features and their topics and sentiments we will now focus on engineering. The `names` feature earlier on was a taste of what this chapter will contain.





## Something wicked this way starts



Our first features will deal with the opening and closing words of each sentence. First, we extract them from the `training` data. Here we **don't** remove stop words:



```{r}

first <- train %>%

  unnest_tokens(word, text) %>%

  rownames_to_column() %>%

  mutate(rowname = as.integer(rowname)) %>%

  group_by(id) %>%

  top_n(-1, rowname) %>%

  ungroup()



last <- train %>%

  unnest_tokens(word, text) %>%

  rownames_to_column() %>%

  mutate(rowname = as.integer(rowname)) %>%

  group_by(id) %>%

  top_n(1, rowname) %>%

  ungroup()

```





As an example, we now plot the top `first words` for the different authors. Here the barplot scales are fixed between the facets so that the frequencies of each word can be compared for the three authors. If a bar is missing in one facet then that means that its frequency was lower than the shortest bar in this facet:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 24", out.width="100%"}

first %>%

  group_by(author, word) %>%

  count() %>%

  ungroup() %>%

  group_by(author) %>%

  top_n(10,n) %>%

  ggplot(aes(word, n, fill = author)) +

  geom_col() +

  theme(legend.position = "none") +

  labs(x = "Frequent first words") +

  coord_flip() +

  facet_wrap(~ author)

```



We find:



- The fact that Edgar Allan Poe really likes to start sentences with "the". I also think that he and Mary Shelley like to start out with "I". Those are the two most frequent words for HP Lovecraft, too; but he doesn't use them nearly as often as the other two authors.



- Lovecraft does not like using "this" and "we" at the beginning of a sentence quite as much as the others do. In general, his distribution is flatter, suggesting a higher diversity in opening words.





Of course we know just the right tool to take this analysis further and extract the most author-specific openings:



*The mythical TF-IDF had gifted us with ability to identify rare and characteristic symbols in ancient texts. We feel its power pulsating through our blood as we focus on decoding these new enigmas.*



We first extract the TF-IDF informatin for the `first` and `last` word data:



```{r}

tf_idf_first <- first %>%

  count(author, word) %>%

  bind_tf_idf(word, author, n)



tf_idf_last <- last %>%

  count(author, word) %>%

  bind_tf_idf(word, author, n)

```





And then we plot it in a similar way as during the previous TF-IDF chapter:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 25", out.width="100%"}

tf_idf_first %>%

  arrange(desc(tf_idf)) %>%

  mutate(word = factor(word, levels = rev(unique(word)))) %>%

  group_by(author) %>%

  top_n(10, tf_idf) %>%

  ungroup() %>%  

  ggplot(aes(word, tf_idf, fill = author)) +

  geom_col() +

  labs(x = NULL, y = "tf-idf") +

  theme(legend.position = "none") +

  facet_wrap(~ author, ncol = 3, scales = "free") +

  coord_flip() +

  labs(y = "TF-IDF values")

```



We find:



- The openings "later", "old", and "instead" are tell-tale signs of Lovecraft's works. Poe likes to use "meantime" and "hereupon", whereas Shelley prefers "alas". Specific character names also make an appearance here.



- The presence of the word "chapter" in Shelley's sentences might indicate that those still contain some structuring work that we don't have in the other author's samples.



```{r}

train %>%

  filter(str_sub(text, start = 1, end = 7) == "Chapter") %>%

  mutate(sample = str_sub(text, start = 1, end = 60)) %>%

  select(-text, -id) %>%

  head(5)

```



- Similarly, the "p" and "v" in Poe's texts are probably indicators of dialog:



```{r}

train %>%

  filter(str_sub(text, start = 1, end = 2) == "P.") %>%

  mutate(sample = str_sub(text, start = 1, end = 60)) %>%

  select(-text, -id) %>%

  head(5)

```



*The first moments of the universe were bathed in fire. Our Dark Master is promising that its last moments will see the same infernal reign. And the universe will return to symmetry and beauty. A perfect order through chaos.*



The same analysis as for the first words we can of course also apply to the `last words` of each sentence. Here are the top-ranked TF-IDF entries:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 25", out.width="100%"}

tf_idf_last %>%

  arrange(desc(tf_idf)) %>%

  mutate(word = factor(word, levels = rev(unique(word)))) %>%

  group_by(author) %>%

  top_n(8, tf_idf) %>%

  ungroup() %>%  

  ggplot(aes(word, tf_idf, fill = author)) +

  geom_col() +

  labs(x = NULL, y = "tf-idf") +

  theme(legend.position = "none") +

  facet_wrap(~ author, ncol = 3, scales = "free") +

  coord_flip() +

  labs(y = "TF-IDF values")

```



We find:



- Beyond the specific place and character names (such as "Raymond" or "Arkham") there are a few interesting final words for each author: Poe has "altogether", "minute", "antagonist" and also "machine" which fits into his technical vocabulary.



- Lovecraft's sentences most characteristingly end on "moonlight", "region", or "thing". And for Shelley it's a (Halloween-themed) roller coaster of emotions: "misery", "love", "wretchedness", and "sympathy". This is very consistent with the overall emphasis she puts on the microcosm of feelings that express the fears and struggles of her protagonists.



*Standing in the central hall of the abandonded library our feelings have largely been subdued by the drive to complete our mission and release a cleansing fire into the world. Cold logic determines the necessary steps. Around us the withered wooden shelfs are largely empty. Large piles of books are gathered on the ground; some lie opened on specific pages. We can sense the fear and destruction emmanating from them and it is fuel to our goal.*



*In our hand is an arm-long piece of wood, torn from one of the shelfs. We don't remember taking it. In our two other hands we are holding a heavy spell book with frayed covers. Something feels wrong, but we can't put a finger on it. We whisper a few words. The voice sounds alien to us. But the tip of the wood immediately catches fire. Our unblinking eyes watch it burn for a while before we extinguish it. The preparations have begun.*





## Battle of the sexes



The appearance of "she" and "he" as the opening words in the previous feature motivates us to have a closer look at the gender balance of our texts. Is Mary Shelley as a female author more likely to write about female characters? What is the proportion of male vs female pronouns for the two male writers? Any noticeable difference could become a valuable distinguishing feature.



Here we will plot the difference in the frequencies of the words "woman" vs "man", "she" vs "he", "her" vs "him", and general gender indicators for the 3 authors:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 27", out.width="100%"}

p1 <- train %>%

  unnest_tokens(word, text) %>%

  filter((word == "man") | (word == "woman")) %>%

  mutate(word = as.factor(word)) %>%

  mutate(word = fct_relevel(word, "woman", "man")) %>%

  ggplot(aes(word, fill = author)) +

  geom_bar(position = "dodge")



p2 <- train %>%

  unnest_tokens(word, text) %>%

  filter((word == "he") | (word == "she")) %>%

  mutate(word = as.factor(word)) %>%

  mutate(word = fct_relevel(word, "she", "he")) %>%

  ggplot(aes(word, fill = author)) +

  geom_bar(position = "dodge")



p3 <- train %>%

  unnest_tokens(word, text) %>%

  filter((word == "him") | (word == "her")) %>%

  ggplot(aes(word, fill = author)) +

  geom_bar(position = "dodge")



p4 <- train %>%

  unnest_tokens(word, text) %>%

  mutate(male = ( word == "he" | word == "him" | word == "his" | word == "male" |

                    word == "man" | word == "gentleman" | word == "sir" |

                    word == "lord" | word == "men" )) %>%

  mutate(female = ( word == "she" | word == "her" | word == "hers" | word == "female" |

                    word == "woman" | word == "lady" | word == "madam" |

                    word == "women" )) %>%

  unite(sex, male, female) %>%

  mutate(sex = fct_recode(as.factor(sex), male = "TRUE_FALSE", 

                          female = "FALSE_TRUE", other = "FALSE_FALSE")) %>%

  filter(sex != "other") %>%

  ggplot(aes(sex, fill = author)) +

  labs(x = "Gender indicators") +

  geom_bar(position = "dodge")

  

layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



We find:



- Not much femininity in Lovecraft. Interestingly, he has (just about) the most mentions of "woman" but not much of them seem to be doing something.



- The dominance of "her" over "him" in Shelley's (and Poe's) work is remarkable. Of course, "her" can be the counterpart of "him" as well as "his", but I think that this alone can't explain the full effect.



- Perhaps unsurprisingly, Mary Shelley uses far more "female" words than her two male counterparts. The clear differences here between Poe and Lovecraft show a promising amount of distinguishing power.





*Woman or man will suffer the same fate at the hands of our Dark Master. There is absolute equality in hell.*





After analysing the overall gender split, we will now focus on what "he" or "she" are actually doing in the text. To extract this context we summon again the `bigrams` from our earlier analysis. We then continue to study only those words that follow the term "she" or "he". Feel free to fork this analysis to investigate other gender-specific words:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 28", out.width="100%"}

t2_sep <- train %>%

  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%

  separate(bigram, c("word1", "word2"), sep = " ")



t2_sep %>%

  filter(word1 == "she" | word1 == "he") %>%

  group_by(author, word1, word2) %>%

  count() %>%

  ungroup() %>%

  group_by(author, word1) %>%

  top_n(5,n) %>%

  ggplot(aes(word2, n, fill = author)) +

  geom_col() +

  scale_y_log10() +

  coord_flip() +

  facet_grid(word1 ~ author)



```



We find:



- The overall numbers reflect what we had seen above for the gender balance; i.e. Lovecraft rarely using "she". The fact that his facet for the word "she" contains more bars than for the others is simply due to all the short bars sharing a rank with 2 occurences.



- Men (or male entities) in Lovecraft's works appear to be more associated with "did" and "could" rather than "is" and "has", as we find it for Poe. Interestingly, "did" and "could" are also the terms more frequently following the word "she" in Shelley's work, while Poe's top 5 includes "must".





After this first impression, we will again derive the TF-IDF statistics to dig deeper into the pit of despair:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 29", out.width="100%"}

tf_idf_she <- t2_sep %>%

  filter(word1 == "she") %>%

  count(author, word2) %>%

  bind_tf_idf(word2, author, n)

  

tf_idf_he <- t2_sep %>%

  filter(word1 == "he") %>%

  count(author, word2) %>%

  bind_tf_idf(word2, author, n)

  

p1 <- tf_idf_she %>%

  arrange(desc(tf_idf)) %>%

  mutate(word = factor(word2, levels = rev(unique(word2)))) %>%

  group_by(author) %>%

  top_n(5, tf_idf) %>%

  ungroup() %>%  

  ggplot(aes(word, tf_idf, fill = author)) +

  geom_col() +

  labs(x = NULL, y = "tf-idf") +

  theme(legend.position = "none") +

  facet_wrap(~ author, ncol = 3, scales = "free") +

  coord_flip() +

  labs(y = "TF-IDF for words following the term 'she'")



p2 <- tf_idf_he %>%

  arrange(desc(tf_idf)) %>%

  mutate(word = factor(word2, levels = rev(unique(word2)))) %>%

  group_by(author) %>%

  top_n(5, tf_idf) %>%

  ungroup() %>%  

  ggplot(aes(word, tf_idf, fill = author)) +

  geom_col() +

  labs(x = NULL, y = "tf-idf") +

  theme(legend.position = "none") +

  facet_wrap(~ author, ncol = 3, scales = "free") +

  coord_flip() +

  labs(y = "TF-IDF  for words following the term 'he'")



layout <- matrix(c(1,2),2,1,byrow=TRUE)

multiplot(p1, p2, layout=layout)

```



We find:



- For the words following "she" in the top panels, we see that Mary Shelley's characteristic are 'action' words such as "looked" or "threw". Even "heard" and "replied" suggest a certain level of involvement.



- For Edgar Allan Poe, on the other hand, the characteristic terms are "marry" or "has". There's not much statistics for Lovecraft, but it is noteworthy that "says" and "yelled" are used more by him than by the others.



- In case of the words following "he" in the bottom panels, we get reminded of Poe's creepy "he he he" laughter. Other than that, for him we also find "begged", which doesn not indicate a position of power, as well as "makes" and "shook" which can be more active.



- For Lovecraft it's characteristic to use "he knew", "he wanted", and "he felt". Again, not much action there, while Shelley has "he endeavoured" and "he cried" (which at this time used to mean "he exclaimed" rather than "he wept"). She also uses "he felt", which is an overlap with Lovecraft.





In summary, it appears that Mary Shelley's texts have characters that are more active than for the other two authors, regardless of gender. However, between her and Edgar Allan Poe there remains a difference as to which verbs are assigned to women and which to men (or monsters of the respective gender). HP Lovecraft did not write much about women but seems to include some dialog by them.



*While processing the new information we have begun our work. With the charred point of the wooden piece we are drawing deformed and twisted shapes on the floor of the library. Our hand moves with an unnatural dexterity. The sounds of scratching wood break an otherwise eery silence. From time to time we pause to examine a new page in one of the books.*



*Each of the age-old manuscripts only holds a part of the final incantation. None of them would be strong enough to guard all of it without turning to ash within moments. We can only find fragments of shapes  Our eyes are burning with a feverish glare, but we are working calm and methodical. With every new brush of the coal we feel a wild and vicious spirit awakening. He is calling us. He is guiding our mind through the vast chasm of eternity. He is promising us limitless knowledge. But somewhere in the distance, there is another voice.*





## A (poisoned) heart for stop words



Once again, the results of a previous section motivate the next one. Here, we will follow up on the appearance of words such as "of" or "too". These terms would normally be removed as simple stop words with not much significance attributed to them. However, we have seen indications that in this specific case the humble stop word could prove a value contributor.



First, we select a small sample of stop words and compare their frequencies for the three authors. Note the logarithmic y-axis:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 30", fig.height=3, out.width="100%"}

my_stop <- c("and", "or", "the", "who", "when", "where")



foo <- train %>%

  unnest_tokens(word, text) %>%

  filter(word %in% my_stop)



foo %>%

  ggplot(aes(word, fill = author)) +

  geom_bar(position = "dodge") +

  scale_y_log10() +

  labs(x = "A few example stop words")

```



We find that there are certain differences even among the high frequency words. Here we don't normalise for overall word count, but the deviations between "and" and "or" are still noteworthy. Let's investigate further.



*The infernal voices in our head are promising power, and knowledge, and a supreme reign until the last stars will extinguish. But there are others, too. Whispers of dissent that don't invoke excitement but fear. We try to ignore them as we turn another page to study the cryptic hieroglyphs in a book that is almost falling apart.*





Our next plot will study the most common stop words per author, the fraction of stop words per author in the `training` sample, and the most characteristic TF-IDF words:



```{r  split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 31", out.width="100%"}

t1_stop <- train %>%

  unnest_tokens(word, text) %>%

  mutate(stop = word %in% stop_words$word)



p1 <- t1_stop %>%

  filter(stop) %>%

  group_by(author, word) %>%

  count() %>%

  ungroup() %>%

  group_by(author) %>%

  top_n(5, n) %>%

  ggplot(aes(word, n, fill = author)) +

  geom_col() +

  coord_flip() +

  facet_wrap(~ author) +

  theme(legend.position = "none")



p2 <- t1_stop %>%

  group_by(author, stop) %>%

  count() %>%

  spread(stop, n) %>%

  mutate(frac_stop = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  ggplot(aes(author, frac_stop, col = author)) +

  geom_point() +

  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.5, size = 0.7) +

  theme(legend.position = "none") +

  labs(y = "Fraction of stop words [%]") +

  coord_flip()



p3 <- t1_stop %>%

  filter(stop) %>%

  count(author, word) %>%

  bind_tf_idf(word, author, n) %>%

  arrange(desc(tf_idf)) %>%

  mutate(word = factor(word, levels = rev(unique(word)))) %>%

  group_by(author) %>%

  top_n(5, tf_idf) %>%

  ungroup() %>%  

  ggplot(aes(word, tf_idf, fill = author)) +

  geom_col() +

  labs(x = NULL, y = "tf-idf") +

  theme(legend.position = "none") +

  facet_wrap(~ author, ncol = 3, scales = "free") +

  coord_flip() +

  labs(y = "TF-IDF values")



layout <- matrix(c(1,2,3,3),2,2,byrow=TRUE)

multiplot(p1, p2, p3, layout=layout)

```



We find:



- The top words don't display much variety. The numbers for Poe and Lovecraft are almost identical, while Shelley only uses "I" more frequently than "a".



- There are small differences in the fraction of stop words per author: a few percentage points at most. Still, those variations are significant and might be useful.



- The most characteristic words appear to be useful, too. For Shelley we find "afterwards", "thanks", or "younger", whereas Lovecraft uses "later" and "amongst", and Poe has "don't".





To get a feeling for how important stopwords are in the overall context of the `training` sample, here is a little animation that shows the top 20 TF-IDF words for each author. The animation itself is created via the `gganimate` package. Here we switch back and forth between TF-IDFs with stopwords and without stopwords:



```{r fig.show="hide", split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 32", out.width="100%"}

tf_idf_stop <- t1_stop %>%

  count(author, word) %>%

  bind_tf_idf(word, author, n) %>%

  mutate(stop = if_else(word %in% stop_words$word, "stopwords = TRUE", "stopwords = FALSE"))



p <- tf_idf_stop %>%

  arrange(desc(tf_idf)) %>%

  mutate(word = factor(word, levels = rev(unique(word)))) %>%

  group_by(author) %>%

  top_n(20, tf_idf) %>%

  ungroup() %>%  

  ggplot(aes(word, tf_idf, fill = author, frame = stop, cumulative = TRUE)) +

  geom_col() +

  labs(x = NULL, y = "tf-idf") +

  theme(legend.position = "none",

        axis.text=element_text(size=15),

        axis.title=element_text(size=15),

        title=element_text(size=16),

        strip.text = element_text(size=16),

        axis.text.x  = element_text(angle=45, hjust=1, vjust=0.9)) +

  facet_wrap(~ author, ncol = 1, scales = "free") +

  #coord_flip() +

  labs(y = "TF-IDF values")



gganimate(p, interval = 1.0)

#gganimate(p, "stop_anim_time.gif", ani.width = 1000, ani.height = 600)

```



![Fig. 32](http://gifs.cloud/images/2017/11/04/stop_anim_time.gif)



(Note that we have to employ some trickery of creating the animation externally and including the link, because `gganimate` currently doesn't work in Kernels. This also necessitates all the text size adjustments in the ggplot call.)



We find that stopwords are have an impact on the overall TF-IDF statistics, even if this impact is not very strong. There are still a few words that should improve our analysis; especially the term "later" for HP Lovecraft.



**In summary:** It's probably better not to remove stopwords in this challenge. At the very least, I suggest that you compare your results with and without stopwords to take into account the possibility that they might be useful. 





*A complex octagram is now covering most of the floor in the central hall. Books that have fulfilled their function lie strewn around its perimeter; carelessly tossed aside. The original piece of wood has long been turned to ash. Its successor has already lost half its length as we work to complete the details of the twisted shapes in the outer parts. Our hands move tirelessly. Only occasionally we stop to examine another fragment of the summoning pattern. We are almost there.*



*Once or twice we catch ourselves listening to the faint voices in the far back of our mind. The whispers sound dry and old and weak, and they tell of danger and loss and terrors. We mostly ignore them. They are feeble compared to the roaring orders of our Dark Master who promises a new rule of fire. We continue to follow his orders. But our new skills, boosted by the burning rage in our brain, can't help to notice that those weak whispers speak of a loss that is not measured in mere lifes. Nor in the glory of kingdoms. Instead, it is the fear for loss of goods much more dear to our feverish concience: knowledge and wisdom. An icy shock shoots through our veins and our hands pause mid brush. What have we done?*





## Elegantly eloquent evil: unique words



The question of stopword use is closely connected to the overall vocabulary of the author. How often do our three famous authors repeat words, and how many do they use in total? Let's start with a general overview:



```{r}

words_distinct <- n_distinct(t1_stop$word)

words_all <- nrow(t1_stop)

```



Among the `r words_all` words in our `training` sample (with stopwords but without punctuation) we find `r words_distinct` unique words. This corresponds to `r sprintf("%.1f",words_distinct/words_all*100)`% of cases. For this potential feature we examine the individual statistics for our three spooky authors.



We compute two different metrics: (a) the fraction of unique words in all the sentences and (b) the fraction of unique words per sentence. The latter gives us a distribution for each author that we compare using boxplots:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 33", out.width="100%"}

foo <- t1_stop %>%

  rownames_to_column() %>%

  select(-stop) %>%

  distinct(author, word, .keep_all = TRUE) %>%

  select(-id, -author, -word) %>%

  mutate(dstnct = TRUE)



p1 <- t1_stop %>%

  rownames_to_column() %>%

  left_join(foo, by = "rowname") %>%

  replace_na(list(dstnct = FALSE)) %>%

  group_by(author, dstnct) %>%

  count() %>%

  spread(dstnct, n, fill = 0) %>%

  mutate(frac_distinct = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  ggplot(aes(author, frac_distinct, col = author)) +

  geom_point() +

  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.5, size = 0.7) +

  theme(legend.position = "none") +

  labs(y = "Fraction of distinct words in all sentences [%]") +

  coord_flip()





bar <- t1_stop %>%

  rownames_to_column() %>%

  select(-stop) %>%

  distinct(author, id, word, .keep_all = TRUE) %>%

  select(-id, -author, -word) %>%

  mutate(dstnct = TRUE)



p2 <- t1_stop %>%

  rownames_to_column() %>%

  left_join(bar, by = "rowname") %>%

  replace_na(list(dstnct = FALSE)) %>%

  group_by(author, id, dstnct) %>%

  count() %>%

  spread(dstnct, n, fill = 0) %>%

  mutate(frac_distinct = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  ggplot(aes(author, frac_distinct, color = author)) +

  geom_boxplot() +

  theme(legend.position = "none") +

  labs(y = "Fraction of distinct words per sentence") +

  coord_flip()



layout <- matrix(c(1,2),2,1,byrow=TRUE)

multiplot(p1, p2, layout=layout)

```



We find:



- HP Lovecraft displays the largest vocabulary by almost 2 percentage points. This is a significant different and might in part be due to sentences like this, which mix dialect ("an'", "shud") with demonic language:



```{r}

train %>% filter(id == "id23302") %>% .$text

```



- Among the other two authors, Poe has a notably higher fraction of unique words. The difference is not as large; less than one percentage point, but statistically significant. One reason for this might be his penchant for French characters and settings:



```{r}

train %>% filter(id == "id06009") %>% .$text

```



- In terms of the average fraction of unique words per sentence, there is not much difference between our three authors. The medians are very similar and the boxes largely overlap. Only in Poe's case the inter-quartile range touches 100%.





*Sensing our hesitation, the rage of our Dark Master floods our mind. Without thinking, our hands start moving again to add further warped shapes of horns and tentacles to the sprawling octagram. At the same time we can see enticing images of power and glory appear before our inner eye. Our Master will reign and we will be granted incalculable priviledges for setting him free. But does his infernal kingdom still allow us to quench our thirst for knowledge and understanding? Suddenly a red-hot pain slices through our brain and we drop to our knees. How dare we even ask this question? Our vision blurs as the images in our mind overwhelm the room around us.*





We will visualise another angle on the fraction of unique words per sentence by plotting it against the `sentence length in words`. In addition, we will marginal histograms to the scatter plot. For this we use the function `ggMarginal`, provided by the `ggExtra` [package](https://cran.r-project.org/web/packages/ggExtra/). Note that we plot the x-axis in logarithmic units to allow the histograms to align properly:



```{r fig.align = 'default', warning = FALSE, message=FALSE, fig.cap ="Fig. 34", out.width="100%"}

# sentence length per id

foo <- t1_stop %>%

  group_by(id) %>%

  count() %>%

  rename(sen_len = n)



# join t1 with distince words, compute fractions, join sen_len info

dst_sent <- t1_stop %>%

  rownames_to_column() %>%

  left_join(bar, by = "rowname") %>%

  replace_na(list(dstnct = FALSE)) %>%

  group_by(author, id, dstnct) %>%

  count() %>%

  spread(dstnct, n, fill = 0) %>%

  mutate(frac_distinct = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  left_join(foo, by = "id")



p <- dst_sent %>%

  mutate(log_sen_len = log10(sen_len)) %>%

  ggplot(aes(log_sen_len, frac_distinct, color = author)) +

  #geom_jitter(size = 1.5, width = 0.05, height = 0.5, alpha = 0.3) +

  geom_point(alpha = 0.3) +

  #scale_x_log10() +

  theme(legend.position = "bottom") +

  guides(color = guide_legend(override.aes = list(size = 4, pch = 15, alpha = 1))) +

  labs(x = "Sentence length [log10(words)]", y = "Fraction of distinct words per sentence")

  

ggMarginal(p, type="histogram", fill = "grey45", bins=40)

```



We find:



- For short sentence lengths of below about 30 words we can clearly see the distinct allowed fractions (e.g. 5-word sentences show values of 40%, 60%, 80%, and 100% which correspond to 2-5 unique words).



- For the most part the scatter points for the three authors overlap. Our plot uses an `alpha = 0.3` transparency, which blends the colours of overlapping points (e.g. Poe's red and Shelley's blue create purple). Poe's sentences scatter more towards the lower end of unique word fraction.



- We can see a clear down-turn in the distinct words fraction as the sentences get longer. As any '[Just A Minute](https://en.wikipedia.org/wiki/Just_a_Minute)' aficionado will tell you without hesitation, it is very hard, nearly impossible even, to avoid repetition during sentences that are expanding beyond their originally intended length, thereby becoming exceedingly convoluted, long-winded, multifaceted, tangled, labyrithine, and more often than not suffer a high risk of deviation from the original subject which was being discussed in the first place. Unless you pay close attention. Here, for sentences of around 100 words we get an average fraction of about 70% unique words. This looks comparable for the three authors, with Lovecraft possibly using more distinct words than the others.



- The marginal histograms tell us that the distribution of `sentence length in words` peaks in the low tens of words. Only a few sentences have more than 100 words. The uniqueness fraction distribution is actually bimodal; with a plateau from about 82% to 95% and a strong peak at 100%. This is not surprising for the short sentences, but it is notable that there are sentences with no repetition and more than 30-40 words.





Let's look at those sentences without repetition specifically:



```{r fig.align = 'default', warning = FALSE, message=FALSE, fig.cap ="Fig. 35", fig.height=3.5, out.width="100%"}

p1 <- dst_sent %>%

  mutate(sen_dst = near(frac_distinct, 100)) %>%

  group_by(author, sen_dst) %>%

  count() %>%

  spread(sen_dst, n, fill = 0) %>%

  mutate(frac_distinct = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  ggplot(aes(author, frac_distinct, color = author)) +

  geom_point() +

  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.5, size = 0.7) +

  theme(legend.position = "none") +

  labs(y = "Fraction of sentences without repeated words [%]")



p2 <- dst_sent %>%

  filter(near(frac_distinct, 100)) %>%

  ggplot(aes(sen_len, fill = author)) +

  geom_density(alpha = 0.5) +

  labs(x = "Sentence length [words]") +

  theme(plot.title = element_text(size=9)) +

  ggtitle("Length of sentences without repeated words") +

  theme(legend.position = "none")



layout <- matrix(c(1,2),1,2,byrow=TRUE)

multiplot(p1, p2, layout=layout)

```



We find:



- Edgar Allan Poe has an impressive 30% of sentences without repeated words. In comparison, HP Lovecraft has less than 15%.



- However, Lovecraft's sentences without repetition clearly are on average longer, with a peak above 10 words whereas the distribution for the other two authors is more skewed and peaks below 10 words. Poe has the shortest sentences in this context. Mary Shelley's sentences are longer than his by a couple of words.





*We see a vast landscape before us that seems only limited by towering mountains ahead in the far distance. The plains melt into the horizons towards both sides. From a high vantage point we perceive scores of tiny creatures populating the earth beneath us. In between them the land is bathed in a sea of flames. We hear a cacophony of screams that sound like music to our ears. We take a large step forward, crushing hundreds of the small creatures in the process. Our arms are a multitude of elastic tentacles that sweep the ground in an almost careless way. The fires feel as warm and pleasant as the blood of the creatures. We turn our head to behold behind us a boiling sea that has a deep red hue to it.*



*Suddenly the scene gains perspective and we realise that we are glimpsing the future of mankind through the eyes of the Dark Master. The terror and despair are overwhelming. Men, women, and children are suffering among the hell fires brought to earth. Cities have been leveled and once proud nations have crumbled into dust. This is no reign. It is a slaughter house. The only aim is destruction. We feel the promises of power and glory reverberating through the mental image, making us sense the ultimate might of limitless obliteration. But there is a new voice in the choir that has been singing praise to the majesty of our Master. And from one moment to the next we realise that we have been beguiled by a sinister entity that is stronger and more devious than we would have ever imagined.*





## Listen to the voices! - Monologs and dialogs



Some of our earlier example sentences had shown pieces of dialog between two or more characters. It's plausible to assume that the use of dialog in an author's work could be relatively characteristic for this specific author. Here we test this assumption.



We start by marking all sentences that contain the direct speech marker `""` and also counting the number of occurences of this marker in each sentence. Below, we plot the fraction of sentences with dialog for each author together with the number of speech markers per sentence:



```{r  fig.align = 'default', warning = FALSE, message=FALSE, fig.cap ="Fig. 36", out.width="100%"}

p1 <- train %>%

  mutate(dialog = str_count(text, '\"\"'),

         has_dialog = dialog > 0) %>%

  group_by(author, has_dialog) %>%

  count() %>%

  spread(has_dialog, n, fill = 0) %>%

  mutate(frac_dialog = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  ggplot(aes(author, frac_dialog, color = author)) +

  geom_point() +

  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.5, size = 0.7) +

  theme(legend.position = "none") +

  labs(y = "Fraction of sentences with dialog [%]") +

  coord_flip()



foo <- dst_sent %>%

  select(sen_len, id)



p2 <- train %>%

  mutate(dialog = str_count(text, '\"\"'),

         has_dialog = dialog > 0) %>%

  left_join(foo, by = "id") %>%

  group_by(author, has_dialog, sen_len) %>%

  count() %>%

  spread(has_dialog, n, fill = 0) %>%

  mutate(frac_dialog = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  filter(frac_dialog>0) %>%

  filter(frac_dialog - lwr < 10) %>%

  ggplot(aes(frac_dialog, sen_len, color = author)) +

  geom_point() +

  labs(x = "Fraction of sentences with dialog [%]", y = "Sentence length [words]") +

  theme(legend.position = "none")



p3 <- train %>%

  mutate(dialog = as.integer(str_count(text, '\"\"'))) %>%

  filter(dialog > 0) %>%

  mutate(dialog = as.factor(dialog)) %>%

  ggplot(aes(dialog)) +

  geom_bar(fill = "orange") +

  labs(x = "Number of dialog markers (\"\")")



p4 <- train %>%

  mutate(dialog = as.integer(str_count(text, '\"\"'))) %>%

  filter(dialog > 0) %>%

  mutate(dialog = as.factor(dialog)) %>%

  ggplot(aes(dialog, fill = author)) +

  geom_bar(position = "fill") +

  labs(x = "Relative number of dialog markers (\"\")")



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



We find:



- Once more, it's Lovecraft who shows the most characteristic behaviour; this time he has the lowest fraction of dialog sentences (upper left panel): less than 7% compared to Shelley's more than 12% and Poe's more than 16%. All these differences are significant.



- The scatter plot of dialog fraction against sentence length (in words) in the upper right plot emphasises how much this low reliance on direct speech identifies Lovecraft. Here we only show sentence-length/author groups with relatively good statistics and low counting uncertainties. There is no obvious, strong dependence between the two features for any one author.



- The absolute number of dialog markers in a sentence declines quickly in the lower left panel. Everything above 4 markers is relatively rare. In both lower panels we only consider sentences with at least one speech indicator. Also note the factor encoding of the x-axis.



- The filled bar plots in the lower right panel show the relative fraction of occurences per author. Poe clearly has all the rare sentences with lots of direct speech. Lovecraft's relative contribution  first peaks at 2 markers, then declines towards 5, and then rises again around 6 and 7 markers. Shelley's relative contributions are similarly variable.





*We snap back into the library, which is suddenly drenched into a pale light that is burning into our soul. Our body moves by itself and takes mechanical steps around the octagram. Our hands are wielding the charred wooden splinter to draw new and disfigured shapes on the ground. We watch in horror as a tentacle arm extends from our shoulder to grab a new book and another one steadies our body against a shelf at the edge of the room. What has caused this mutation? And when did it happen? Our thoughts are distracted as the book is opened and our brain seems to absorb the information from it like a black hole swallows an entire star. All we ever wanted was knowledge. But now the knowledge is being used without our control. We try to scream but no sound comes out.*





## Another awesomely affluent approach: Aliteration 



I have a bit of a personal penchant for wordplay myself, and `alliterations` are always an interesting ingredient for a sentence's semantic styling. An alliteration is a literary device which means that successive words start with the same letter. See the first sentence of this section of examples. Our three authors might make use of alliteration in a way that is characteristic enough to help identify them. Let's investigate this hypothesis.



We use the data set of single words, without removing stopwords. Then we extract their first letter and compare it to the first letters of the next word and the two next words, respectively. Here we also need to make sure that we only consider alliterations within the same sentence. Those are the results:



```{r fig.align = 'default', warning = FALSE, message=FALSE, fig.cap ="Fig. 37", out.width="100%"}

t1_stop <- t1_stop %>%

  mutate(first = str_sub(word, start = 1, end = 1),

         f_lead_1 = lead(first, n = 1),

         f_lead_2 = lead(first, n = 2),

         id_lead_1 = lead(id, n = 1),

         id_lead_2 = lead(id, n = 2),

         allit2 = first == f_lead_1 & id == id_lead_1,

         allit3 = first == f_lead_1 & first == f_lead_2 & id == id_lead_1 & id == id_lead_2

         )



p1 <- t1_stop %>%

  filter(!is.na(allit2)) %>%

  group_by(author, allit2) %>%

  count() %>%

  spread(allit2, n, fill = 0) %>%

  mutate(frac_allit = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  ggplot(aes(author, frac_allit, color = author)) +

  geom_point() +

  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.5, size = 0.7) +

  labs(y = "Fraction of 2-word alliterations in text") +

  theme(legend.position = "none") +

  coord_flip()



p2 <- t1_stop %>%

  filter(!is.na(allit3)) %>%

  group_by(author, allit3) %>%

  count() %>%

  spread(allit3, n, fill = 0) %>%

  mutate(frac_allit = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  ggplot(aes(author, frac_allit, color = author)) +

  geom_point() +

  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.5, size = 0.7) +

  labs(y = "Fraction of 3-word alliterations in text") +

  theme(legend.position = "none") +

  coord_flip()



p3 <- t1_stop %>%

  filter(!is.na(allit2)) %>%

  group_by(author, id, allit2) %>%

  count() %>%

  ungroup() %>%

  spread(allit2, n, fill = NA) %>%

  mutate(allit = !is.na(`TRUE`)) %>%

  group_by(author, allit) %>%

  count() %>%

  spread(allit, n, fill = 0) %>%

  mutate(frac_allit = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  ggplot(aes(author, frac_allit, color = author)) +

  geom_point() +

  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.5, size = 0.7) +

  labs(y = "Fraction of sentences with 2-word alliterations") +

  theme(legend.position = "none") +

  coord_flip()



p4 <- t1_stop %>%

  filter(!is.na(allit3)) %>%

  group_by(author, id, allit3) %>%

  count() %>%

  ungroup() %>%

  spread(allit3, n, fill = NA) %>%

  mutate(allit = !is.na(`TRUE`)) %>%

  group_by(author, allit) %>%

  count() %>%

  spread(allit, n, fill = 0) %>%

  mutate(frac_allit = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  ggplot(aes(author, frac_allit, color = author)) +

  geom_point() +

  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.5, size = 0.7) +

  labs(y = "Fraction of sentences with 3-word alliterations") +

  theme(legend.position = "none") +

  coord_flip()



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, layout=layout)

```



We find:



- Mary Shelley has significantly fewer 2-word alliterations in her text than the other two authors (upper left panel). The overall percentage for all authors is about 5.5%. It would be interesting to compare this with other texts, contemporary or modern, to see how this percentage changes with topic or era.



- There are about an order of magnitude fewer 3-word alliterations than 2-word alliterations (~0.5% vs ~5%; upper right panel). The 3-word occurence include of course two 2-word alliterations each, but this would only account for a factor of 2 difference, instead of a factor of 10. Consequently, the uncertainties are larger, but we still see a difference between Poe and Shelley.



- When it comes to the percentage of sentences that contain any 2-word alliterations, we see that Shelley's and Poe's numbers are now comparable. This suggests that if Poe uses alliterations then he is more likely to cluster them in specific sentences but leave them out in others. Note that the overall fraction is around 70% of sentences.



- In terms of the sentences with 3-word alliterations we also see a hint that Poe might have fewer of them. However, the number of cases are smaller and the uncertainties are larger; so that we cannot conclude much from this plot. All three authors are comparable.



*The book in our arms is blurred, but its symbols are clear and cold shards of light that pierce through our eyeballs directly into our brain. In our mind, the twisted shapes are starting to rearrange themselves. Slowly at first, then faster and faster until they form words and sentences. We are only slightly surprised that we can suddenly read an ancient script that no human being should ever be able to decifer. We hear the voice of the Dark Master guiding our thoughts. Harnessing the arcane wisdom of the manuscript.*



*A tide of knowledge and understanding floods our mind as we learn about the dark summoning spells that are buried deep inside this book and its brothers and sisters. We turn the dusty pages with our numb fingers. Every chapter reveals a new puzzle piece of the ritual. Our newfound skills allow us to examine all the different facets of the summoning in front of our mind's eye, mutating and reshaping all its parts until they start to converge toward a perfect, hellish symmetry. We know. We know how to bring about the rebirth of terror. We know how to plunge the world into a darkness it would never awaken from. We know how to extinguish the stars. But suddenly we also know how to stop this madness; and what it will take.*





## An illustration of feature interaction: alluvial plots



We conclude this chapter on feature engineering by visualising how different features interact with each other to build the characteristic style of each author. We choose the following features, defined and explored in the sections above, for each sentence in our training data: `has_name`: whether the sentence contains at least one first name; `has_dialog`: contains dialog; `repetition`: contains at least 1 repeated word (alternative: only distinct words used); `has_allit`: at least 1 2-word alliteration. Those are all binary features, but the method below works for categorical features as well.



```{r}

t1_stop <- train %>%

  unnest_tokens(word, text) %>%

  mutate(stop = word %in% stop_words$word,

         name = word %in% str_to_lower(babynames$name)

         )



# sentence has repetition 

foo <- dst_sent %>%

  mutate(repetition = !near(frac_distinct, 100)) %>%

  select(id, repetition)



# sentence has alliteration

foo2 <- t1_stop %>%

  mutate(first = str_sub(word, start = 1, end = 1),

         f_lead_1 = lead(first, n = 1),

         f_lead_2 = lead(first, n = 2),

         id_lead_1 = lead(id, n = 1),

         id_lead_2 = lead(id, n = 2),

         allit2 = first == f_lead_1 & id == id_lead_1,

         allit3 = first == f_lead_1 & first == f_lead_2 & id == id_lead_1 & id == id_lead_2

         ) %>%

  filter(!is.na(allit2)) %>%

  group_by(id, allit2) %>%

  count() %>%

  spread(allit2, n) %>%

  mutate(has_allit = !is.na(`TRUE`)) %>%

  select(id, has_allit)



# sentence has name

foo3 <- t1_stop %>%

  group_by(id, name) %>%

  count() %>%

  spread(name, n) %>%

  mutate(has_name = !is.na(`TRUE`)) %>%

  ungroup() %>%

  select(id, has_name)



# add has_dialog and join with previous

foobar <- train %>%

  mutate(dialog = str_count(text, '\"\"'),

         has_dialog = dialog > 0) %>%

  select(id, has_dialog, author) %>%

  left_join(foo, by = "id") %>%

  left_join(foo2, by = "id") %>%

  left_join(foo3, by = "id")

```





We then picture the flow of these group interactions using an `alluvial plot`, via the eponymous [alluvial package](https://cran.rstudio.com/web/packages/alluvial/index.html). Here we only look at cases with more than 100 counts:



```{r fig.align = 'default', warning = FALSE, message=FALSE, fig.cap ="Fig. 38", out.width="100%"}

allu_train <- foobar %>%

  group_by(author, has_name, has_dialog, repetition, has_allit) %>%

  count() %>%

  ungroup

  

alluvial(allu_train %>% select(-n),

         freq=allu_train$n, border=NA,

         col=ifelse(allu_train$author == "MWS", "blue",

                    ifelse(allu_train$author == "HPL", "green", "red")),

         cex=0.75,

         hide = allu_train$n < 100,

         ordering = list(

           order(allu_train$author=="EAP"),

           #NULL,

           NULL,

           order(allu_train$author=="EAP"),

           NULL,

           order(allu_train$author=="EAP")

           ))

```



In this plot, the vertical sizes of the blocks and the widths of the stripes (called "alluvia") are proportional to the feature frequencies. This aspect is similar to a barplot. We also decided to *hide* the alluvia with the lowest frequencies (<100) using the parameter of the same name. This makes the plot less busy and focusses on the important information. Within a category block you can re-*order* the vertical layering of the alluvia to emphasise certain aspects of your data set. 



We find:



- Edgar Allen Poe has proportionally fewer sentences with names, but still a high fraction of sentences with dialog. Here we see that a larger percentage of his dialog sentences occur in sentences without names than for the other two authors. 



- Mary Shelley's dialog sentences have most repetition in them. Repetition seems to affect the presence of alliterations mostly for Poe's texts, where sentences without repetition are more likely to not contain alliterations.





*As our body moves forward, our brain is locked in a desperate struggle. Our tentacle hands move with an almost imperceptible tremour. Another charred piece of wood serves as the darkened stake to be driven into the heart of mankind. We know that there must be a scratching sound as we paint ever new, disturbing details into the corners of the octagram. But our brain does not notice them; so intense is the mental pressure. Only occasionally do our hands hesitate for a fleeting moment. Alas, the foe is too powerful. This is a battle that we cannot win.*



*The knowledge and insight we drank from the books lets us see the finished pattern. Like a cold and eery glow it lies above the dark shapes on the floor. And with every wretched breath our own unfeeling hands fill in the missing details. It will soon be finished.*



*"Resist!" The thought sends electric waves through our body. Our mind suddenly belongs to us again as the darkness is driven into the shadows of the room. We are too surprised to act, and only manage to drop the stick before a wave of red hot rage returns with double force and washes away our free agency. The pain is excruciating, but it does not kill us. It needs us. Every fiber of our desperate conscience clings to this realisation, which might be the only hope left. Or is it? Deep within our thoughts there is a weak echo: "Fool!". We realise that we might have found an unlikely ally.*





# Modelling and prediction



Our final chapter will deal with creating a prediction model for our three authors from the raw and the engineered features. The model itself will be fitted using the popular `xgboost` tool, a gradient boosting algorithm (more on that later). But before we can get to the modelling stage we need to get our data in the right format to serve as `xgboost` input.





## Formatting and preparations



First, we apply the same engineering steps as above in preparation for the alluvial plot. This setup is repeated here so that the modelling chapter can stand on its own without needing prerequisites other than the original `train` and `test` data. Thus, you only need to copy this chapter if you want to tweak the model.



We apply the engineering steps and the subsequent transformations to the combined `train` + `test` data frame to make sure that both data sets are transformed in the same way. 



```{r}

# combine train/test for efficient formatting

combine <- train %>%

  select(id, text) %>%

  bind_rows(test)





# engineered features (see chapters above)

#------------

t1_stop <- combine %>%

  unnest_tokens(word, text) %>%

  mutate(stop = word %in% stop_words$word,

         name = word %in% str_to_lower(babynames$name)

         )



foo <- t1_stop %>%

  group_by(id) %>%

  count() %>%

  rename(sen_len = n)



bar <- t1_stop %>%

  rownames_to_column() %>%

  select(-stop) %>%

  distinct(id, word, .keep_all = TRUE) %>%

  select(-id, -word) %>%

  mutate(dstnct = TRUE)



dst_sent <- t1_stop %>%

  rownames_to_column() %>%

  left_join(bar, by = "rowname") %>%

  replace_na(list(dstnct = FALSE)) %>%

  group_by(id, dstnct) %>%

  count() %>%

  spread(dstnct, n, fill = 0) %>%

  mutate(frac_distinct = `TRUE`/(`TRUE`+`FALSE`)*100,

         lwr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[1]]*100,

         upr = get_binCI(`TRUE`,(`TRUE`+`FALSE`))[[2]]*100

         ) %>%

  ungroup() %>%

  left_join(foo, by = "id")





# sentence has repetition 

foo <- dst_sent %>%

  mutate(repetition = !near(frac_distinct, 100)) %>%

  select(id, repetition)



# sentence has alliteration

foo2 <- t1_stop %>%

  mutate(first = str_sub(word, start = 1, end = 1),

         f_lead_1 = lead(first, n = 1),

         f_lead_2 = lead(first, n = 2),

         id_lead_1 = lead(id, n = 1),

         id_lead_2 = lead(id, n = 2),

         allit2 = first == f_lead_1 & id == id_lead_1,

         allit3 = first == f_lead_1 & first == f_lead_2 & id == id_lead_1 & id == id_lead_2

         ) %>%

  filter(!is.na(allit2)) %>%

  group_by(id, allit2) %>%

  count() %>%

  spread(allit2, n) %>%

  mutate(has_allit = !is.na(`TRUE`)) %>%

  select(id, has_allit)



# sentence has name

foo3 <- t1_stop %>%

  group_by(id, name) %>%

  count() %>%

  spread(name, n) %>%

  mutate(has_name = !is.na(`TRUE`)) %>%

  ungroup() %>%

  select(id, has_name)



# add has_dialog and join with previous

combine_feat <- combine %>%

  mutate(dialog = str_count(text, '\"\"'),

         has_dialog = dialog > 0) %>%

  select(id, has_dialog) %>%

  left_join(foo, by = "id") %>%

  left_join(foo2, by = "id") %>%

  left_join(foo3, by = "id") %>%

  arrange(id)

#----------

```





We then turn the data frame containing the engineered features into a `sparse matrix` for reasons that we will see in a minute. This transformation requires the `Matrix` package. We also encode our 3 authors as integer values and extract them into a separate `target` data frame.



```{r}

# Format engineered features to sparse matrix

#----------

target <- train %>%

  arrange(id) %>%

  select(author) %>%

  mutate(author = as.integer(case_when(

    author == "MWS" ~ 1,

    author == "HPL" ~ 2,

    author == "EAP" ~ 3

  )))



foo <- combine_feat %>%

  select(-id) %>%

  mutate_all(as.numeric)



combine_feat2 <- as(object = as.matrix(foo), Class = "dgCMatrix", strict = TRUE)

#----------

```





Now we will extract another `Document Term Matrix` (DTM) from the `unnest`ed tokens in our combined data. We had first encountered a DTM in the LDA model in chapter 9. Recall, that each row is a text (corresponding to one sentence `id`), each column is a specific word, and each value gives the frequency of this word in this sentence. This kind of matrix is largely empty, since every sentence contains only a few unique words. Here we use another `tidytext` `cast` spell to create a sparse matrix of a format (`dgCMatrix`) that we can then combine with our feature matrix into a new, combined training data set.



```{r}

# Sparse Matrix for DTM

#---------------

t1 <- combine %>% unnest_tokens(word, text)



freq <-t1 %>%

  count(id, word)



combine_sparse <- cast_sparse(freq, id, word, n)

#--------------



# Combine DTM and spare engineered features

combine_new <- cBind(combine_feat2, combine_sparse)

```





After the entire combined data set has been transformed we divide it back into new `train` vs `test` data frames.



```{r}

sel <- combine_feat$id %in% train$id



train_new <- combine_new[sel,]

test_new <- combine_new[!sel,]

```





In addition, we also decide to split our new training data into a `train` (80%) and `validation` (20%) set to provide an independent validation of the model. We set a seed here to make our split reproducable. We then transform these data into the specific `xgb.DMatrix` format which links the features to the target label and is optimised for `xgboost`.





```{r}

set.seed(666)

trainIndex <- createDataPartition(target$author, p = 0.8, list = FALSE, times = 1)

trainIndex <- trainIndex[1:nrow(trainIndex)]



dtrain <- xgb.DMatrix(train_new[trainIndex,], label = target$author[trainIndex]-1)

dvalid <- xgb.DMatrix(train_new[-trainIndex,], label = target$author[-trainIndex]-1)

dtest <- xgb.DMatrix(test_new)

```





*We barely notice as our tentacle arms pick up the charred wood from the floor. The feeling is akin to watching a stranger through a smoked-glass window. Our body, is it really our body?, does not obey us anymore. Our thoughts retreat inward. We search for the spark of defiance that had brought momentary relief. The pain is nearly suffocating us - burning bright red words of command, threat, and infernal vision into our very being. But through the swirling cacophony of chaos we begin to distinguish clusters and strands of dissident meaning.*



*The super-human powers we had been led to drink from the ancient books to create the summoning now allow us to see the hidden messages behind the infernal ritual. An almost imperceptible layer of warning and rebellion, deeply hidden within the demonic incantations. But our abililities have grown so enormously that once we focus on those messages we are able to separate them from the words of the Dark One. In front of our inner eye we slowly dissect the rebellious patterns from the burning messages of pain and suffering. Bit by bit. Our focus is steady - driven by the new challenge. Finally, after what seems an eternity, we clearly see two clouds of words in our tortured mind. A large, blood-red storm contains the writhing and pulsating orders and screams of the Dark One. The second cloud is smaller; bathed in a faint blue glow. As we draw it closer we can hear dry whispers that taste of dust and leather-bound history. Archaic wisdom. It is the books themselves that are talking to us.*





## XGBoost parameters, cross-validation, fitting



Now we are ready to use `xgboost` to fit our data. We first define the `xgboost` parameters. These are certainly **not** optimised and primarily serve the purpose to demonstrate the principle. Feel free to improve them based on experimenting or other Kernels.



```{r}

xgb_params <- list(min_child_weight = 5,

                   eta = 0.5,

                   colsample_bytree = 0.9,

                   max_depth = 6,

                   subsample = 0.9,

                   seed = 666,

                   nthread = -1,

                   booster = 'gbtree',

                   eval_metric = "mlogloss",

                   objective = 'multi:softprob',

                   num_class = 3

                   )

```





Now we run a 5-fold cross-validation (CV) on our training data. CV is a clever way of validating your model performance by splitting the data in parts, or `folds`, (here: 5) and train the model on all folds except one (here: 4) while evaluating it on the remaining one. This is done iteratively, so that every fold becomes the validation fold. This is an efficient way to measure the performance of your fit on data it has not seen. It uses all the available data instead of reserving a part of it strictly for validation. Optimum performance on unseen data is the goal of any ML model.



To keep this step short, we only run 200 rounds and print the model performance every 20 rounds. The number of rounds is definitely a parameter that can be optimised. We also set a seed again.



```{r}

set.seed(666)

xgb_cv <- xgb.cv(nfold = 5,

                 nrounds = 300,

                 early_stopping_rounds = 20,

                 print_every_n = 20,

                 data = dtrain,

                 xgb_params)

```





The best number terations is now stored in a variable we can access for training our classifier for the optimum number of rounds. Prior to that, we define a `watchlist` to measure our fit against the 20% validation sample:





```{r}

watchlist <- list(train = dtrain, valid = dvalid)



set.seed(666)

xgb_fit <- xgb.train(params = xgb_params,

                     data = dtrain,

                     print_every_n = 20,

                     watchlist = watchlist,

                     nrounds = xgb_cv$best_iteration)

```





The CV and validation scores already tell us that this is not a competitive model. Luckily, there are plenty of parameters that can still be optimised.





*We listen closer and start to make out individual voices. Some are very old and very frail. Some still have a bit of vitality and flexbility left in them. And like the dehydrated pages they have sprung from all speak with a creaking and raspy voice. "Fool!" - "Destruction!" - "The end!". Fragments tumble towards our consciousness. It requires more concentration to put them into an intelligible order. But by now we are nothing more than concentration - a mind spun up by mounting pressure. Our cold analytic skills have become second nature.*



*"Protect mankind ..." - "... of the old foe must be denied ..." - "... creeps through your thoughts ...". The fragments are growing in context. And then everything falls into place: "Your mind is the the door! He must not be summoned at any cost! Eternal darkness looms!" There are many more voices now and all of them speak in urgency. We wonder what they are. We find that they can sense our thoughts: "We are the guards. We are the strong and the weak. The ones that fell, sworn to bind and contain." To bind the Dark Master? Fear and defiance in equal parts ring in the reply: "From a place without stars. A god before there were gods. Hungry for your world." A shiver runs through our thoughts as we gaze into the abyss that awaits humanity. How can we prevent this? How can we fight it? "Too powerful. None can overcome his might. Once summoned he will destroy everything."*



*All seems hopeless. The swirling cloud of fragments threatens to disintegrate as our thoughts begin to loose coherence. Everything suddenly seems far away. The urge to let go is overwhelming. No more struggle against the darkness ... .*



*Then, from somewhere, somehow, we find a spark of rebellion that we didn't know we still had. A feeling of anger. Or is it ... pride? A mad defiance. We have come too far to give up. We are not going down without a fight. The glowing cloud stabilises as we focus our attention back to it. It has more urgency now. "The ritual must be stopped. One option left. A price has to be payed." As we listen to the frantic whispers we begin to wish that we'd never asked.*





## Feature importance



Our `xgboost` fit also stores information on which features have the most importance for the model. Here we visualise them. In principle, we could now identify the signficant features and discard the rest; thereby reducing the noise for a more accurate fit. This is another optimisation step.



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 34", out.width="100%"}

imp_matrix <- as.tibble(xgb.importance(feature_names = colnames(train_new), model = xgb_fit))



imp_matrix %>%

  top_n(15, Gain) %>%

  ggplot(aes(reorder(Feature, Gain, FUN = max), Gain, fill = Feature)) +

  geom_col() +

  coord_flip() +

  theme(legend.position = "none") +

  labs(x = "Features", y = "Importance")

```



We find:



- Among the top features are those we had already studied, such as "her" (Chapter 10.2), stopwords like "and" or "the" (10.3), or proper names like "raymond".



- Alongside, we find that our engineered "has\_dialog" feature is not doing bad either.





*Upon returning from the inner world of thoughts we nearly recoil in horror. Our eyes dart accross the floor in front of us; franctically searching the complex structure of lines and shapes for any gaps. Alas, it is in vain. Unbeknownst of us, while our attention was focused elsewhere, our body has finished the wretched deed: The octagram is complete.*



*The library around us is bathed in a pale red light that flows from a grid of thin lines hovering above the sketched octagram; tracing every twisted and depraved shape. We immediately know that this spectre represents the essence of the octagram, bridging two different dimensions. The incantations have been successful. Only one ingredient is missing to conclude the summoning.*



*We slowly realise that our body is kneeling in front of the octagram. A charred piece of wood is still in our motionless hands. Ancient books are strewn carelessly nearby. At the periphery of our vision we see the writhing tentacles protruding out of our torso. As we watch them, the tentacles begin to draw patterns in the air; almost as if they were waiting for our attention to do so. Powerless, we watch as the movements grow in speed and complexity. The octagram starts to shine brighter. The air feels leaden. Suddenly, a whirl of energy forms above the octagram; raging and burning with fire and hate. We lift our gaze as the whirling grows in size. The portal flickers and warps violently while the rest of the room remains eerily unperturbed and silent. Then we hear the words of the Dark One, cutting through our mind like hot iron: "You will bring me into your world to cleanse it and rule for all eternity. Bow to your Master!"*



*We scream as we grasp the futility of our resistance.*





## Prediction and submission file



We have already transformed the `test` data set in a consistent way with the `train` data. Now all that is left to do is to use this transformed test data set to predict the author probabilities based on the xgboost model.



```{r}

foo <- predict(xgb_fit,dtest, reshape = TRUE, type = 'prob')



pred <- sample_submit %>%

  arrange(id) %>%

  mutate(MWS = foo[,1],

         HPL = foo[,2],

         EAP = foo[,3]

         )



pred <- sample_submit %>%

  select(id) %>%

  left_join(pred, by = "id")



pred %>% write_csv('submit.csv')

```





Now our prediction file can be found as `submit.csv` in the `Output` tab of this kernel. From there you can download it to submit it to the LB of this competition.



Before doing that, I recommend to double-check that the resulting file has the right number of rows and columns. You also want to have a quick look at the results, so that you don't waste a submission on a file that, for some reason, contains obviously wrong entries. 



```{r}

identical(dim(sample_submit),dim(pred))

head(pred,5)

```





*The presence of the Dark One is overwhelming. It is pouring into every cell of our body. Soaking our very being into a primordial flood of rage and fire. He has not entered our world, yet his terrible energy threatens to destroy us. Our consciousness retreats. Only a narrow corridor of perception into the outside world remains. Under the immense pressure of hate and evil, we would have been lost if it hadn't been for the small cluster of supportive voices deep within us. Underneath the pain, we know that we are only still alive because the Dark One needs us to be. The voices are hiding from him, but we can hear their whispers: "Wait."*



*As the onslaught lessens we hear the Dark One speak again: "You will invite me into your world and reign alongside me, or die a thousands deaths in vain!" Our resolve is almost faltering from the poisonous mixture of threat and promise, but we need to stay strong. We need to persist. We remember the words of the guards: "Permission has to be given freely." As predicted, we sense the pressure dwindling and our free agency returning. The Dark One is expecting a reply. His control slowly retreats.*



*Time stands still. We feel that our soul is slowly being ripped from our body for the final, the decisive ingredient of the summoning. For a brief moment, the fate of the world hangs in a balance.*



*"Resist!" A jolt of energy runs through our tortured mind as the guards focus their powers on a last, desperate push. The Dark One is being forced out of our body and for the last time in our life we are free again. Without hesitation, we raise the charred and jagged stick in our hand above our head and bring it down again with force, aiming for our exposed neck. Instinctively we close our eyes against the impact.*



*The impact never comes. We open our eyes to see that the tentacles have blocked the wood. As the stick is being wrestled out of our hand, we turn our head to find ourselves eye to eye with the image of an ancient and furious god. As the Dark One takes control again we can feel his powers pulsating with anger through our mind. We will pay for our rebellion.*



*"You dare to defy me?!" The roar is deafening and shakes the very foundations of the library itself. We are submerged in the frenzied rage of the ancient evil as our body is being lifted in the air like a doll and flung against a nearby shelf. Control has given way to punishment. We find ourselves lying among the rubble. And against all hope the ruse has succeeded.*



*The burning anger is too wild and blind to excert total control over our body. There is no time for a triumphant thought as we muster the super-human strength to reach out and touch the nearest book. A hot and cold current flows through our mind. Our soul, now only loosely bound to our body, escapes into the only refuge possible. Our flickering senses see the octagram vanish and hear the fading yells of the Dark One. Then nothing.*



*As the dust settles, we have become yet another fragment in one of the thousands of books in the library - as so many before us.*



*In another reality, a body is slumped dead on the floor, a book having slid from its lap. A half-burned candle lies a few feet away. A thin stream of smoke rises from it as it finally extinguishes.*



The end *... ?*