{"cells":[{"metadata":{"_uuid":"93ef65449e8406d6ec06064409be1aac3575f0a8"},"cell_type":"markdown","source":" **INTRODUCTION**\n \n We'll learn, practise and compare 6 classification models in this project. So, you'll see in this kernel:\n\n* EDA (Exploratory Data Analysis)\n* What is Confusion Matrix?\n* Test-Train Datas Split\n* Logistic Regression Classification\n* KNN Classification\n* Support Vector Machine (SVM) Classification\n* Naive Bayes Classification\n* Desicion Tree Classification\n* Random Forest Classification\n* Compare all of these Classification Models\n* Conclusion"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#For confusion matrixes\nfrom sklearn.metrics import confusion_matrix\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read our data from dataset.\ndata = pd.read_csv(\"../input/voice.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d16f254ff43bd2d432af4b83be38b02687e89e6"},"cell_type":"code","source":"#Let's looking at top 5 datas.\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6317d5fd24158eb559e315652b6e3aa2a2ac1e77"},"cell_type":"code","source":"#Let's looking at last 10 datas.\ndata.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bbf4561298c6cbef9e6f5cdd4429d2ca2f56aea"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84a40284b0db178f7a4897e2232e6c401ebe9d1e"},"cell_type":"code","source":"data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48d1da4e83f4f341d5686f25987aa419314d9179"},"cell_type":"code","source":"# Firstly, we must check our data. If we have NaN values, we should drop them.\ndata.info()\n#As we can see easily, we have no NaN values.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59c8b597ed024f73a53fcc7f7bf256a653098f70"},"cell_type":"markdown","source":"**Our 'label' feature has 2 valuable: male and female. These are string but we need integers for classification. Therefore, we must convert them from object to integer.**"},{"metadata":{"trusted":true,"_uuid":"280902ca67769ad5a3770c321584aff95e3d69e0"},"cell_type":"code","source":"data.label = [1 if each == \"female\" else 0 for each in data.label]\n#We assign 1 to female, 0 to male.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"956f0a43e88d1c5405902e118720caa0f27dcc5d"},"cell_type":"markdown","source":"**Let's check it!**"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"be9490ba55b2a3540d14b09b4d68a7087fe9dd09"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e222ec4a7949394abee6ff2244e93f6853d4993"},"cell_type":"markdown","source":"**Confusion Matrix**\n\nBefore start the classifications, we should know one thing: Confusion Matrix!\nFor example; we have 100 data point (dogs and cats) and we make a prediction. Our prediction score is 0.8 so we predict well %80. Confusion matrix gives us; \n* How many true values we predict true  (TP = True Positive)\n* How many true values we predict false  (FP = False Positive)\n* How many false values we predict false  (TN = True Negative)\n* How many false values we predict true (FN = False Negative\n"},{"metadata":{"_uuid":"f1c24ba91ef7c1cbf4fb870cde74f45b525d22d7"},"cell_type":"markdown","source":"**As you can see; our label features converted integer!**"},{"metadata":{"trusted":true,"_uuid":"accbaf1b52b358827b5f0ac1c5f1cb004656fbc6"},"cell_type":"code","source":"#We should have x and y values for test-train datas.\ny = data.label.values\nx_data = data.drop([\"label\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8528c7ef80c02ffdaf1c661d04520680d39ee5c"},"cell_type":"code","source":"#Normalization\nx = (x_data - np.min(x_data)) / (np.max(x_data)).values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0d7c4ba6c43d3f93ed8a0aa67b79adfff4410b0"},"cell_type":"markdown","source":"**After assign x and y value; we should train and test datas split.**"},{"metadata":{"trusted":true,"_uuid":"fb30587b984da0fbb9fbd3d38e7fc696ace61137"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 42)\n#test_size=0.2 means %20 test datas, %80 train datas\nmethod_names = []\nmethod_scores = []\n#These are for barplot in conclusion","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a80bf6ccd3789092ade5ad42b0cc08998218b0e"},"cell_type":"markdown","source":"**And now time to classification our data!**\n\n**We start with:**\n\n**LOGISTIC REGRESSION CLASSIFICATION**"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e23a0c1338e2fa06012c123f59f24ed11908475d"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train) #Fitting\nprint(\"Logistic Regression Classification Test Accuracy {}\".format(log_reg.score(x_test,y_test)))\nmethod_names.append(\"Logistic Reg.\")\nmethod_scores.append(log_reg.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = log_reg.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b5dbe07bd22773c31efa82b4c07eb02fc4c0d69"},"cell_type":"markdown","source":"**KNN (K-Nearest Neighbour) CLASSIFICATION**"},{"metadata":{"trusted":true,"_uuid":"d8d08ad590ec8f264b439fab40dec1edd80fa841"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\nprint(\"Score for Number of Neighbors = 3: {}\".format(knn.score(x_test,y_test)))\nmethod_names.append(\"KNN\")\nmethod_scores.append(knn.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = knn.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e496523ef8d8c69bc03eaa6b0298004193bab857"},"cell_type":"markdown","source":"**n_neighbors is an optional parameter. I wrote it 3 but you can write anything. Let's learn the best value of n_neighbors parameter.**"},{"metadata":{"trusted":true,"_uuid":"b1c9dc31b4d5d5dfd738a62842881f64e85faeff"},"cell_type":"code","source":"score_list=[]\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n\nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"score\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9c681be1cffe767b1b993eed1625d9065058003"},"cell_type":"markdown","source":"**As we can see; the best value of n_neighbor is 2. Let's find score when n_neighbors=2**"},{"metadata":{"trusted":true,"_uuid":"9550a288b46bb4a0a1a689fbeb79a305bf719c18"},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(x_train,y_train)\nprint(\"Score for Number of Neighbors = 2: {}\".format(knn.score(x_test,y_test)))\n\n#Confusion Matrix\ny_pred = knn.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44ebea74170022f16e9922e8f0b2a5f55fa9397f"},"cell_type":"markdown","source":"**SUPPORT VECTOR MACHINE (SVM)**"},{"metadata":{"trusted":true,"_uuid":"d25ad50f5f1007983743849a485b07faa1f23fb4"},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm = SVC(random_state=42)\nsvm.fit(x_train,y_train)\nprint(\"SVM Classification Score is: {}\".format(svm.score(x_test,y_test)))\nmethod_names.append(\"SVM\")\nmethod_scores.append(svm.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = svm.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f533d781b67e00a673a1dd9cecc3e765a62271e"},"cell_type":"markdown","source":"**NAIVE BAYES CLASSIFICATION**"},{"metadata":{"trusted":true,"_uuid":"80f7190d3220ed5a8bd5d29178407ff34712273b"},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnaive_bayes = GaussianNB()\nnaive_bayes.fit(x_test,y_test)\nprint(\"Naive Bayes Classification Score: {}\".format(naive_bayes.score(x_test,y_test)))\nmethod_names.append(\"Naive Bayes\")\nmethod_scores.append(naive_bayes.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = naive_bayes.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7a170db510f9e19ac4b7561dd05cd3f07393549"},"cell_type":"markdown","source":"**DECISION TREE CLASSIFICATION**"},{"metadata":{"trusted":true,"_uuid":"76c049b20bb528d679e6f7bc5d8aafa042679c76"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndec_tree = DecisionTreeClassifier()\ndec_tree.fit(x_train,y_train)\nprint(\"Decision Tree Classification Score: \",dec_tree.score(x_test,y_test))\nmethod_names.append(\"Decision Tree\")\nmethod_scores.append(dec_tree.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = dec_tree.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ac8c57e0090cc570250104b80f4afe9fda60ef8"},"cell_type":"markdown","source":"**RANDOM FOREST CLASSIFICATION**"},{"metadata":{"trusted":true,"_uuid":"28d3a32eab1876ebd5c4d1eb1975a696e16b7d7d"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrand_forest = RandomForestClassifier(n_estimators=100, random_state=42)\nrand_forest.fit(x_train,y_train)\nprint(\"Random Forest Classification Score: \",rand_forest.score(x_test,y_test))\nmethod_names.append(\"Random Forest\")\nmethod_scores.append(rand_forest.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = rand_forest.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7833e5bbe5f9f12181e2c782c34030dec3a5391f"},"cell_type":"markdown","source":"**CONCLUSION**\n\nWe completed seven different classification on this data and we see; Random Forest Classification is the best way to make classification on this dataset. Of course not everytime but for this practice Random Forest gave us the best classifications!\n\nLet's see differences between our methods scores!"},{"metadata":{"trusted":true,"_uuid":"db7f517b5b58396d7e086f16c2f5cae19069f49b"},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.ylim([0.85,1])\nplt.bar(method_names,method_scores,width=0.5)\nplt.xlabel('Method Name')\nplt.ylabel('Method Score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cccaeeb83ea5a9791a1867a304fe4f1ac07e145"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}