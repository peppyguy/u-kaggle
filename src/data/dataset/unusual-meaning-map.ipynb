{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "97a4020f-95de-c0de-9ce2-b8e2307b4906"
      },
      "source": [
        "Unusual meaning map: Treating question pairs as image / surface\n",
        "---------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "97a5d3c2-fa3b-452d-d629-3116968b6c1c"
      },
      "source": [
        "Other people have already written really nice exploratory kernels which helped me to write the minimal code myself. \n",
        "\n",
        "In this kernel, I have tried to extract a different type of feature from which we can learn using any algorithm which can learn via image. The basic assumption behind this exercise is to capture non-sequential closeness between words.\n",
        "\n",
        "For example:\n",
        "A Question pair has pointing arrows from each of the words of one sentence to each of the words from another sentence\n",
        "![A Question pair has pointing arrows from each of the words of one sentence to each of the words from another sentence][1]\n",
        "\n",
        "  [1]: http://image.prntscr.com/image/97e92b0357a843078b61eef5ad8a183b.png\n",
        "\n",
        "To capture this we can create NxM matrix with Word2Vec distance between each word with other. and resize the matrix just like an image to a 10x10 matrix and use this as a feature to xgboost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9a81d538-d1a1-358a-8c34-7670147aeaec"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pip\n",
        "from gensim import corpora, models, similarities\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "train_file = \"../input/train.csv\"\n",
        "df = pd.read_csv(train_file, index_col=\"id\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "90d844ec-5dae-bafc-ee58-79397f9dd6d5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pylab as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "20a5a730-45be-6880-0191-20347be4788c"
      },
      "source": [
        "**Extracting unique questions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "38f3883f-8258-97c5-9b35-dc9199d86fb5"
      },
      "outputs": [],
      "source": [
        "questions = dict()\n",
        "\n",
        "for row in df.iterrows():\n",
        "    questions[row[1]['qid1']] = row[1]['question1']\n",
        "    questions[row[1]['qid2']] = row[1]['question2']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e7f1e6ae-52f1-d715-b7e6-276edfa278f0"
      },
      "source": [
        "**Creating a simple tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bf70144f-d19d-d6ab-3160-b2b666eef5f9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "def basic_cleaning(string):\n",
        "    string = str(string)\n",
        "    try:\n",
        "        string = string.decode('unicode-escape')\n",
        "    except Exception:\n",
        "        pass\n",
        "    string = string.lower()\n",
        "    string = re.sub(' +', ' ', string)\n",
        "    return string\n",
        "sentences = []\n",
        "for i in questions:\n",
        "    sentences.append(nltk.word_tokenize(basic_cleaning(questions[i])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7147800f-b006-e754-afe6-057975bdffba"
      },
      "source": [
        "**Creating a simple Word2Vec model from the question pair, we can use a pre-trained model instead to get better results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a80b9de3-48df-b6b4-2701-554f53d31d84"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fe010b8f-1fea-d87a-5733-8cfecc960157"
      },
      "source": [
        "**A very simple term frequency and document frequency extractor** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c5784c45-78b4-f124-5452-405777f0500e"
      },
      "outputs": [],
      "source": [
        "tf = dict()\n",
        "docf = dict()\n",
        "total_docs = 0\n",
        "for qid in questions:\n",
        "    total_docs += 1\n",
        "    toks = nltk.word_tokenize(basic_cleaning(questions[qid]))\n",
        "    uniq_toks = set(toks)\n",
        "    for i in toks:\n",
        "        if i not in tf:\n",
        "            tf[i] = 1\n",
        "        else:\n",
        "            tf[i] += 1\n",
        "    for i in uniq_toks:\n",
        "        if i not in docf:\n",
        "            docf[i] = 1\n",
        "        else:\n",
        "            docf[i] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a7e3ce1a-4cfa-ea08-8703-0659ac38b4a3"
      },
      "source": [
        "Mimic the IDF function but penalize the words which have fairly high score otherwise, and give a strong boost to the words which appear sporadically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "831e8912-bfed-8bc0-478d-714286d8b618"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import math\n",
        "def idf(word):\n",
        "    return 1 - math.sqrt(docf[word]/total_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "146df649-d4c4-434b-78f7-32c1c9299b37"
      },
      "outputs": [],
      "source": [
        "print(idf(\"kenya\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e8693960-2062-3042-6cb4-f1affe4e4986"
      },
      "source": [
        "A simple cleaning module for feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "01a2e717-4c55-0d11-b1a0-68c62a934f69"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "def basic_cleaning(string):\n",
        "    string = str(string)\n",
        "    string = string.lower()\n",
        "    string = re.sub('[0-9\\(\\)\\!\\^\\%\\$\\'\\\"\\.;,-\\?\\{\\}\\[\\]\\\\/]', ' ', string)\n",
        "    string = ' '.join([i for i in string.split() if i not in [\"a\", \"and\", \"of\", \"the\", \"to\", \"on\", \"in\", \"at\", \"is\"]])\n",
        "    string = re.sub(' +', ' ', string)\n",
        "    return string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c19639c7-73ce-115d-ece0-afad6f390c21"
      },
      "outputs": [],
      "source": [
        "def w2v_sim(w1, w2):\n",
        "    try:\n",
        "        return model.similarity(w1, w2)*idf(w1)*idf(w2)\n",
        "    except Exception:\n",
        "        return 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9f9e1c16-d62d-ead9-395a-ed698eafcca2"
      },
      "source": [
        "**Visualizing features**\n",
        "\n",
        "This function will create a 10x10 matrix using MxN word pairs among the words of question pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b9bd9077-7147-5649-7aa5-e3e170a7d57c"
      },
      "outputs": [],
      "source": [
        "\n",
        "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
        "import matplotlib.cm as cm\n",
        "from scipy import *\n",
        "df = df.sample(n=30000)\n",
        "def imagify(row):\n",
        "    s1 = row['question1']\n",
        "    s2 = row['question2']\n",
        "    t1 = list((basic_cleaning(s1)).split())\n",
        "    t2 = list((basic_cleaning(s2)).split())\n",
        "    print(\"Q1: \"+ s1)\n",
        "    print(\"Q2: \"+ s2)\n",
        "    print(\"Duplicate: \" + str(row['is_duplicate']))\n",
        "    \n",
        "    img = [[w2v_sim(x, y) for x in t1] for y in t2] \n",
        "    a = np.array(img, order='C')\n",
        "    img = np.resize(a,(10,10))\n",
        "    # print img\n",
        "    fig = plt.figure()\n",
        "    # tell imshow about color map so that only set colors are used\n",
        "    image = plt.imshow(img,interpolation='nearest')\n",
        "    # make a color bar\n",
        "    plt.colorbar(image)\n",
        "    plt.show()\n",
        "s = df.sample(n=3)\n",
        "plt.close()\n",
        "s.apply(imagify, axis=1, raw=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "08d3a251-1858-dbd9-58c1-b959b11fb932"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import *\n",
        "\n",
        "plt.close()\n",
        "def surface(row):\n",
        "    s1 = row['question1']\n",
        "    s2 = row['question2']\n",
        "    t1 = list((basic_cleaning(s1)).split())\n",
        "    t2 = list((basic_cleaning(s2)).split())\n",
        "    print(\"Q1: \"+ s1)\n",
        "    print(\"Q2: \"+ s2)\n",
        "    print(\"Duplicate: \" + str(row['is_duplicate']))\n",
        "    \n",
        "#     img = [[w2v_sim(x, y) for x in t1] for y in t2] \n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = Axes3D(fig)\n",
        "    X = linspace(0,10,10)\n",
        "    Y = linspace(0,10,10)\n",
        "    X, Y = meshgrid(X, Y)\n",
        "    Z = [[w2v_sim(x, y) for x in t1] for y in t2] \n",
        "    a = np.array(Z, order='C')\n",
        "    Z = np.resize(a,(10,10))\n",
        "    \n",
        "    ax.plot_surface(Y, X, Z, rstride=1, cstride=1, cmap=cm.jet)\n",
        "    ax.set_xlabel(\"X Axis\")\n",
        "    ax.set_ylabel(\"Y Axis\")\n",
        "    ax.set_zlabel(\"Z Axis\")\n",
        "    plt.show()\n",
        "    \n",
        "s = df.sample(n=3)\n",
        "plt.close()\n",
        "s.apply(surface, axis=1, raw=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6ea1b1e7-1d71-abeb-b6c1-eb27649c23cb"
      },
      "outputs": [],
      "source": [
        "def img_feature(row):\n",
        "    s1 = row['question1']\n",
        "    s2 = row['question2']\n",
        "    t1 = list((basic_cleaning(s1)).split())\n",
        "    t2 = list((basic_cleaning(s2)).split())\n",
        "    Z = [[w2v_sim(x, y) for x in t1] for y in t2] \n",
        "    a = np.array(Z, order='C')\n",
        "    return [np.resize(a,(10,10)).flatten()]\n",
        "s = df\n",
        "\n",
        "img = s.apply(img_feature, axis=1, raw=True)\n",
        "pix_col = [[] for y in range(100)] \n",
        "for k in img.iteritems():\n",
        "        for f in range(len(list(k[1][0]))):\n",
        "           pix_col[f].append(k[1][0][f])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3f70ee65-961b-e504-6319-1dc2f1fbdb9b"
      },
      "source": [
        "**Extracting Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0dc37757-8f09-a77a-0219-d75f6ff4b3dc"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from __future__ import division\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "def word_match_share(row):\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in str(row['question1']).lower().split():\n",
        "        if word not in stops:\n",
        "            q1words[word] = 1\n",
        "    for word in str(row['question2']).lower().split():\n",
        "        if word not in stops:\n",
        "            q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q2words) == 0:\n",
        "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
        "        return 0\n",
        "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
        "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
        "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
        "    return R\n",
        "\n",
        "train_word_match = df.apply(word_match_share, axis=1, raw=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "734650f7-3d1b-3368-e7d0-7177e77f0927"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "x_train = pd.DataFrame()\n",
        "\n",
        "for g in range(len(pix_col)):\n",
        "    x_train['img'+str(g)] = pix_col[g]\n",
        "\n",
        "    \n",
        "x_train['word_match'] = train_word_match\n",
        "\n",
        "y_train = s['is_duplicate'].values\n",
        "pos_train = x_train[y_train == 1]\n",
        "neg_train = x_train[y_train == 0]\n",
        "# Now we oversample the negative class\n",
        "# There is likely a much more elegant way to do this...\n",
        "p = 0.165\n",
        "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
        "while scale > 1:\n",
        "    neg_train = pd.concat([neg_train, neg_train])\n",
        "    scale -=1\n",
        "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
        "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
        "\n",
        "x_train = pd.concat([pos_train, neg_train])\n",
        "y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
        "del pos_train, neg_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e689a400-39d0-02b9-54ab-21b0048193a8"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b3ad6c45-cfb6-2af7-2d0a-ac832e548349"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Set our parameters for xgboost\n",
        "params = {}\n",
        "params['objective'] = 'binary:logistic'\n",
        "params['eval_metric'] = 'logloss'\n",
        "params['eta'] = 0.02\n",
        "params['max_depth'] = 7\n",
        "\n",
        "d_train = xgb.DMatrix(x_train, label=y_train)\n",
        "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
        "\n",
        "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
        "\n",
        "bst = xgb.train(params, d_train, 500, watchlist, early_stopping_rounds=100, verbose_eval=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6b4e1dfc-ad3c-569a-b012-c6873f9bb978"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12.0, 30.0)\n",
        "xgb.plot_importance(bst); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bbc9ef3a-d2a9-5a76-ff82-66c836e58093"
      },
      "source": [
        "Using this technique and combining it with word match features I got log loss of **0.31858** on test dataset. \n",
        "\n",
        "I thought this feature can be of some help to others hence shared. Enjoy :)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}