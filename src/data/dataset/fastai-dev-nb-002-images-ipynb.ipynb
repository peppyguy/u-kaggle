{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Image Data & Augmentation\n\nSource: https://github.com/fastai/fastai_old/blob/master/dev_nb/002_images.ipynb"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Clean up (to avoid errors)\n!rm -rf data\n!rm -rf nb_001b.py\n\n# Get the code for the previous notebook as a python script\n!wget https://raw.githubusercontent.com/fastai/fastai_old/master/dev_nb/nb_001b.py\n\n# Install any additional dependencies\n!pip install --upgrade dataclasses\n\n# Download and unzip data\n!wget http://files.fast.ai/data/cifar10.tgz\n!tar -xzf cifar10.tgz\n\n# Create data directories\n!mkdir data\n!mkdir data/cifar10_dog_air\n!mkdir data/cifar10_dog_air/train\n!mkdir data/cifar10_dog_air/test\n\n# Move the subset of data required for training\n!mv cifar10/train/dog data/cifar10_dog_air/train/dog\n!mv cifar10/train/airplane data/cifar10_dog_air/train/airplane\n!mv cifar10/test/dog data/cifar10_dog_air/test/dog\n!mv cifar10/test/airplane data/cifar10_dog_air/test/airplane\n\n# Clean up (remove files and folders that are not required)\n!rm cifar10.tgz\n!rm -rf cifar10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3df1e1e82bf619aeea417ca9ea0e0f32773ba6a"},"cell_type":"code","source":"from nb_001b import *\nimport sys, PIL, matplotlib.pyplot as plt, itertools, math, random, collections, torch\nimport scipy.stats, scipy.special\n\nfrom enum import Enum, IntEnum\nfrom torch import tensor, Tensor, FloatTensor, LongTensor, ByteTensor, DoubleTensor, HalfTensor, ShortTensor\nfrom operator import itemgetter, attrgetter\nfrom numpy import cos, sin, tan, tanh, log, exp\nfrom dataclasses import field\nfrom functools import reduce\nfrom collections import defaultdict, abc, namedtuple, Iterable\nfrom typing import Tuple, Hashable, Mapping, Dict\n\nimport mimetypes, abc, functools\nfrom abc import abstractmethod, abstractproperty","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"077bc1d8f9d96e9ecea258f0b8e968df16410e54"},"cell_type":"markdown","source":"## Data setup\nFirst we want to view our data to check if everything is how we expect it to be."},{"metadata":{"trusted":true,"_uuid":"62156a297fd43d2225db82bfe847f0d6c8b0a6a4"},"cell_type":"code","source":"DATA_PATH = Path('data')\nPATH = DATA_PATH/'cifar10_dog_air'\nTRAIN_PATH = PATH/'train'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9bddee285a1918da1f524fd209d60d286482b3c"},"cell_type":"code","source":"dog_fn = list((TRAIN_PATH/'dog').iterdir())[0]\ndog_image = PIL.Image.open(dog_fn)\ndog_image.resize((64,64))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3259aa747e2e8213e77d62380a89e0f3db2d3177"},"cell_type":"code","source":"air_fn = list((TRAIN_PATH/'airplane').iterdir())[1]\nair_image = PIL.Image.open(air_fn)\nair_image.resize((64,64))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27b34ef6e7acc1576917d30b140defd9361050e7"},"cell_type":"markdown","source":"## Simple Dataset & Dataloader\n\nWe will build a Dataset class for our image files. A Dataset class needs to have two functions: `__len__` and `__getitem__`. Our ImageDataset class additionally gets image files from their respective directories and transforms them to tensors. We start by defining a few helper functions and classes to encapsulate small parts of logic required to implement the Dataset class."},{"metadata":{"trusted":true,"_uuid":"8c382acf11ad31a060a4f943522e810058eaf2e4"},"cell_type":"code","source":"# Helper types\nFilePathList = Collection[Path]\nTensorImage = Tensor\nNPImage = np.ndarray\nPathOrStr = Union[Path,str]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c4c0cafac1bd577686cd03e1794710a18bf4f8f"},"cell_type":"code","source":"def pil2tensor(image:NPImage)->TensorImage:\n    \"\"\"Convert PIL style `image` array to torch style image tensor\"\"\"\n    arr = torch.ByteTensor(torch.ByteStorage.from_buffer(image.tobytes()))\n    arr = arr.view(image.size[1], image.size[0], -1)\n    return arr.permute(2,0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"023299aaa16c71f133a3f4d0e7a070131a3c8aab"},"cell_type":"code","source":"def image2np(image:Tensor)->np.ndarray:\n    \"\"\"Convert from torch style image to numpy style\"\"\"\n    res = image.cpu().permute(1,2,0).numpy()\n    return res[...,0] if res.shape[2] == 1 else res\n\ndef show_image(img:Tensor, ax:plt.Axes=None, figsize:tuple=(3,3), \n               hide_axis:bool=True, title:Optional[str]=None, \n               cmap:str='binary', alpha:Optional[float]=None)->plt.Axes:\n    \"\"\"Plot tensor `img` using matplotlib axis `ax`.\"\"\"\n    if ax is None: fig,ax = plt.subplots(figsize=figsize)\n    ax.imshow(image2np(img), cmap=cmap, alpha=alpha)\n    if hide_axis: ax.axis('off')\n    if title: ax.set_title(title)\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b388033eaecf47121f358c80a802755fba35224"},"cell_type":"code","source":"class Image():\n    \"\"\"Helper class to wrap image tensor\"\"\"\n    def __init__(self, px): self.px = px\n    def show(self, ax=None, **kwargs): \n        return show_image(self.px, ax=ax, **kwargs)\n    @property\n    def data(self): return self.px","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a14fb52bd27f0b5cfb116bc160c7be326ecab483"},"cell_type":"code","source":"def find_classes(folder:Path)->FilePathList:\n    \"\"\"Get class subdirectories in an imagenet style train folder\"\"\"\n    classes = [d for d in folder.iterdir() \n               if d.is_dir() and not d.name.startswith('.')]\n    assert(len(classes)>0)\n    return sorted(classes, key=lambda d: d.name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2e0db50d9f4570c222e9971b22b04dad1319ef7"},"cell_type":"code","source":"find_classes(TRAIN_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57352b0f17055336b68a1caa499391c46ba4bca8"},"cell_type":"code","source":"image_extensions = set(k for k,v in mimetypes.types_map.items() \n                       if v.startswith('image/'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"656538c10f6a6aed253691bea3059ae9780a5ef6"},"cell_type":"code","source":"image_extensions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f65a3ff1191859dcb7842c29b067bfb20a2da04"},"cell_type":"code","source":"def get_image_files(c:Path, check_ext:bool=True)->FilePathList:\n    \"\"\"List image files inside the class directory `c`\"\"\"\n    return [o for o in list(c.iterdir()) \n            if not (o.name.startswith('.') or \n                    o.is_dir() or \n                    (check_ext and o.suffix not in image_extensions))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77079083824e23ab69c971c665410601a58e2e5e"},"cell_type":"code","source":"classes = find_classes(TRAIN_PATH)\nprint(classes)\nimgs = get_image_files(classes[0])\nprint(len(imgs))\nimgs[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21f17beb366150ffe026dcf9ad19df994d6944b5"},"cell_type":"code","source":"def open_image(fn:PathOrStr):\n    \"\"\"Return `Image` object created from image in file `fn`\"\"\"\n    x = PIL.Image.open(fn).convert('RGB')\n    return Image(pil2tensor(x).float().div_(255))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be71321048c90a117e3fc48698f6c741626697ec"},"cell_type":"code","source":"img = open_image(imgs[0])\nprint(img.data)\nimg.show(title=imgs[0].name);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"deab8a9d334671696d47b15bcd7a601815646155"},"cell_type":"markdown","source":"We also need some helper functions to extract a random sample as the validation set from our data."},{"metadata":{"trusted":true,"_uuid":"f1c6cfdd59b40205ad89f2a7e19ac95a77971dd1"},"cell_type":"code","source":"# More types\nNPArraybleList = Collection[Union[np.ndarray, list]]\nNPArrayMask = np.ndarray\nSplitArrayList = List[Tuple[np.ndarray,np.ndarray]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86c8acfe6ac21a51968abe7a94cb917acb56f92c"},"cell_type":"code","source":"def arrays_split(mask:NPArrayMask, *arrs:NPArraybleList)->SplitArrayList:\n    \"\"\"Given `arrs` is [a,b,...] and `mask` index, get [(a[mask],b[mask],...), (a[~mask], b[~mask],...)]\"\"\"\n    mask = array(mask)\n    return list(zip(*[(a[mask],a[~mask]) for a in map(np.array, arrs)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78b1b71f47a8be1d073c485b668aba66459b1f41"},"cell_type":"code","source":"a = [1, 2, 3, 4]\nb = [5, 6, 7, 8]\nc = [9, 10, 11, 12]\nmask = [True, False, False, True]\narrays_split(mask, a, b, c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ba3dbaa136b64d7f26deecc6d496f862380d92e"},"cell_type":"code","source":"def random_split(valid_pct:float, *arrs:NPArraybleList)->SplitArrayList:\n    \"\"\"Randomly `array_split` with `valid_pct` ratio.\"\"\"\n    is_train = np.random.uniform(size=(len(arrs[0],))) > valid_pct\n    return arrays_split(is_train, *arrs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdec81fd6b7c45c7b39abd6a54ac59e369b17ef8"},"cell_type":"code","source":"random_split(0.3, a, b, c)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2c9e0ab7305317deb24d9c0bc52b8fe34269bf5"},"cell_type":"markdown","source":"Now we're ready to define some `Dataset` classes to work with images and labels"},{"metadata":{"trusted":true,"_uuid":"c9bea2f56df51e4c941a320773f0f7fbb5a6c97a"},"cell_type":"code","source":"class DatasetBase(Dataset):\n    \"\"\"Base class for all fastai classification datasets\"\"\"\n    def __len__(self): return len(self.x)\n\n    @property\n    def c(self):\n        \"\"\"Number of classes expressed by dataset y variable\"\"\"\n        return self.y.shape[-1] if len(self.y.shape) > 1 else 1\n    \n    def __repr__(self):\n        return f'{type(self).__name} for len {len(self)}'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10cb821c701673e17e2f5d59dc59de58234be7de"},"cell_type":"code","source":"class LabelDataset(DatasetBase):\n    \"\"\"Base class for fastai datsets for classification\"\"\"\n    @property\n    def c(self):\n        \"\"\"Number of classes expressed by dataset y variable\"\"\"\n        return len(self.classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f03378a5712b05a90007f6461eeefa38d61506d"},"cell_type":"code","source":"# More types\nImgLabel = str\nImgLabels = Collection[ImgLabel]\nClasses = Collection[Any]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5c66a64a71ed6b6b3639f4d1d919e7f4c1c3947"},"cell_type":"code","source":"class ImageDataset(LabelDataset):\n    \"\"\"Dataset for folders of images in style {folder}/{class}/{images}\"\"\"\n    def __init__(self, fns:FilePathList, labels:ImgLabels, \n                 classes:Optional[Classes]=None):\n        self.classes = ifnone(classes, list(set(labels)))\n        self.class2idx = {v:k for k,v in enumerate(self.classes)}\n        self.x = np.array(fns)\n        self.y = np.array([self.class2idx[o] for o in labels], dtype=np.int64)\n        \n    def __getitem__(self, i): return open_image(self.x[i]), self.y[i]\n    \n    @staticmethod\n    def _folder_files(folder:Path, label:ImgLabel, \n                      check_ext=True)->Tuple[FilePathList,ImgLabels]:\n        \"\"\"Get image files and labels for the given `folder` and `label`\"\"\"\n        fnames = get_image_files(folder, check_ext=check_ext)\n        return fnames, [label]*len(fnames)\n    \n    @classmethod\n    def from_single_folder(cls, folder:Path, classes:Classes,\n                           check_ext:bool=True):\n        \"\"\"Typically used for test set, gives dummy labels\"\"\"\n        fns,labels = cls._folder_files(folder, classes[0], check_ext)\n        return cls(fns, labels, classes)\n    \n    @classmethod\n    def from_folder(cls, folder:Path, classes:Optional[Classes]=None, \n                    valid_pct:float=0., check_ext:bool=True)-> Union['ImageDataset', List['ImageDataset']]:\n        \"\"\"Dataset of `classes` labeled images in `folder`. Optional `valid_pct`\"\"\"\n        if classes is None: \n            classes = [cls.name for cls in find_classes(folder)]\n        fns,labels = [],[]\n        for cl in classes:\n            f,l = cls._folder_files(folder/cl, cl, check_ext)\n            fns+=f; labels+=l\n        if valid_pct==0.: return cls(fns, labels, classes)\n        return [cls(*a, classes) for a in random_spilt(valid_pct, fns, labels)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e751d7ca08ef6b273385a30a1bea62c6078d188"},"cell_type":"code","source":"train_ds = ImageDataset.from_folder(TRAIN_PATH)\nimg,label = train_ds[10]\nprint(img.data.shape)\nprint(label)\nprint(train_ds.classes)\nimg.show(title=str(train_ds.classes[label]));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7eeadd8cfad810c88bacc1bb794d573729ec5c83"},"cell_type":"markdown","source":"# Data Augmentation\n\nWe are going to augment our data to increase the size of our training set with artificial images. These new images are basically \"free\" data that we can use in our training to help our model generalize better (reduce overfitting).\n\n## Lighting\nWe will start by changing the **brightness** and **contrast** of our images.\n\n### Method\n#### Brightness\n\nBrightness refers to where does our image stand on the dark-light spectrum. Brightness is applied by adding a positive constant to each of the image's channels. This works because each of the channels in an image goes from 0 (darkest) to 255 (brightest) in a dark-light continum. (0, 0, 0) is black (total abscence of light) and (255, 255, 255) is white (pure light). You can check how this works by experimenting by yourself here.\n\nParameters\n\n1. **Change** How much brightness do we want to add to (or take from) the image.\n\nDomain: Real numbers\n\n### Contrast\n\nContrast refers to how sharp a distinction there is between brighter and darker sections of our image. To increase contrast we need darker pixels to be darker and lighter pixels to be lighter. In other words, we would like channels with a value smaller than 128 to decrease and channels with a value of greater than 128 to increase.\n\nParameters\n\n1. **Scale** How much contrast do we want to add to (or remove from) the image.\n\nDomain: `[0, +inf]`\n\n**On logit and sigmoid**\n\nGuide to logit and sigmoid functions: https://blacklen.wordpress.com/2011/01/01/sigmoid-and-logit-function/\n\nNotice that for both transformations we first apply the logit to our tensor, then apply the transformation and finally take the sigmoid. This is important for two reasons.\n\nFirst, we don't want to overflow our tensor values. In other words, we need our final tensor values to be between `[0,1]`. Imagine, for instance, a tensor value at 0.99. We want to increase its brightness, but we can’t go over 1.0. By doing logit first, which first moves our space to `-inf` to `+inf`, this works fine. The same applies to contrast if we have a scale S > 1 (might make some of our tensor values greater than one).\n\nSecond, when we apply contrast, we need to affect the dispersion of values around the middle value. Say we want to increase contrast. Then we need the bright values (>0.5) to get brighter and dark values (<0.5) to get darker. We must first transform our tensor values so our values which were originally <0.5 are now negative and our values which were originally >0.5 are now positive. This way, when we multiply by a constant, the dispersion around 0 will increase. The logit function does exactly this and allows us to increase or decrease dispersion around a mid value."},{"metadata":{"trusted":true,"_uuid":"f066e67c9d2416ec1b4107a9aacff2d3e97fd3ba"},"cell_type":"code","source":"def logit(x:Tensor)->Tensor:  return -(1/x-1).log()\ndef logit_(x:Tensor)->Tensor: return (x.reciprocal_().sub_(1)).log_().neg_()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2d149a405c7723215a5582e6625794706f03842"},"cell_type":"code","source":"def contrast(x:Tensor, scale:float)->Tensor: return x.mul_(scale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1aa40fdc42c16cb45080744fbf57d2d5a364abf2"},"cell_type":"code","source":"FlowField = Tensor\nLogitTensorImage = TensorImage\nAffineMatrix = Tensor\nKWArgs = Dict[str,Any]\nArgStar = Collection[Any]\nTensorImageSize = Tuple[int,int,int]\n\nLightingFunc = Callable[[LogitTensorImage, ArgStar, KWArgs], LogitTensorImage]\nPixelFunc = Callable[[TensorImage, ArgStar, KWArgs], TensorImage]\nCoordFunc = Callable[[FlowField, TensorImageSize, ArgStar, KWArgs], LogitTensorImage]\nAffineFunc = Callable[[KWArgs], AffineMatrix]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b2b22b10d45ca25ea471b74fed8449661daa8b0"},"cell_type":"code","source":"class ItemBase():\n    \"All transformable dataset items use this type\"\n    @property\n    @abstractmethod\n    def device(self): pass\n    @property\n    @abstractmethod\n    def data(self): pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d6507c98d444bfea3ef053df8ea54b5ecfdc82f"},"cell_type":"code","source":"class ImageBase(ItemBase):\n    \"Img based `Dataset` items derive from this. Subclass to handle lighting, pixel, etc\"\n    def lighting(self, func:LightingFunc, *args, **kwargs)->'ImageBase': return self\n    def pixel(self, func:PixelFunc, *args, **kwargs)->'ImageBase': return self\n    def coord(self, func:CoordFunc, *args, **kwargs)->'ImageBase': return self\n    def affine(self, func:AffineFunc, *args, **kwargs)->'ImageBase': return self\n\n    def set_sample(self, **kwargs)->'ImageBase':\n        \"Set parameters that control how we `grid_sample` the image after transforms are applied\"\n        self.sample_kwargs = kwargs\n        return self\n    \n    def clone(self)->'ImageBase': \n        \"Clones this item and its `data`\"\n        return self.__class__(self.data.clone())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0e102037492dcbbe3312db70aea9e38ca88f9be"},"cell_type":"code","source":"class Image(ImageBase):\n    \"Supports appying transforms to image data\"\n    def __init__(self, px)->'Image':\n        \"create from raw tensor image data `px`\"\n        self._px = px\n        self._logit_px=None\n        self._flow=None\n        self._affine_mat=None\n        self.sample_kwargs = {}\n\n    @property\n    def shape(self)->Tuple[int,int,int]: \n        \"Returns (ch, h, w) for this image\"\n        return self._px.shape\n    \n    @property\n    def size(self)->Tuple[int,int]: \n        \"Returns (h, w) for this image\"\n        return self.shape[-2:]\n    \n    @property\n    def device(self)->torch.device: return self._px.device\n    \n    def __repr__(self): return f'{self.__class__.__name__} ({self.shape})'\n\n    def refresh(self)->None:\n        \"Applies any logit or affine transfers that have been \"\n        if self._logit_px is not None:\n            self._px = self._logit_px.sigmoid_()\n            self._logit_px = None\n        if self._affine_mat is not None or self._flow is not None:\n            self._px = grid_sample(self._px, self.flow, **self.sample_kwargs)\n            self.sample_kwargs = {}\n            self._flow = None\n        return self\n\n    @property\n    def px(self)->TensorImage:\n        \"Get the tensor pixel buffer\"\n        self.refresh()\n        return self._px\n    @px.setter\n    def px(self,v:TensorImage)->None: \n        \"Set the pixel buffer to `v`\"\n        self._px=v\n\n    @property\n    def flow(self)->FlowField:\n        \"Access the flow-field grid after applying queued affine transforms\"\n        if self._flow is None:\n            self._flow = affine_grid(self.shape)\n        if self._affine_mat is not None:\n            self._flow = affine_mult(self._flow,self._affine_mat)\n            self._affine_mat = None\n        return self._flow\n    \n    @flow.setter\n    def flow(self,v:FlowField): self._flow=v\n\n    def lighting(self, func:LightingFunc, *args:Any, **kwargs:Any)->'Image':\n        \"Equivalent to `image = sigmoid(func(logit(image)))`\"\n        self.logit_px = func(self.logit_px, *args, **kwargs)\n        return self\n\n    def pixel(self, func:PixelFunc, *args, **kwargs)->'Image':\n        \"Equivalent to `image.px = func(image.px)`\"\n        self.px = func(self.px, *args, **kwargs)\n        return self\n\n    def coord(self, func:CoordFunc, *args, **kwargs)->'Image':\n        \"Equivalent to `image.flow = func(image.flow, image.size)`\"        \n        self.flow = func(self.flow, self.shape, *args, **kwargs)\n        return self\n\n    def affine(self, func:AffineFunc, *args, **kwargs)->'Image':\n        \"Equivalent to `image.affine_mat = image.affine_mat @ func()`\"        \n        m = tensor(func(*args, **kwargs)).to(self.device)\n        self.affine_mat = self.affine_mat @ m\n        return self\n\n    def resize(self, size:Union[int,TensorImageSize])->'Image':\n        \"Resize the image to `size`, size can be a single int\"\n        assert self._flow is None\n        if isinstance(size, int): size=(self.shape[0], size, size)\n        self.flow = affine_grid(size)\n        return self\n\n    @property\n    def affine_mat(self)->AffineMatrix:\n        \"Get the affine matrix that will be applied by `refresh`\"\n        if self._affine_mat is None:\n            self._affine_mat = torch.eye(3).to(self.device)\n        return self._affine_mat\n    @affine_mat.setter\n    def affine_mat(self,v)->None: self._affine_mat=v\n\n    @property\n    def logit_px(self)->LogitTensorImage:\n        \"Get logit(image.px)\"\n        if self._logit_px is None: self._logit_px = logit_(self.px)\n        return self._logit_px\n    @logit_px.setter\n    def logit_px(self,v:LogitTensorImage)->None: self._logit_px=v\n    \n    def show(self, ax:plt.Axes=None, **kwargs:Any)->None: \n        \"Plots the image into `ax`\"\n        show_image(self.px, ax=ax, **kwargs)\n    \n    @property\n    def data(self)->TensorImage: \n        \"Returns this images pixels as a tensor\"\n        return self.px","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cf87fc0a126e84d543c96962d0801cb8d31147f"},"cell_type":"code","source":"train_ds = ImageDataset.from_folder(PATH/'train')\nvalid_ds = ImageDataset.from_folder(PATH/'test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a85dbb06e0b7c40842941206b73033730762cdd"},"cell_type":"code","source":"x = lambda: train_ds[4][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b3629e1fb6a95d6a55dfc11b75e97ec66c0439f"},"cell_type":"code","source":"img = x()\nimg.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6b00ded39273d066674f990b9f83315a5a49f33"},"cell_type":"code","source":"img = x()\nimg.logit_px = contrast(img.logit_px, 1.9)\nimg.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"319dcabebe5817bee7e0f2e729678857b327d013"},"cell_type":"code","source":"x().lighting(contrast, 1.9).show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd3860caeb0ec9c0fb492cd217c1633247dc9c96"},"cell_type":"markdown","source":"## Transform class"},{"metadata":{"trusted":true,"_uuid":"b2e9307248c0999be4eacf4987a37c31755bd05e"},"cell_type":"code","source":"class Transform():\n    _wrap=None\n    def __init__(self, func): self.func=func\n    def __call__(self, x, *args, **kwargs):\n        if self._wrap: return getattr(x, self._wrap)(self.func, *args, **kwargs)\n        else:          return self.func(x, *args, **kwargs)\n    \nclass TfmLighting(Transform): _wrap='lighting'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d1ef2af394a7bf45a32ecbbaf128fbd022fd80c"},"cell_type":"code","source":"@TfmLighting\ndef brightness(x, change): return x.add_(scipy.special.logit(change))\n@TfmLighting\ndef contrast(x, scale): return x.mul_(scale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"374707d2b2e3449718f9ff325d7604dd8a59d4f4"},"cell_type":"code","source":"_,axes = plt.subplots(1,4, figsize=(12,3))\n\nx().show(axes[0])\ncontrast(x(), 1.0).show(axes[1])\ncontrast(x(), 0.5).show(axes[2])\ncontrast(x(), 2.0).show(axes[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ba421720961bf957cfdc8dfe401b2f38aaf07f5"},"cell_type":"code","source":"_,axes = plt.subplots(1,4, figsize=(12,3))\n\nx().show(axes[0])\nbrightness(x(), 0.8).show(axes[1])\nbrightness(x(), 0.5).show(axes[2])\nbrightness(x(), 0.2).show(axes[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"759b81bad783bdcee0805174986321cfdf6b9394"},"cell_type":"code","source":"def brightness_contrast(x, scale_contrast, change_brightness):\n    return brightness(contrast(x, scale=scale_contrast), change=change_brightness)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eab826bde29aa012be4c9c0596e2659fd41355c8"},"cell_type":"code","source":"_,axes = plt.subplots(1,4, figsize=(12,3))\n\nbrightness_contrast(x(), 0.75, 0.7).show(axes[0])\nbrightness_contrast(x(), 2.0,  0.3).show(axes[1])\nbrightness_contrast(x(), 2.0,  0.7).show(axes[2])\nbrightness_contrast(x(), 0.75, 0.3).show(axes[3])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"995335d45f3b4ea22d71e54a24ce393852280446"},"cell_type":"markdown","source":"## Random lighting\n\nNext, we will make our previous transforms random since we are interested in automatizing the pipeline. We will achieve this by making our parameters stochastic with a specific distribution.\n\nWe will use a [uniform](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)) distribution for brightness change since its domain is the real numbers and the impact varies linearly with the scale. For contrast change we use [log_uniform](https://www.vosesoftware.com/riskwiki/LogUniformdistribution.php) for two reasons. First, contrast scale has a domain of [0, inf]. Second, the impact of the scale in the transformation is non-linear (i.e. 0.5 is as extreme as 2.0, 0.2 is as extreme as 5). The log_uniform function is appropriate because it has the same domain and correctly represents the non-linearity of the transform, P(0.5) = P(2)."},{"metadata":{"trusted":true,"_uuid":"20e7c5abe2be0ffc3c1ae54f5f03a8029a5c7011"},"cell_type":"code","source":"FloatOrTensor = Union[float,Tensor]\nBoolOrTensor = Union[bool,Tensor]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b9c510d58d0c02a5cf91fb310d720caa8c056ac"},"cell_type":"code","source":"def uniform(low:Number, high:Number, size:List[int]=None)->FloatOrTensor:\n    \"Draw 1 or shape=`size` random floats from uniform dist: min=`low`, max=`high`\"\n    return random.uniform(low,high) if size is None else torch.FloatTensor(*listify(size)).uniform_(low,high)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee59f1df6e3e38623164593977d990e7e9dd1fa8"},"cell_type":"code","source":"def log_uniform(low, high, size=None)->FloatOrTensor:\n    \"Draw 1 or shape=`size` random floats from uniform dist: min=log(`low`), max=log(`high`)\"\n    res = uniform(log(low), log(high), size)\n    return exp(res) if size is None else res.exp_()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42c82b99139a531f3c6c379b9608caee8d1decdc"},"cell_type":"code","source":"def rand_bool(p:float, size=None)->BoolOrTensor: \n    \"Draw 1 or shape=`size` random booleans (True occuring probability p)\"\n    return uniform(0,1,size)<p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a956a92d25e1e2f74e15ab0ebb6a65076498e26"},"cell_type":"code","source":"scipy.stats.gmean([log_uniform(0.5,2.0) for _ in range(1000)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e697f0622e52e4966c476fb365bf26febff9955"},"cell_type":"code","source":"import inspect\nfrom copy import copy,deepcopy\n\ndef get_default_args(func:Callable):\n    return {k: v.default\n            for k, v in inspect.signature(func).parameters.items()\n            if v.default is not inspect.Parameter.empty}\n\nListOrItem = Union[Collection[Any],int,float,str]\nOptListOrItem = Optional[ListOrItem]\ndef listify(p:OptListOrItem=None, q:OptListOrItem=None):\n    \"Makes `p` same length as `q`\"\n    if p is None: p=[]\n    elif not isinstance(p, Iterable): p=[p]\n    n = q if type(q)==int else len(p) if q is None else len(q)\n    if len(p)==1: p = p * n\n    assert len(p)==n, f'List len mismatch ({len(p)} vs {n})'\n    return list(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fb3ef1c47471b8d748a829f0e00fd7dba2d0af5"},"cell_type":"code","source":"class Transform():\n    \"Utility class for adding probability and wrapping support to transform funcs\"\n    _wrap=None\n    order=0\n    def __init__(self, func:Callable, order:Optional[int]=None)->None:\n        \"Create a transform for `func` and assign it an priority `order`, attach to Image class\"\n        if order is not None: self.order=order\n        self.func=func\n        functools.update_wrapper(self, self.func)\n        self.func.__annotations__['return'] = Image\n        self.params = copy(func.__annotations__)\n        self.def_args = get_default_args(func)\n        setattr(Image, func.__name__,\n                lambda x, *args, **kwargs: self.calc(x, *args, **kwargs))\n        \n    def __call__(self, *args:Any, p:float=1., is_random:bool=True, **kwargs:Any)->Image:\n        \"Calc now if `args` passed; else create a transform called prob `p` if `random`\"\n        if args: return self.calc(*args, **kwargs)\n        else: return RandTransform(self, kwargs=kwargs, is_random=is_random, p=p)\n        \n    def calc(self, x:Image, *args:Any, **kwargs:Any)->Image:\n        \"Apply this transform to image `x`, wrapping it if necessary\"\n        if self._wrap: return getattr(x, self._wrap)(self.func, *args, **kwargs)\n        else:          return self.func(x, *args, **kwargs)\n\n    @property\n    def name(self)->str: return self.__class__.__name__\n    \n    def __repr__(self)->str: return f'{self.name} ({self.func.__name__})'\n\nclass TfmLighting(Transform): order,_wrap = 8,'lighting'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4ce9b531e87f45d0a97f6e6f5ed32e522c135e1"},"cell_type":"code","source":"@dataclass\nclass RandTransform():\n    \"Wraps `Transform` to add randomized execution\"\n    tfm:Transform\n    kwargs:dict\n    p:int=1.0\n    resolved:dict = field(default_factory=dict)\n    do_run:bool = True\n    is_random:bool = True\n    def __post_init__(self): functools.update_wrapper(self, self.tfm)\n    \n    def resolve(self)->None:\n        \"Bind any random variables needed tfm calc\"\n        if not self.is_random:\n            self.resolved = {**self.tfm.def_args, **self.kwargs}\n            return\n\n        self.resolved = {}\n        # for each param passed to tfm...\n        for k,v in self.kwargs.items():\n            # ...if it's annotated, call that fn...\n            if k in self.tfm.params:\n                rand_func = self.tfm.params[k]\n                self.resolved[k] = rand_func(*listify(v))\n            # ...otherwise use the value directly\n            else: self.resolved[k] = v\n        # use defaults for any args not filled in yet\n        for k,v in self.tfm.def_args.items():\n            if k not in self.resolved: self.resolved[k]=v\n        # anything left over must be callable without params\n        for k,v in self.tfm.params.items():\n            if k not in self.resolved and k!='return': self.resolved[k]=v()\n\n        self.do_run = rand_bool(self.p)\n\n    @property\n    def order(self)->int: return self.tfm.order\n\n    def __call__(self, x:Image, *args, **kwargs)->Image:\n        \"Randomly execute our tfm on `x`\"\n        return self.tfm(x, *args, **{**self.resolved, **kwargs}) if self.do_run else x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f42f2619303d29280dbb5b63f2ff16da38644d36"},"cell_type":"code","source":"@TfmLighting\ndef brightness(x, change:uniform): \n    \"`change` brightness of image `x`\"\n    return x.add_(scipy.special.logit(change))\n\n@TfmLighting\ndef contrast(x, scale:log_uniform): \n    \"`scale` contrast of image `x`\"\n    return x.mul_(scale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e166c9c91a2b00651c8643d2461f8f8c27d51e7"},"cell_type":"code","source":"x().contrast(scale=2).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e0a4191d4592118957acefa3a9a2e1cd0fd0753"},"cell_type":"code","source":"x().contrast(scale=2).brightness(0.8).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"068e60095584cb83f45505dc86c736498426a974"},"cell_type":"code","source":"tfm = contrast(scale=(0.3,3))\ntfm.resolve()\ntfm,tfm.resolved,tfm.do_run","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c94f7a7743e5db4923ce76c0a375a566493f2e9"},"cell_type":"code","source":"# all the same\ntfm.resolve()\n\n_,axes = plt.subplots(1,4, figsize=(12,3))\nfor ax in axes: tfm(x()).show(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f9aca4fb0adaa8b938b0c8df97d3dcc9ee62fb6"},"cell_type":"code","source":"tfm = contrast(scale=(0.3,3))\n\n# different\n_,axes = plt.subplots(1,4, figsize=(12,3))\nfor ax in axes:\n    tfm.resolve()\n    tfm(x()).show(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"244371ccde6f7cbb243b776e293db64cd0b5afce"},"cell_type":"code","source":"tfm = contrast(scale=4, is_random=False)\ntfm.resolve()\ntfm(x()).show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc07df03bbe01f67375481ef92d5a6d394f62300"},"cell_type":"markdown","source":"## Composition\nWe are interested in composing the transform functions so as to apply them all at once. We will try to feed a list of transforms to our pipeline for it to apply all of them.\n\nApplying a function to our transforms before calling them in Python is easiest if we use a decorator. You can find more about decorators [here](https://www.thecodeship.com/patterns/guide-to-python-function-decorators/)."},{"metadata":{"trusted":true,"_uuid":"ef4bad57f34de6d72b9b03c0a32ae9ba2511006d"},"cell_type":"code","source":"TfmList=Union[Transform, Collection[Transform]]\ndef resolve_tfms(tfms:TfmList):\n    \"Resolve every tfm in `tfms`\"\n    for f in listify(tfms): f.resolve()\n\ndef apply_tfms(tfms:TfmList, x:Image, do_resolve:bool=True):\n    \"Apply all the `tfms` to `x`, if `do_resolve` refresh all the random args\"\n    if not tfms: return x\n    tfms = listify(tfms)\n    if do_resolve: resolve_tfms(tfms)\n    x = x.clone()\n    for tfm in tfms: x = tfm(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8eb611d13342a2feac5dc27bbf40d6b33e01185b"},"cell_type":"code","source":"x = train_ds[1][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1fb1ce0456c9ea48036b4ffa3088bd151d84309"},"cell_type":"code","source":"tfms = [contrast(scale=(0.1,6.0), p=0.9),\n        brightness(change=(0.35,0.65), p=0.9)]\n\n_,axes = plt.subplots(1,4, figsize=(12,3))\nfor ax in axes: apply_tfms(tfms,x).show(ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c677ed5d26b569645f839bf2838a86e6a850101a"},"cell_type":"markdown","source":"## DatasetTfm"},{"metadata":{"trusted":true,"_uuid":"fcaef8e9bde9b3b21bfd7f860699b4fbd159368c"},"cell_type":"code","source":"class DatasetTfm(Dataset):\n    \"A `Dataset` that applies a list of transforms to every item drawn\"\n    def __init__(self, ds:Dataset, tfms:TfmList=None, **kwargs:Any):\n        \"this dataset will apply `tfms` to `ds`\"\n        self.ds,self.tfms,self.kwargs = ds,tfms,kwargs\n        \n    def __len__(self)->int: return len(self.ds)\n    \n    def __getitem__(self,idx:int)->Tuple[Image,Any]:\n        \"returns tfms(x),y\"\n        x,y = self.ds[idx]\n        return apply_tfms(self.tfms, x, **self.kwargs), y\n    \n    def __getattr__(self,k): \n        \"passthrough access to wrapped dataset attributes\"\n        return getattr(self.ds, k)\n\nimport nb_001b\nnb_001b.DatasetTfm = DatasetTfm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c07beee5394f94b11f4c1ac35d242efd5e77a8e3"},"cell_type":"code","source":"bs=64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd30b0d044e8fc3d12478cab456109e04d299072"},"cell_type":"code","source":"ItemsList = Collection[Union[Tensor,ItemBase,'ItemsList',float,int]]\ndef to_data(b:ItemsList):\n    \"Recursively maps lists of items to their wrapped data\"\n    if is_listy(b): return [to_data(o) for o in b]\n    return b.data if isinstance(b,ItemBase) else b\n\ndef data_collate(batch:ItemsList)->Tensor:\n    \"Convert `batch` items to tensor data\"\n    return torch.utils.data.dataloader.default_collate(to_data(batch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"305c2adb0c322d8e06708f1ae3b0970fbce6e112"},"cell_type":"code","source":"@dataclass\nclass DeviceDataLoader():\n    \"DataLoader that ensures items in each batch are tensor on specified device\"\n    dl: DataLoader\n    device: torch.device\n    def __post_init__(self)->None: self.dl.collate_fn=data_collate\n\n    def __len__(self)->int: return len(self.dl)\n    def __getattr__(self,k:str)->Any: return getattr(self.dl, k)\n    def proc_batch(self,b:ItemsList)->Tensor: return to_device(b, self.device)\n\n    def __iter__(self):\n        self.gen = map(self.proc_batch, self.dl)\n        return iter(self.gen)\n\n    @classmethod\n    def create(cls, *args, device=default_device, **kwargs)->'DeviceDataLoader':\n        \"Creates `DataLoader` and make sure its data is always on `device`\"\n        return cls(DataLoader(*args, **kwargs), device=device)\n    \nnb_001b.DeviceDataLoader = DeviceDataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7957b24f26a91ce6acfccf0dfdc22be48b9ff389"},"cell_type":"code","source":"data = DataBunch.create(train_ds, valid_ds, bs=bs, num_workers=4)\nlen(data.train_dl), len(data.valid_dl), data.train_dl.dataset.c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95f9aa696039f848bd318cd94a730512e5893d2f"},"cell_type":"code","source":"def show_image_batch(dl:DataLoader, classes:Collection[str], \n                     rows:Optional[int]=None, figsize:Tuple[int,int]=(12,15))->None:\n    \"Show a batch of images from `dl` titled according to `classes`\"\n    x,y = next(iter(dl))\n    if rows is None: rows = int(math.sqrt(len(x)))\n    show_images(x[:rows*rows],y[:rows*rows],rows, classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ccc91f9e1349afff16776fd9effc188e520f9ef"},"cell_type":"code","source":"def show_images(x:Collection[Image],y:int,rows:int, classes:Collection[str], figsize:Tuple[int,int]=(9,9))->None:\n    \"Plot images (`x[i]`) from `x` titled according to classes[y[i]]\"\n    fig, axs = plt.subplots(rows,rows,figsize=figsize)\n    for i, ax in enumerate(axs.flatten()):\n        show_image(x[i], ax)\n        ax.set_title(classes[y[i]])\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8575de59de21efffb462131e2ccb49312a482fb3"},"cell_type":"code","source":"show_image_batch(data.train_dl, train_ds.classes, 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6b8161508b09a88ffe6567dfb60fe1795465ea5"},"cell_type":"code","source":"data = DataBunch.create(train_ds, valid_ds, bs=bs, train_tfm=tfms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"933e31b184fbecd62a5120ecc9ed8836569b8a24"},"cell_type":"code","source":"show_image_batch(data.train_dl, train_ds.classes, 6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f19ef7e2cfc6f3208041d090331015472b05a2a4"},"cell_type":"markdown","source":"## Affine\nWe will now add affine transforms that operate on the coordinates instead of pixels like the lighting transforms we just saw. An affine transformation is a function \"(...) between affine spaces which preserves points, straight lines and planes.\"\n\nGuide to affine transformations: https://medium.com/hipster-color-science/computing-2d-affine-transformations-using-only-matrix-multiplication-2ccb31b52181\n\n### Details\nOur implementation first creates a grid of coordinates for the original image. The grid is normalized to a [-1, 1] range with (-1, -1) representing the top left corner, (1, 1) the bottom right corner and (0, 0) the center. Next, we build an affine matrix representing our desired transform and we multiply it by our original grid coordinates. The result will be a set of x, y coordinates which references where in the input image will each of the pixels in the output image be mapped. It has a size of `w * h * 2` since it needs two coordinates for each of the h * w pixels of the output image.\n\nThis is clearest if we see it graphically. We will build an affine matrix of the following form:\n```\n[[a, b, e],\n [c, d, f]]\n```\nwith which we will transform each pair of x, y coordinates in our original grid into our transformation grid:\n```\n[[a, b],       [[x],       [[e],       [[x'],\n  [c, d]]   x    [y]]    +   [f]]    =   [y']]\n```\nSo after the transform we will get a new grid with which to map our input image into our output image. This will be our map of where from exactly does our transformation source each pixel in the output image.\n\n#### Enter problems\n\nAffine transforms face two problems that must be solved independently:\n\n1. **The interpolation problem**: The result of our transformation gives us float coordinates, and we need to decide, for each (i,j), how to assign these coordinates to pixels in the input image.\n2. **The missing pixel problem**: The result of our transformation may have coordinates which exceed the [-1, 1] range of our original grid and thus fall outside of our original grid.\n\n#### Solutions to problems\n\n1. **The interpolation problem**: We will perform a bilinear interpolation. This takes an average of the values of the pixels corresponding to the four points in the grid surrounding the result of our transformation, with weights depending on how close we are to each of those points.\n2. **The missing pixel problem**: For these values we need padding, and we face a few options:\n\nAdding zeros on the side (so the pixels that fall out will be black)\nReplacing them by the value at the border\nMirroring the content of the picture on the other side (reflect padding).\n\n### Transformation Method\n\n#### Zoom\n\nZoom changes the focus of the image according to a scale. If a scale of >1 is applied, grid pixels will be mapped to coordinates that are more central than the pixel's coordinates (closer to 0,0) while if a scale of <1 is applied, grid pixels will be mapped to more perispheric coordinates (closer to the borders) in the input image.\n\nWe can also translate our transform to zoom into a non-centrical area of the image. For this we use $col_c$ which displaces the x axis and $row_c$ which displaces the y axis.\n\nParameters\n\n1. **Scale** How much do we want to zoom in or out to our image.\n\n    Domain: Real numbers\n\n3. **Col_pct** How much do we want to displace our zoom along the x axis.\n \n    Domain: Real numbers between 0 and 1\n\n5. **Row_pct** How much do we want to displace our zoom along the y axis.\n\n    Domain: Real numbers between 0 and 1\n```\n[[1/scale, 0,       col_c],\n  [0,       1/scale, row_c]]\n```\n\n#### Rotate\n\nRotate shifts the image around its center in a given angle theta. The rotation is counterclockwise if theta is positive and clockwise if theta is negative. If you are curious about the derivation of the rotation matrix you can find it here.\n\nParameters\n\n1. **Degrees** By which angle do we want to rotate our image.\n\n    Domain: Real numbers\n```\n[[cos(theta), -sin(theta), 0],\n  [sin(theta),  cos(theta), 0]]\n  ```"},{"metadata":{"trusted":true,"_uuid":"be4f36afaeb7002b65b0921983e241bcca6bb4d1"},"cell_type":"code","source":"def grid_sample_nearest(input:TensorImage, coords:FlowField, padding_mode:str='zeros')->TensorImage:\n    \"Grab pixels in `coords` from `input`. sample with nearest neighbor mode, pad with zeros by default\"\n    if padding_mode=='border': coords.clamp(-1,1)\n    bs,ch,h,w = input.size()\n    sz = tensor([w,h]).float()[None,None]\n    coords.add_(1).mul_(sz/2)\n    coords = coords[0].round_().long()\n    if padding_mode=='reflection':\n        mask = (coords[...,0] < 0) + (coords[...,1] < 0) + (coords[...,0] >= w) + (coords[...,1] >= h)\n        mask.clamp_(0,1)\n    coords[...,0].clamp_(0,w-1)\n    coords[...,1].clamp_(0,h-1)\n    result = input[...,coords[...,1],coords[...,0]]\n    if padding_mode=='zeros': result[...,mask] = result[...,mask].zero_()\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2263e3309689b6046f770f7061941a51fc8ecd43"},"cell_type":"code","source":"def grid_sample(x:TensorImage, coords:FlowField, mode:str='bilinear', padding_mode:str='border')->TensorImage:\n    \"Grab pixels in `coords` from `input` sampling by `mode`. pad is reflect or zeros.\"\n    if padding_mode=='reflect': padding_mode='reflection'\n    if mode=='nearest': return grid_sample_nearest(x[None], coords, padding_mode)[0]\n    return F.grid_sample(x[None], coords, mode=mode, padding_mode=padding_mode)[0]\n\ndef affine_grid(size:TensorImageSize)->FlowField:\n    size = ((1,)+size)\n    N, C, H, W = size\n    grid = FloatTensor(N, H, W, 2)\n    linear_points = torch.linspace(-1, 1, W) if W > 1 else tensor([-1])\n    grid[:, :, :, 0] = torch.ger(torch.ones(H), linear_points).expand_as(grid[:, :, :, 0])\n    linear_points = torch.linspace(-1, 1, H) if H > 1 else tensor([-1])\n    grid[:, :, :, 1] = torch.ger(linear_points, torch.ones(W)).expand_as(grid[:, :, :, 1])\n    return grid\n\ndef affine_mult(c:FlowField, m:AffineMatrix)->FlowField:\n    if m is None: return c\n    size = c.size()\n    c = c.view(-1,2)\n    c = torch.addmm(m[:2,2], c,  m[:2,:2].t()) \n    return c.view(size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90feba77470605ca6197ed94180862125adf4642"},"cell_type":"code","source":"def rotate(degrees):\n    angle = degrees * math.pi / 180\n    return [[cos(angle), -sin(angle), 0.],\n            [sin(angle),  cos(angle), 0.],\n            [0.        ,  0.        , 1.]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5de84d89c63df421307939250b20a67ead8cd13"},"cell_type":"code","source":"def xi(): return train_ds[1][0]\nx = xi().data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dce3459f14b4de1f497d342c260d6afd14d44d48"},"cell_type":"code","source":"c = affine_grid(x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"057f30d62e40da06a0796b83b81d0983df95bca9"},"cell_type":"code","source":"m = rotate(30)\nm = x.new_tensor(m)\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"553df2827abe7b73edef18a03f8b11286af3140b"},"cell_type":"code","source":"c[0,...,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5dabfeb72ec631d55cae88d3eb66a8871847712"},"cell_type":"code","source":"img2 = grid_sample(x, c, padding_mode='zeros')\nshow_image(img2);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa96dce305f2cb9dfe1c1d0e71b8ec5ee0452aef"},"cell_type":"code","source":"xi().affine(rotate, 30).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea291a267ea6968e808de388ced772a503ecb929"},"cell_type":"code","source":"class TfmAffine(Transform): \n    \"Wraps affine tfm funcs\"\n    order,_wrap = 5,'affine'\nclass TfmPixel(Transform): \n    \"Wraps pixel tfm funcs\"\n    order,_wrap = 10,'pixel'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e300b78e01f4a1bce43e13f37468559e226772fc"},"cell_type":"code","source":"@TfmAffine\ndef rotate(degrees:uniform):\n    \"Affine func that rotates the image\"\n    angle = degrees * math.pi / 180\n    return [[cos(angle), -sin(angle), 0.],\n            [sin(angle),  cos(angle), 0.],\n            [0.        ,  0.        , 1.]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01233881eaeb2115f1c02f6d01bed6981318783d"},"cell_type":"code","source":"def get_zoom_mat(sw:float, sh:float, c:float, r:float)->AffineMatrix:\n    \"`sw`,`sh` scale width,height - `c`,`r` focus col,row\"\n    return [[sw, 0,  c],\n            [0, sh,  r],\n            [0,  0, 1.]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e27b5cbfbf0e3c2c224ab73e99d2e0e5beaa441d"},"cell_type":"code","source":"@TfmAffine\ndef zoom(scale:uniform=1.0, row_pct:uniform=0.5, col_pct:uniform=0.5):\n    \"Zoom image by `scale`. `row_pct`,`col_pct` select focal point of zoom\"\n    s = 1-1/scale\n    col_c = s * (2*col_pct - 1)\n    row_c = s * (2*row_pct - 1)\n    return get_zoom_mat(1/scale, 1/scale, col_c, row_c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd298f5dadc23cd3ad3f2d5a681a7be2fb4591da"},"cell_type":"code","source":"@TfmAffine\ndef squish(scale:uniform=1.0, row_pct:uniform=0.5, col_pct:uniform=0.5):\n    \"Squish image by `scale`. `row_pct`,`col_pct` select focal point of zoom\"\n    if scale <= 1: \n        col_c = (1-scale) * (2*col_pct - 1)\n        return get_zoom_mat(scale, 1, col_c, 0.)\n    else:          \n        row_c = (1-1/scale) * (2*row_pct - 1)\n        return get_zoom_mat(1, 1/scale, 0., row_c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee8aba5a020890f1ced56d5dcc45a98f58686567"},"cell_type":"code","source":"rotate(xi(), 30).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d120ce6bbf44cd90da1c10ba3bb2d4e0393e298"},"cell_type":"code","source":"zoom(xi(), 0.6).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7366da39d681c03a2b5883af6239b8488555875"},"cell_type":"code","source":"zoom(xi(), 0.6).set_sample(padding_mode='zeros').show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0753f7f929994ea76880f80ccd12414d6d3aef15"},"cell_type":"code","source":"zoom(xi(), 2, 0.2, 0.2).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fb8dad230c0498c1c75c804cd9905fb674d5b94"},"cell_type":"code","source":"scales = [0.75,0.9,1.1,1.33]\n\n_,axes = plt.subplots(1,4, figsize=(12,3))\nfor i, ax in enumerate(axes): squish(xi(), scales[i]).show(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1aabf4f37fcca4864f5d54a7b7bbf1fac1bbb0c0"},"cell_type":"code","source":"img2 = rotate(xi(), 30).refresh()\nimg2 = zoom(img2, 1.6)\n_,axes=plt.subplots(1,3,figsize=(9,3))\nxi().show(axes[0])\nimg2.show(axes[1])\nzoom(rotate(xi(), 30), 1.6).show(axes[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52a89991ad6a7310fe40cc5e8612c51d55060155"},"cell_type":"code","source":"xi().show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09bf717137d5b643b4404b05c58ba1b47035f67b"},"cell_type":"code","source":"xi().resize(48).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e9542790e04997e70ad3a6c4d0600a4a9227790"},"cell_type":"code","source":"img2 = zoom(xi().resize(48), 1.6, 0.8, 0.2)\nrotate(img2, 30).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b16f29e10cd287336543edc7f9e70dd8b6e8e3e8"},"cell_type":"code","source":"img2 = zoom(xi().resize(24), 1.6, 0.8, 0.2)\nrotate(img2, 30).show(hide_axis=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a714efb40bfaced7185cf62e91fd314a0f347523"},"cell_type":"code","source":"img2 = zoom(xi().resize(48), 1.6, 0.8, 0.2)\nrotate(img2, 30).set_sample(mode='nearest').show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"807928ee506c31d4ae69fdf16a61c52c8d39afb8"},"cell_type":"markdown","source":"## Random affine\nAs we did with the Lighting transform, we now want to build randomness into our pipeline so we can increase the automatization of the transform process.\n\nWe will use a uniform distribution for both our transforms since their impact is linear and their domain is the real numbers.\n\n### Apply all transforms\n\nWe will make all transforms try to do as little calculations as possible.\n\nWe do only one affine transformation by multiplying all the affine matrices of the transforms, then we apply to the coords any non-affine transformation we might want (jitter, elastic distorsion). Next, we crop the coordinates we want to keep and, by doing it before the interpolation, we don't need to compute pixel values that won't be used afterwards. Finally we perform the interpolation and we apply all the transforms that operate pixelwise (brightness, contrast)."},{"metadata":{"trusted":true,"_uuid":"b3c18669dee095498ab79a9bdb46b346f7cfc643"},"cell_type":"code","source":"tfm = rotate(degrees=(-45,45.), p=0.75); tfm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2819f96b758d0265d34b6f8698df1b30010a3e7d"},"cell_type":"code","source":"tfm.resolve(); tfm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d4da1ed35579f7544654443cc11e71052198833"},"cell_type":"code","source":"x = xi()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e29bfffb38075693f83f244df0a36edcea88eaa"},"cell_type":"code","source":"_,axes = plt.subplots(1,4, figsize=(12,3))\nfor ax in axes: apply_tfms(tfm, x).show(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7294941da6c4ab3514b6b96a28a8366e7214991"},"cell_type":"code","source":"tfms = [rotate(degrees=(-45,45.), p=0.75),\n        zoom(scale=(0.5,2.0), p=0.75)]\n\n_,axes = plt.subplots(1,4, figsize=(12,3))\nfor ax in axes: apply_tfms(tfms,x).show(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e212bf45ab752f2bd0356a5906181d7256f9f484"},"cell_type":"code","source":"def apply_tfms(tfms:TfmList, x:TensorImage, do_resolve:bool=True, \n               xtra:Optional[Dict[Transform,dict]]=None, size:TensorImageSize=None, **kwargs:Any)->TensorImage:\n    \"Apply `tfms` to x, resize to `size`. `do_resolve` rebind random params. `xtra` custom args for a tfm\"\n    if not (tfms or size): return x\n    if not xtra: xtra={}\n    tfms = sorted(listify(tfms), key=lambda o: o.tfm.order)\n    if do_resolve: resolve_tfms(tfms)\n    x = x.clone()\n    if kwargs: x.set_sample(**kwargs)\n    if size: x.resize(size)\n    for tfm in tfms:\n        if tfm.tfm in xtra: x = tfm(x, **xtra[tfm.tfm])\n        else:               x = tfm(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89a265e4ca36c4c74fde663ebcf20a89a1a05b34"},"cell_type":"code","source":"tfms = [rotate(degrees=(-45,45.), p=0.75),\n        zoom(scale=(1.0,2.0), row_pct=(0,1.), col_pct=(0,1.))]\n\n_,axes = plt.subplots(1,4, figsize=(12,3))\nfor ax in axes: apply_tfms(tfms,x, padding_mode='zeros', size=64).show(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f115407de891af2c4164754be799b503e0baa2b"},"cell_type":"code","source":"tfms = [squish(scale=(0.5,2), row_pct=(0,1.), col_pct=(0,1.))]\n\n_,axes = plt.subplots(1,4, figsize=(12,3))\nfor ax in axes: apply_tfms(tfms,x).show(ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bd95a6f93021c12357ef4683a242c7976d866d1"},"cell_type":"markdown","source":"## Coord and pixel\n\n### Jitter / flip\nThe last two transforms we will use are jitter and flip.\n\n### Jitter\n\nJitter is a transform which adds a random value to each of the pixels to make them somewhat different than the original ones. In our implementation we first get a random number between (-1, 1) and we multiply it by a constant M which scales it.\n\nParameters\n\n1. **Magnitude** How much random noise do we want to add to each of the pixels in our image.\n\n    Domain: Real numbers between 0 and 1.\n\n### Flip\n\nFlip is a transform that reflects the image on a given axis.\n\nParameters\n\n1. **P** Probability of applying the transformation to an input.\n\n    Domain: Real numbers between 0 and 1."},{"metadata":{"trusted":true,"_uuid":"840bb10a94ef59b74a71d11901f6c8088c74d0c2"},"cell_type":"code","source":"class TfmCoord(Transform): order,_wrap = 4,'coord'\n\n@TfmCoord\ndef jitter(c, size, magnitude:uniform):\n    return c.add_((torch.rand_like(c)-0.5)*magnitude*2)\n\n@TfmPixel\ndef flip_lr(x): return x.flip(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9a1c3e501354bf7c945e8a495412d2597a36cf5"},"cell_type":"code","source":"tfm = jitter(magnitude=(0,0.1))\n\n_,axes = plt.subplots(1,4, figsize=(12,3))\nfor ax in axes:\n    tfm.resolve()\n    tfm(xi()).show(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dd1a6e184c5848c75a0a8427d19dc00cee7a4af"},"cell_type":"code","source":"tfm = flip_lr(p=0.5)\n\n_,axes = plt.subplots(1,4, figsize=(12,3))\nfor ax in axes:\n    tfm.resolve()\n    tfm(xi()).show(ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffb187d63fa271b7ca179486e1edc3924cce1757"},"cell_type":"markdown","source":"## Crop/Pad\n\n**Crop**\n\nCrop is a transform that cuts a series of pixels from an image. It does this by removing rows and columns from the input image.\n\n_Parameters_\n\n1. **Size** What is the target size of each side in pixels. If only one number *s* is specified, image is made square with dimensions *s* \\* *s*.\n\n    Domain: Positive integers.\n    \n2. **Row_pct** Determines where to cut our image vertically on the bottom and top (which rows are left out). If <0.5, more rows will be cut in the top than in the bottom and viceversa (varies linearly).\n\n    Domain: Real numbers between 0 and 1.\n    \n3. **Col_pct** Determines where to cut our image horizontally on the left and right (which columns are left out). If <0.5, more rows will be cut in the left than in the right and viceversa (varies linearly).\n\n    Domain: Real numbers between 0 and 1.\n    \nOur three parameters are related with the following equations:\n\n1. output_rows = [**row_pct***(input_rows-**size**):**size**+**row_pct***(input_rows-**size**)]\n\n2. output_cols = [**col_pct***(input_cols-**size**):**size**+**col_pct***(input_cols-**size**)]\n\n**Pad**\n\n\nPads each of the four borders of our image with a certain amount of pixels. Can pad with reflection (reflects border pixels to fill new pixels) or zero (adds black pixels). \n\n_Parameters_\n\n1. **Padding** Amount of pixels to add to each border. [More details](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad)\n\n    Domain: Positive integers.\n    \n2. **Mode** How to fill new pixels. For more detail see the Pytorch subfunctions for padding.\n\n    Domain: \n    - Reflect (default): reflects opposite pixels to fill new pixels. [More details](https://pytorch.org/docs/stable/nn.html#torch.nn.ReflectionPad2d)\n    - Constant: adds pixels with specified value (default is 0, black pixels) [More details](https://pytorch.org/docs/stable/nn.html#torch.nn.ConstantPad2d)\n    - Replicate: replicates border row or column pixels to fill new pixels [More details](https://pytorch.org/docs/stable/nn.html#torch.nn.ReplicationPad2d)\n    \n    \n***On using padding and crop***\n\nA nice way to use these two functions is to combine them into one transform. We can add padding to the image and then crop some of it out. This way, we can create a new image to augment our training set without losing image information by cropping. Furthermore, this can be done in several ways (modifying the amount and type of padding and the crop style) so it gives us great flexibility to add images to our training set. You can find an example of this in the code below."},{"metadata":{"trusted":true,"_uuid":"3d3627ec86e4113138b8f090adf56124db2963e8"},"cell_type":"code","source":"[(o.__name__,o.order) for o in\n    sorted((Transform,TfmAffine,TfmCoord,TfmLighting,TfmPixel),key=attrgetter('order'))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13a2556d98f9a17783d1642a4c4ccd66716be268"},"cell_type":"code","source":"@partial(TfmPixel, order=-10)\ndef pad(x, padding, mode='reflect'):\n    \"Pad `x` with `padding` pixels. `mode` fills in space ('reflect','zeros',etc)\"\n    return F.pad(x[None], (padding,)*4, mode=mode)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef76817506d4d557662b4913119c22a6ec93de02"},"cell_type":"code","source":"@TfmPixel\ndef crop(x, size, row_pct:uniform=0.5, col_pct:uniform=0.5):\n    \"Crop `x` to `size` pixels. `row_pct`,`col_pct` select focal point of crop\"\n    size = listify(size,2)\n    rows,cols = size\n    row = int((x.size(1)-rows+1) * row_pct)\n    col = int((x.size(2)-cols+1) * col_pct)\n    return x[:, row:row+rows, col:col+cols].contiguous()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0afaf29d98f93d94e5aee7b2a41599c974eaa41"},"cell_type":"code","source":"pad(xi(), 4, 'constant').show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15eafdeae1956c874a8e53b00f2e3831e40e006e"},"cell_type":"code","source":"crop(pad(xi(), 4, 'constant'), 32, 0.25, 0.75).show(hide_axis=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e989969af29f52ffad534f62b9b6259ddeddac8"},"cell_type":"code","source":"crop(pad(xi(), 4), 32, 0.25, 0.75).show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adf46d397816743f65c99ca525de372bbf48dc6d"},"cell_type":"markdown","source":"## Combine Transformations"},{"metadata":{"trusted":true,"_uuid":"6bb3f17af92bf8fd46ba270dac863713505ab3f8"},"cell_type":"code","source":"tfms = [flip_lr(p=0.5),\n        pad(padding=4, mode='constant'),\n        crop(size=32, row_pct=(0,1.), col_pct=(0,1.))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f02a8ca520bb5ead008fbb72849ad822d1196eae"},"cell_type":"code","source":"_,axes = plt.subplots(1,4, figsize=(12,3))\nfor ax in axes: apply_tfms(tfms, x).show(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ca0856bbc887b84f07894d78b11e91776b62f8f"},"cell_type":"code","source":"tfms = [\n    flip_lr(p=0.5),\n    contrast(scale=(0.5,2.0)),\n    brightness(change=(0.3,0.7)),\n    rotate(degrees=(-45,45.), p=0.5),\n    zoom(scale=(0.5,1.2), p=0.8)\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4ee1ba9028ca70b22aefcf5b6883b3448cf5c8c"},"cell_type":"code","source":"_,axes = plt.subplots(1,4, figsize=(12,3))\nfor ax in axes: apply_tfms(tfms, x).show(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f71f2d9c766e2d97bc8dc75762e9e19e4b29527b"},"cell_type":"code","source":"_,axes = plt.subplots(2,4, figsize=(12,6))\n\nfor i in range(4):\n    apply_tfms(tfms, x, padding_mode='zeros', size=48).show(axes[0][i], hide_axis=False)\n    apply_tfms(tfms, x, mode='nearest', do_resolve=False).show(axes[1][i], hide_axis=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc484c521db2e90deee94fdb73f6c8b3f72118ad"},"cell_type":"markdown","source":"## Random Resized Crop (TorchVision Version)"},{"metadata":{"trusted":true,"_uuid":"faa1b4ff4a8bfda423adcc33be9b05d2b863f950"},"cell_type":"code","source":"def compute_zs_mat(sz:TensorImageSize, scale:float, squish:float, \n                   invert:bool, row_pct:float, col_pct:float)->AffineMatrix:\n    \"Utility routine to compute zoom/squish matrix\"\n    orig_ratio = math.sqrt(sz[2]/sz[1])\n    for s,r,i in zip(scale,squish, invert):\n        s,r = math.sqrt(s),math.sqrt(r)\n        if s * r <= 1 and s / r <= 1: #Test if we are completely inside the picture\n            w,h = (s/r, s*r) if i else (s*r,s/r)\n            w /= orig_ratio\n            h *= orig_ratio\n            col_c = (1-w) * (2*col_pct - 1)\n            row_c = (1-h) * (2*row_pct - 1)\n            return get_zoom_mat(w, h, col_c, row_c)\n        \n    #Fallback, hack to emulate a center crop without cropping anything yet.\n    if orig_ratio > 1: return get_zoom_mat(1/orig_ratio**2, 1, 0, 0.)\n    else:              return get_zoom_mat(1, orig_ratio**2, 0, 0.)\n\n@TfmCoord\ndef zoom_squish(c, size, scale:uniform=1.0, squish:uniform=1.0, invert:rand_bool=False, \n                row_pct:uniform=0.5, col_pct:uniform=0.5):\n    #This is intended for scale, squish and invert to be of size 10 (or whatever) so that the transform\n    #can try a few zoom/squishes before falling back to center crop (like torchvision.RandomResizedCrop)\n    m = compute_zs_mat(size, scale, squish, invert, row_pct, col_pct)\n    return affine_mult(c, FloatTensor(m))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d8c3c724fd3ffddc78407668103e8ec3de828c3"},"cell_type":"code","source":"rrc = zoom_squish(scale=(0.25,1.0,10), squish=(0.5,1.0,10), invert=(0.5,10),\n                                  row_pct=(0,1.), col_pct=(0,1.))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7342b55345125b5bf45f2ca95fad3ece334153ba"},"cell_type":"code","source":"_,axes = plt.subplots(2,4, figsize=(12,6))\nfor i in range(4):\n    apply_tfms(rrc, x, size=48).show(axes[0][i])\n    apply_tfms(rrc, x, do_resolve=False, mode='nearest').show(axes[1][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6245a89bfa18c5aca3b2b9480be7e10df2d1d58c"},"cell_type":"code","source":"# Clean up (to avoid errors)\n!rm -rf data\n!rm -rf nb_001b.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5c3c0477b042e05df4ba84864b2bffdec41a487"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}