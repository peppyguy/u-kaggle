{"cells":[{"metadata":{"_uuid":"6a1864d392b7763e4efcb01cdb13337bf01034df","_execution_state":"idle","trusted":false},"cell_type":"markdown","source":"\n\n# Introduction\n\nThis Kernel involves weekly returns for the S&P 500 stock index between 1990 and 2010. This data will initially be explored and looked at with numerical and graphical summaries to identify patterns (or lack of patterns). Then various classification models (logistic regression, linear discriminant analysis, quadratic discriminant analysis, and k-nearest neighbors) will be used on the data. Different combinations and transformations of predictors will also be looked at. Models will be analyzed and compared. \n\n# Data\n\nThe data set is from the ISLR package. The data set is called Weekly. As mentioned in the introduction, Weekly has data on weekly returns for the S&P 500 stock index between 1990 and 2010. Specifically, it contains 1089 observations (weekly returns) on the following 9 variables listed in the table below along with a description.\n"},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true,"_uuid":"98a96b1d3915220a739554ae13df69c2e9b0d5f4"},"cell_type":"code","source":"rm(list=ls()) # Clear the workspace\ngraphics.off() # Clear graphics\nlibrary(ggplot2) # Plotting\nlibrary(knitr) # kable\nlibrary(GGally) # ggpairs plot\nlibrary(ISLR) # Source of Data\nlibrary(MASS) # Some Classification Models (LDA, QDA)\nlibrary(class) #KNN\nlibrary(caret) # Showing Confusion Matrix Data\nlibrary(tidyverse) # Tidy up data\nattach(Weekly)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13d2e24af1a2ecee2842f54992c26cef395fd8e3"},"cell_type":"markdown","source":"___\n<center>\nTable 1: Variables Names and Descriptions of Data Set\n</center>"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true,"_uuid":"60f10398b72c3ab36cdb776338e204a220a40ee9"},"cell_type":"code","source":"\nVariable = c(\"Year\", \"Lag1\", \"Lag2\", \"Lag3\", \"Lag4\", \"Lag5\", \"Volume\", \"Today\",\"Direction\")\nDescription = c(\"The year that the observation was recorded\",\n                \"Percentage return for previous week\",\n                \"Percentage return for 2 weeks previous\",\n                \"Percentage return for 3 weeks previous\",\n                \"Percentage return for 4 weeks previous\",\n                \"Percentage return for 5 weeks previous\",\n                \"Volume of shares traded (avg. daily shares traded in billions)\", \n                \"Percentage return for this week\",\n                \"Down or Up indicating market's return on given week\")\ndf = data.frame(Variable, Description)\nkable(df, format = 'markdown')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"3147225c374f7fb65560cc29a0b49c58f43c700a"},"cell_type":"code","source":"### Helper Functions\n\n# Mostly From https://stackoverflow.com/questions/23891140/r-how-to-visualize-confusion-matrix-using-the-caret-package\ndraw_confusion_matrix <- function(cm) {\n\n  layout(matrix(c(1,1,2)))\n  par(mar=c(2,2,2,2))\n  plot(c(100, 345), c(300, 450), type = \"n\", xlab=\"\", ylab=\"\", xaxt='n', yaxt='n')\n  title('CONFUSION MATRIX', cex.main=2)\n\n  # create the matrix \n  rect(150, 430, 240, 370, col='#3F97D0')\n  text(195, 435, 'Down', cex=1.2)\n  rect(250, 430, 340, 370, col='#F7AD50')\n  text(295, 435, 'Up', cex=1.2)\n  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)\n  text(245, 450, 'Actual', cex=1.3, font=2)\n  rect(150, 305, 240, 365, col='#F7AD50')\n  rect(250, 305, 340, 365, col='#3F97D0')\n  text(140, 400, 'Down', cex=1.2, srt=90)\n  text(140, 335, 'Up', cex=1.2, srt=90)\n\n  # add in the cm results \n  res <- as.numeric(cm$table)\n  text(195, 400, res[1], cex=1.6, font=2, col='white')\n  text(195, 335, res[2], cex=1.6, font=2, col='white')\n  text(295, 400, res[3], cex=1.6, font=2, col='white')\n  text(295, 335, res[4], cex=1.6, font=2, col='white')\n\n  # add in the specifics \n  plot(c(100, 0), c(100, 0), type = \"n\", xlab=\"\", ylab=\"\", main = \"DETAILS\", xaxt='n', yaxt='n')\n  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)\n  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)\n  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)\n  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)\n  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)\n  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)\n  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)\n  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)\n  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)\n  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)\n\n  # add in the accuracy information \n  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)\n  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)\n  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)\n  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)\n}  \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c2ea6b3e32cefdd4b171b2dfa52286a9232d996"},"cell_type":"markdown","source":"___\n\n# Analysis\n\nThe Weekly data will be looked at in exploratory analysis. Then the full data set will be used to create a logistic regression model. This model will be analyzed. \n\nFrom this model, the single most relevant predictor will be selected. This will be used as the only predictor in a logistic regression model. This model will also only use a subset of the full data as training data, while the other set is saved for testing purposes. Then the tests results of this model will be looked at. \n\nWith the same predictor and training/testing data subsets, this procedure is repeated for linear discriminant analysis, quadratic discriminant analysis, and k-nearest neighbors (k=1) models. Then, these methods will be compared to determine the one that provided the best results in this situation.\n\nAfterwards, other predictor combinations and transformations will be considered for the models. Also, different values of K for the k-nearest neighbors will be looked at. The model with the best results will be noted.\n\n## Exploratory Analysis\n\nIn this section, we will explore the data prior to any modeling. Some numerical and graphical summaries of the Weekly data will be produced and any patterns (or lack of patterns) will be noted. \n\nLet us take a look at the numerical summary of the Weekly data frame that is produced by R.\n___"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"52b92ddeca7ea1ba03a3628c3b8d8b0023bd5f66"},"cell_type":"code","source":"\nsummary(Weekly)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8765d4df71600f88039145374bf73ae2a26a1c12"},"cell_type":"markdown","source":"\n___\nHere, one can notice that Direction is a categorical variable (factor in R) with two categories, up and down. Recall, this variable indicates whether the market had a positive or negative return on a given week. This will be used as the response variable later.\n\nAlso, notice how all the Today and Lag variables have close to identical summary values for Min, 1st, 2nd, 3rd quartiles, and max. This points to the lag variables having a similar distribution. This can be seen visually with a pairs plot. We also color the plots and variables by Direction since Direction will be the main variable of concern as the response variable.\n___\n<center>\nFigure 1: Visual Look at the Dataset with Pairs Plot Colored by Direction\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"dbfe7a652cb409bbe0cc79631093a484c85ea70b"},"cell_type":"code","source":"\np = ggpairs(Weekly,\n        aes(colour = Direction, alpha=0.9), \n        upper = list(continuous = wrap(\"cor\", size = 2)),\n        diag = list(continuous = \"barDiag\"),\n        lower = list(continuous = \"smooth\"))\n        \nsuppressMessages(print(p))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef6c97613f5462fa186beb9d7a2104995857ab7c"},"cell_type":"markdown","source":"___\n\nOne can see the Today and Lag histograms in the diagonals look almost identical, which supports the numbers we saw in the numerical summary shown earlier. For Volume, the distribution looks skewed with some potential large outliers. This will be looked at later.\n\nFrom the two colors, one can see most of the plots and histograms do not show much differentiation between up and down (aside from Today, which Direction is based on). It looks like Volume versus Lag2 or Lag5 might show some difference between Up and Down. It could be a possible interaction.\n\nAlso, looking at the correlation values in the top right portion of the figure, one can see almost all the variables have no correlation with each other (< 0.1 in magnitude), aside from Volume and Year. This can also be seen in the generally flat trend lines on the bottom left (except for Volume).\n\nLooking the correlations between the Lag and Today variables in particular, they are all close to zero as one expect and show no obvious patterns with one another. In other words, there appears to be little correlation between current weekly returns and previous weeks’ returns. \n\nThe only substantial correlation is between Year and Volume, which seems to be intuitive. As the Year increases, one could guess the number of people making trades, the number of companies in the market, and the amount of money in the market would generally increase, which would lead to more shares being traded. This can be looked at visually.\n\n<center> \nFigure 2: How Volume Changes with Year\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b8291a53834b4112682d8e4d1aad302dd9eeffa3"},"cell_type":"code","source":"plot(Year,Volume)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14de52517575f03dcca4ae7ae6b342bcca617cf2"},"cell_type":"markdown","source":"\nBy plotting the data, we see that Volume is increasing over time. In other words, the average number of shares traded daily generally increases from 1990 to 2010. We also see the spread of the data also increases with the year. That is, there is more variance in the Volume as the Year increases.\n\nDirection is a categorical version of the Today variable and this is shown by plots for Today and Direction showing a cut off value for splitting the two categories. As stated earlier, Direction will be the variable that we will be modeling as the response variable.\n \n## Logistic Regression on Full Model\n\nWith the data explored, the full data set will be used to perform logistic regression with Direction as the response and the five lag variables plus Volume as predictors. \n\nThe model summary from R is printed below.\n\n___"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"8334cb58083c901c5e2adaffd79faa9e638259fd"},"cell_type":"code","source":"glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Weekly,family=binomial)\nsummary(glm.fit)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b5d90a97305d88f54fcffc42053184fca3db071"},"cell_type":"markdown","source":"\n___\n\nLooking at the coefficient summary section (the p-values or Pr(>|z|) in particular), one can see that Lag2 was the predictor that is statistically significant since it has a p-value less than a 0.05 significance level. The rest of the predictors are not significant at the 0.05 level.\n \nOne method to see how well a classification model has done is a confusion matrix, which is a table that can show the number of correct predictions from the model and the number of incorrect predictions. A number of statistics can be calculated from this table. \n___\n<center>\nFigure 3: Logistic Regression Results on Full Model\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"af215fff96d139a3c7491899eea5ca5d25813554"},"cell_type":"code","source":"glm.probs=predict(glm.fit,type=\"response\")\nglm.pred=rep(\"Down\",length(glm.probs))\nglm.pred[glm.probs>.5]=\"Up\"\ncmLogRegFull = confusionMatrix(data = factor(glm.pred), reference = Direction)\ndraw_confusion_matrix(cmLogRegFull)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f1afb4a4e8e9cf10363ed89daf0a746a8f6f9fd"},"cell_type":"markdown","source":"\n___\n\nIn the top table, this displays the number of predictions for each combination of predicting Direction Down or Up and whether Direction was truly Down or Up. The correct predictions are the elements down the diagonal. The errors or mispredictions are the off diagonal elements. These are often called False positives or negatives in binary classification. \n\nA binary classifier such as this one can make two types of errors: it can incorrectly assign a Direction of \"Up\" when the Direction is actually \"Down\", or it can incorrectly assign \"Down\" when it is actually \"Up\".\n\nThe overall fraction of correct predictions (or accuracy in the figure) is essentially the sum of the diagonal (correct) elements divided by the total number of observations (sum of the entire table). This equates to about 0.5610 for our situation, so our model correctly predicted the movement of the market 56.1% of the time.\n\nIt appears that the logistic regression model is working a little better than random guessing. However, this result can be misleading because we trained and tested the model on the same set of observations. In other words, 100−56.1=43.9% is the training error rate. The training error rate is often overly optimistic—it tends to underestimate the test error rate. Splitting the data up into a training set and testing set can help alleviate this issue.\n\n## Training Data and Testing Data\n\nIn order to better assess the accuracy of the logistic regression model (or any classification model) in this setting, we can fit the model using part of the data (training data), and then examine how well it predicts the held out data (testing data). This will yield a more realistic error rate. We are more interested in the model’s performance not on the data that we used to fit the model, but rather on future weeks for which the market’s movements are unknown.\n\nTo implement this strategy, we will first use the observations from 1990 to 2008 as the training data set. Also, since lag2 was the only significant variable at a 0.05 level, we will just use Lag2 as a predictor for Direction. The test data set will have observations from 2009 and onward.\n\nAfterwards, the model is trained on the training data set and then predictions are made for the test data. This is compared to the actual results of the test data. From this, a confusion matrix along with the overall fraction of correct predictions (accuracy) for the test data is calculated.\n\nThis process will be done with various classification models beginning with logistic regression. Then linear discriminant analysis, quadratic discriminant analysis, and k-nearest neighbors will also follow a similar process. Once that is complete, a comparison of the models will be conducted.\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d44dc34e1d32a1c8c2f0491d3ff429919f673a26"},"cell_type":"code","source":"# Split into training data\ntrain <- (Year < 2009)\nWeekly.2009_2010 <- Weekly[!train, ]\nDirection.2009_2010 <- Direction[!train]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd7ff8fe504f59e92fbe481b03bbbbeb2b36626f"},"cell_type":"markdown","source":"\n\n### Logistic Regression\n\nWe now fit a logistic regression model using only the training subset of the observations and Lag2 as the only predictor. We then obtain predicted probabilities of the stock market going up for each of the weeks in our test set. From this, we can compare the actual Direction in the test set with the predicted values and create a confusion matrix along with some statistics derived from the matrix.\n\n<center>\nFigure 4: Logistic Regression Results on Test Data (Lag2 only predictor)\n</center>"},{"metadata":{"trusted":true,"_uuid":"88218cb63c2618fcbca1a94b8f3eeaac9fbc6e14","_kg_hide-input":true},"cell_type":"code","source":"glm.fit2 = glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)\nprobs2 = predict(glm.fit2, Weekly.2009_2010, type = \"response\")\nglm.pred2 = rep(\"Down\", length(probs2))\nglm.pred2[probs2 > 0.5] = \"Up\"\ncmLogRegTest = confusionMatrix(data = factor(glm.pred2), reference = Direction.2009_2010)\ndraw_confusion_matrix(cmLogRegTest)\npropOfCorrectUpPredLogReg = cmLogRegTest$table[1]/(cmLogRegTest$table[1]+cmLogRegTest$table[3])\npropOfCorrectDownPredLogReg = cmLogRegTest$table[4]/(cmLogRegTest$table[4]+cmLogRegTest$table[2])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7342bbb69a089cf0a303850c9316aebe2470982c"},"cell_type":"markdown","source":"___\nThe percentage of correct predictions on the test data is 62.5%. 37.5% is the test error rate.\n\nIn other words, 62.5% of the weekly movements have been correctly predicted. The confusion matrix suggests that on days when logistic regression predicts that the market will decline, it is correct 64.29% of the time. On days when it predicts an increase in the market, it has a 62.22% accuracy.\n\nThis is interesting because the rates for the test set are better than the results of using the full data set earlier. Let us see how linear discriminant analysis performs in this situation\n\n### Linear Discriminant Analysis (LDA)\n\nWe now fit an LDA model using the same training subset of the observations. We then obtain predicted probabilities of the stock market going up for each of the weeks in the same test set. Like before, we can compare the actual Direction in the test set with the predicted values and create a confusion matrix along with some statistics derived from the matrix.\n\n<center>Figure 5: LDA Results on Test Data (Lag2 only predictor)\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f57c9377093a20caabed8af169c73983144f7fab"},"cell_type":"code","source":"\nlda.fit=lda(Direction ~ Lag2,data=Weekly,subset=train)\npred.lda = predict(lda.fit, Weekly.2009_2010)\ncmLda = confusionMatrix(data = factor(pred.lda$class), reference = Direction.2009_2010)\ndraw_confusion_matrix(cmLda)\npropOfCorrectUpPredLda = cmLda$table[1]/(cmLda$table[1]+cmLda$table[3])\npropOfCorrectDownPredLda = cmLda$table[4]/(cmLda$table[4]+cmLda$table[2])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"461062f67a3c28853d1c5c1aaed5e87e84b0491e"},"cell_type":"markdown","source":"The percentage of correct predictions on the test data is 62.5%. 37.5% is the test error rate. This is the same results as the logistic regression model.\n\nIn other words, 62.5% of the weekly movements have been correctly predicted. The confusion matrix suggests that on days when LDA predicts that the market will decline, it is correct 64.29% of the time. On days when it predicts an increase in the market, it has a 62.22% accuracy. Again, this is the same results as logistic regression model.\n\n### Quadratic Discriminant Analysis (QDA)\n\nWe now fit a QDA model using the same training subset of the observations. We then obtain predicted probabilities of the stock market going up for each of the weeks in the same test set. Like before, we can compare the actual Direction in the test set with the predicted values and create a confusion matrix along with some statistics derived from the matrix.\n\n<center>Figure 6: QDA Results on Test Data (Lag2 only Predictor)\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"cbd3daa06252625e175529fd75ed0c02581a00fa"},"cell_type":"code","source":"\n\nqda.fit=qda(Direction ~ Lag2,data=Weekly,subset=train)\nqda.class=predict(qda.fit,Weekly.2009_2010)$class\ncmQda = confusionMatrix(data = qda.class, reference = Direction.2009_2010)\ndraw_confusion_matrix(cmQda)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1013bb72a4fda9ab05ff884b2b177f6d42e2ce43"},"cell_type":"markdown","source":"\n<center>------------------------------\n</center>\n\nOne interesting observation is that the QDA model did not predict any cases for down with the test data set. Some of the statistics cannot be calculated properly without any predictions for one class. Regardless, 58.65% of the weekly movements have been correctly predicted.\n\n### K-Nearest Neighbors (KNN)\n\nWe now fit a K=1 KNN model using the same training subset of the observations. We then obtain predicted probabilities of the stock market going up for each of the weeks in the same test set. Like before, we can compare the actual Direction in the test set with the predicted values and create a confusion matrix along with some statistics derived from the matrix.\n\n<center>Figure 7: KNN Results on Test Data (K=1 and Lag2 only Predictor)\n</center>\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"187f05235c7bcbe3a74a900ae9be744ed7e8d434"},"cell_type":"code","source":"\ntrain.X=as.matrix(Lag2[train])\ntest.X=as.matrix(Lag2[!train])\ntrain.Direction=Direction[train]\nset.seed(1)\nknn.pred=knn(train = train.X, test = test.X, cl= train.Direction,k=1)\ncmKnn = confusionMatrix(data = factor(knn.pred), reference = Direction.2009_2010)\ndraw_confusion_matrix(cmKnn)\npropOfCorrectUpPredKnn = cmKnn$table[1]/(cmKnn$table[1]+cmKnn$table[3])\npropOfCorrectDownPredKnn = cmKnn$table[4]/(cmKnn$table[4]+cmKnn$table[2])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b368b4bb4dc80cc5ce67cdc613b9cd947f38b72a"},"cell_type":"markdown","source":"The percentage of correct predictions on the test data is 50%. 50% is the test error rate.\n\nIn other words, 50% of the weekly movements have been correctly predicted. The confusion matrix suggests that on days when KNN (K=1) predicts that the market will decline, it is correct 41.18% of the time. On days when it predicts an increase in the market, it has a 58.49% accuracy.\n\n### Lag2 Predictor Model Comparison\n\nNow, we will compare the models with each other to see how they perform relative to each other and to provide the model with the best results. To compare the models, a table was created below to easily see the different numbers and results that were attained.\n\n<center>Table 2: Lag2 Only Predictor Model Comparison\n</center>\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"62fc01ccd7e0afa70e9a67db0a7fbc8c2971ea71"},"cell_type":"code","source":"\ncomparisonResults = cbind(cmLogRegTest$overall[\"Accuracy\"], cmLda$overall[\"Accuracy\"], cmQda$overall[\"Accuracy\"],cmKnn$overall[\"Accuracy\"])\ncolnames(comparisonResults) = c(\"LogReg\",\"LDA\",\"QDA\",\"KNN\")\nrownames(comparisonResults) = \"Overall Correct Prediction Proportion\"\nkable(comparisonResults, format = 'markdown')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9e6adacea17ba53b9a516849adbe0bc5b61525a"},"cell_type":"markdown","source":"___\n\nFrom Table 2 overall correct prediction proportion row, the logistic regression model and linear discrimination analysis performed the best of the four different models. Both had the same highest overall fraction of correct prediction of 0.625; hence, it had the lowest test error rate.\n\n## Predictor and Number of K Experimentation\n\nThe previous models that were looked at either used just Lag2 as the only predictor or included all the predictors. This section will look at potential combinations of predictors, including possible transformations and interactions, for each of the methods. \n\nThis section will also look at how varying K in KNN and how it can change the overall accuracy.\n\n### Varying K in KNN Model\n\nUsing K=1 makes KNN flexible and may be too flexible in our case. Let us look at how changing k changes our overall prediction accuracy using the same setup as before.\n\nWe shall use repeated cross validation on the training data to determine the optimal K (10-fold CV repeated 3 times in this case). The figure below gives accuracy as K (#Neighbors) is changed.\n\n<center>\nFigure 8: Accuracy versus K with Lag2 as only Predictor (on Training Data)\n</center>\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7812cf887754b5d3393cc2f2523a205a17659e9e"},"cell_type":"code","source":"\n# Setup CV for selecting optimal K (using our training data)\ntrctrl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\nset.seed(1)\nknn_fit <- train(Direction ~ Lag2,data=Weekly[train,], method = \"knn\",\n trControl=trctrl,\n preProcess = c(\"center\", \"scale\"),\n tuneLength = 25)\nplot(knn_fit)\n# Predict values using test data\ntest_pred = predict(knn_fit, newdata=Weekly[!train,])\ncmKnnLag2 = confusionMatrix(factor(test_pred), Direction.2009_2010)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"688c300cf50d184b5ef30940ae8e2c4f8f9bc229"},"cell_type":"markdown","source":"\n___\nFrom Figure 8, we can see k=1 produced the worst possible overall prediction accuracy. After k-1, the accuracy seems to peak around 0.56 near k=25. Afterwards, the accuracy begins to fall off. Below is more details on the model.\n___"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f247d9b87e7154e8bba1585e9831d6f767621d51"},"cell_type":"code","source":"knn_fit","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a2fc5db7df0be117fad2f9db5ac6163dfa05d41"},"cell_type":"markdown","source":"\n___\nHere we see K=25 had the best accuracy as accuracy was used to select the optimal model. This training model was used to predict Direction for the test data. Like before, we can compare the actual Direction in the test data with the predicted values and create a confusion matrix along with some statistics derived from the matrix.\n<center>Figure 9: KNN Results on Test Data (K=25 and Lag2 only Predictor)\n</center>\n"},{"metadata":{"trusted":true,"_uuid":"d236eebba4d07942a4c88249ef114884ac1215f2"},"cell_type":"code","source":"draw_confusion_matrix(cmKnnLag2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"902cd1f1f2e7ca02633230975b2ac6fcb7f2afb3"},"cell_type":"markdown","source":"\n___\n\nHere, we can see an accuracy of 0.538. This is better than the 0.50 accuracy of the initial KNN model with K=1.\n\nLet us now look at potential predictor combinations and interactions.\n\n### Volume Interaction\n\nAs noted earlier, Volume looked like it had potential interactions with Lag2 and Lag5 after sorting/coloring by Direction. Here we will look at the potential interaction between Lag2 and Volume, as Lag2 was the most significant variable in the earlier full Logistic Regression Model.\n\nA similar procedure was performed as in the Lag2 only predictor scenario, except now Volume and a Lag2/Volume interaction term has been added. For space and reduce redundancy purposes, only the interesting results are presented instead of the full results from each model (Logistic, LDA, QDA, KNN). Let us look at the results from QDA.\n<center>Figure 10: QDA Results on Test Data with Volume, Lag2, and Interaction\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5958430685507368fa4b84613dc7678112052828"},"cell_type":"code","source":"glm.fit2 = glm(Direction ~ Lag2:Volume, data = Weekly, family = binomial, subset = train)\nprobs2 = predict(glm.fit2, Weekly.2009_2010, type = \"response\")\nglm.pred2 = rep(\"Down\", length(probs2))\nglm.pred2[probs2 > 0.5] = \"Up\"\ncmGlmLag2Vol = confusionMatrix(data = factor(glm.pred2), reference = Direction.2009_2010)\n\nlda.fit=lda(Direction ~ Lag2:Volume,data=Weekly,subset=train)\npred.lda = predict(lda.fit, Weekly.2009_2010)$class\ncmLdaLag2Vol = confusionMatrix(data = factor(pred.lda), reference = Direction.2009_2010)\n\nqda.fit=qda(Direction ~ Lag2:Volume,data=Weekly,subset=train)\nqda.class=predict(qda.fit,Weekly.2009_2010)$class\ncmQdaLag2Vol = confusionMatrix(data = factor(qda.class), reference = Direction.2009_2010)\ndraw_confusion_matrix(cmQdaLag2Vol)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00dea2f76a73959ddda2ef41082284369c654f6e"},"cell_type":"markdown","source":"___\n\nHere QDA produces both up and down predictions in this case. The case when Lag2 was the only predictor, the QDA model had no Down predictions. Although, the overall accuracy in this case was lower than using only Lag2. This was true for most of the models. \n\nFor KNN, let us look at the optimal K to get the best accuracy (based on the training data). Like before, we will use 10-fold CV repeated 3 times to help determine the optimal K.\n___\n<center>\nFigure 11: Accuracy of varying K with Lag2, Volume And Lag2/Volume Interaction (Training Data)\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"565715c51cff7155dec16c958f9a948750871b92"},"cell_type":"code","source":"# Setup CV for selecting optimal K (using our training data)\ntrctrl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\nset.seed(1)\nknn_fit <- train(Direction ~ Lag2:Volume,data=Weekly[train,], method = \"knn\",\n trControl=trctrl,\n preProcess = c(\"center\", \"scale\"),\n tuneLength = 25)\nplot(knn_fit)\n# Predict values using test data\ntest_pred = predict(knn_fit, newdata=Weekly[!train,])\ncmKnnLag2Vol = confusionMatrix(factor(test_pred), Direction.2009_2010)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f048253398dcf207b4b42afd97f4f3fa5b988a2"},"cell_type":"markdown","source":"\n___\nFrom Figure 11, one can see the accuracy peaks around 0.57 with a K near 35. Below is more detailed results for the model.\n___"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ba1f265c164f0a5f68a100891ab333c35e44fdeb"},"cell_type":"code","source":"knn_fit","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37995314ecda027120b038f6dccb44203a6e6738"},"cell_type":"markdown","source":"\n___\nHere we see K=33 had the best accuracy as accuracy was used to select the optimal model. The predicted values from this training model was used against the test data results to get a test accuracy. Let us see how each classification model compares with each other when including Volume and its interaction.\n___\n<center>\nTable 3: Model Comparison of Accuracy for using Lag2, Volume, and Lag2*Volume Predictors\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"68e68c01b88d6b630d5496b73be528332202bbd3"},"cell_type":"code","source":"Lag2.Vol.Lag2_Vol = c(cmGlmLag2Vol$overall[\"Accuracy\"],\n                    cmLdaLag2Vol$overall[\"Accuracy\"],\n                    cmQdaLag2Vol$overall[\"Accuracy\"],\n                    cmKnnLag2Vol$overall[\"Accuracy\"])\nnames(Lag2.Vol.Lag2_Vol) = colnames(comparisonResults)\nLag2.Vol.Lag2_Vol = t(as.data.frame(Lag2.Vol.Lag2_Vol))\nkable(Lag2.Vol.Lag2_Vol, format = 'markdown')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"974751c6df00b8274b0df72a2b7e324d1f834176"},"cell_type":"markdown","source":"\n___\nFrom Table 3, one can see KNN did worse than guessing and that logistic regression produced the best accuracy. Let us take a closer look at the logistic regression model.\n___\n<center>\nFigure 12: Logistic Regression Results on Test Data with Volume, Lag2, and Interaction\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"dfca7a09a016b6004a5f2a7f1cdb22b8d66d84b2"},"cell_type":"code","source":"draw_confusion_matrix(cmGlmLag2Vol)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b93f1d2b397564cbb2cc201401486514ee825f0"},"cell_type":"markdown","source":"\n\n___\nFrom Figure 12, one can see the model has an accuracy of 0.615, a sensitivity (or recall) of 0.209, and specificity of 0.902. These are all very similar results to just using Lag2 as a predictor and points to negligible influence provided by including Volume. In fact, the overall accuracy is a bit less than the 0.625 from just using Lag2 as a predictor.\n\nLet us take a look at transformation of a variable.\n\n### Volume Transformation\n\nAs mentioned earlier, the Volume distribution was skewed. Here, we apply a log transformation to the variable and see how the distribution changes.\n____\n<center>\nFigure 13: Log Volume vs Raw Volume Distributions\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"dc426dd27c0d62ba9772b0aabe05fb3209dd198e"},"cell_type":"code","source":"volumePlotting = as.data.frame(cbind(Volume, log(Volume)))\ncolnames(volumePlotting) = c(\"Volume\",\"Log Volume\")\nvolumePlotting %>%\n  keep(is.numeric) %>% \n  gather() %>% \n  ggplot(aes(value)) +\n    facet_wrap(~ key, scales = \"free\") +\n    geom_histogram(bins=sqrt(nrow(volumePlotting)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"120e2c6bc0d79c7ad1675cec9a85a85d4e29eb5a"},"cell_type":"markdown","source":"___\nFrom Figure 13, one can see the log transformation allowed the distribution to be less skewed than the original distribution for Volume.\n\nBefore comparing different model accuracy values, an optimal K needs to be found for KNN. As before, it will have the same predictors of Lag2, Volume, and the Volume/Lag2 interaction, but now, Volume will be logged. Due to computational resources (and a decent sized optimal k), an optimal K will be determined from the training data using 5-fold CV repeated 2 times (instead of 10-fold CV repeated 3 times).\n\n___\n<center>\n\nFigure 14: Accuracy of varying K with Lag2, LogVolume And Lag2/LogVolume Interaction (Training Data)\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"2e01797020328f59e3cef6529eb08d0f2baf1b42"},"cell_type":"code","source":"# Setup CV for selecting optimal K (using our training data)\ntrctrl <- trainControl(method = \"repeatedcv\", number = 5, repeats = 2)\nset.seed(1)\nknn_fit <- train(Direction ~ Lag2:log(Volume),data=Weekly[train,], method = \"knn\",\n trControl=trctrl,\n preProcess = c(\"center\", \"scale\"),\n tuneLength = 100)\nplot(knn_fit)\n# Predict values using test data\ntest_pred = predict(knn_fit, newdata=Weekly[!train,])\ncmKnnLag2LogVol = confusionMatrix(factor(test_pred), Direction.2009_2010)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48cc949091d9c6f22c81ac4475ea0aa8ff6dc006"},"cell_type":"markdown","source":"___\nFrom Figure 14, one can the best accuracy is around 0.55 with a K value of 189 (due to number of K values iterated through, the results of the KNN training model will not presented). The predicted values from this training model was used against the test data results to get a test accuracy for KNN.\n\nLet us see how each classification model compares with each other when including Lag2, LogVolume and their interaction.\n\n___\n<center>\nTable 4: Model Comparison of Accuracy for using Lag2, LogVolume, and Lag2*LogVolume Predictors\n</center>"},{"metadata":{"trusted":true,"_uuid":"06ed408652f6171bb9e8464c85f499ca1a89f656"},"cell_type":"code","source":"glm.fit2 = glm(Direction ~ Lag2:log(Volume), data = Weekly, family = binomial, subset = train)\nprobs2 = predict(glm.fit2, Weekly.2009_2010, type = \"response\")\nglm.pred2 = rep(\"Down\", length(probs2))\nglm.pred2[probs2 > 0.5] = \"Up\"\ncmGlmLag2LogVol = confusionMatrix(factor(glm.pred2), Direction.2009_2010)\n\n\nlda.fit=lda(Direction ~ Lag2:log(Volume),data=Weekly,subset=train)\npred.lda = predict(lda.fit, Weekly.2009_2010)$class\n\nqda.fit=qda(Direction ~ Lag2:log(Volume),data=Weekly,subset=train)\nqda.class=predict(qda.fit,Weekly.2009_2010)$class\n\nLag2.LogVol.Lag2_LogVol = c(cmGlmLag2LogVol$overall[\"Accuracy\"],\n                    mean(pred.lda == Direction.2009_2010),\n                    mean(qda.class == Direction.2009_2010),\n                    cmKnnLag2LogVol$overall[\"Accuracy\"])\nnames(Lag2.LogVol.Lag2_LogVol) = colnames(comparisonResults)\nLag2.LogVol.Lag2_LogVol = t(as.data.frame(Lag2.LogVol.Lag2_LogVol))\nkable(Lag2.LogVol.Lag2_LogVol, format = 'markdown')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14523a88433c7287f2ebb0b38298b02eba6d3114"},"cell_type":"markdown","source":"\n___\n\nFrom Table 4, one can see that all the models had similar results that were close to each other. Logistic regression and LDA had the most accurate predictions; however, these were slightly worse than not transforming the variable. This points to potentially not needing to transform Volume. Regardless, we shall take a look at the logistic regression results."},{"metadata":{"_uuid":"b95651118cf2ac72e8d34fdd7b5cc601bf5451f9"},"cell_type":"markdown","source":"___\n<center>\nFigure 15: Logistic Regression Results on Test Data with LogVolume, Lag2, and Interaction\n</center>\n"},{"metadata":{"trusted":true,"_uuid":"e8e1d1f70ea4725c6cbc60298a782ad2e9e90e7b"},"cell_type":"code","source":"draw_confusion_matrix(cmGlmLag2LogVol)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cff7ce9bd3b1b97e7e62cefc06e2b53d83fc9a84"},"cell_type":"markdown","source":"\n___\n\nHere, we can see there are only a few test data points that are predicted Down. The model has a 50-50 chance to get it right when it does predict down (0.5 Precision).\n\nNow that we have tried different predictor combinations (including interactions) along with varying values for K in KNN. We will compare all models we have looked at.\n\n## Final Model Comparison\n\nThis section will see how the models we created compare with one another based on accuracy. Below is a table that summarizes the accuracy values presented earlier. It should be noted that KNN values correspond to the optimal K values discussed earlier.\n\n___\n<center>\nTable 5: Model Comparison of Overall Prediction Accuracy for each Predictor Combination\n</center>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e8a9b5142de4a21d7a82d4cf15c3936a0dbff352"},"cell_type":"code","source":"finalComparisonResults = comparisonResults\nfinalComparisonResults[,\"KNN\"] = cmKnnLag2$overall[\"Accuracy\"] # Use best KNN accuracy\nrownames(finalComparisonResults) = \"Lag2\"\nfinalComparisonResults = rbind(finalComparisonResults, Lag2.Vol.Lag2_Vol)\nfinalComparisonResults = rbind(finalComparisonResults, Lag2.LogVol.Lag2_LogVol)\nkable(finalComparisonResults, format = 'markdown')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b73f9dd065180a47d3e10a22c3ca7a001305a19"},"cell_type":"markdown","source":"___\n\nFrom Table 5, one can see that the model with just Lag2 as the predictor performs the best for all the models except KNN. For KNN, having Lag2, LogVolume, and a Lag2/LogVolume interaction produced the best accuracy for KNN. Continuing with KNN, it is interesting the Lag2, Volume, and Lag2/Volume interaction model had a worse performance that just guessing since it was below 0.50.\n\nRegardless, logistic regression had the best accuracy for each combination of predictors and transformations. LDA had very similar results. Looking at logistic regression and LDA accuracy values, adding Volume or a log transformation of Volume as a predictor along with interactions did not increase the test accuracy. Since LDA and logistic regression performed the best, this points to using only Lag2 predictor as the best case for these combinations of predictors.\n\n\n# Conclusion\n\nThis paper involved looking at weekly returns for the S&P 500 stock index between 1990 and 2010 and seeing if predictions of the market direction was possible with the data. First, the data itself was explored with numerical and graphical summaries of the data. Some of the distributions were almost identical, like the lag variables, while Volume had a more skewed distribution.\n\nA logistic regression model was fitted to the whole dataset. Lag2 was the only significant predictor at a 0.05 significance variable. With this is mind, Lag2 was selected as the only predictor to test a set of logistic regression, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), and K-nearest neighbors (KNN) with K = 1 models for predicting Direction. \n\nIn order to properly test and compare the models, the data was split into a training set that contain weekly returns prior to 2009 and then a testing set for 2009 data and onward. After each classification model was fit to the training data, the training model was used to predict results based on the test data and compared with the test data. Logistic regression and LDA had the best accuracy values.\n\nOther predictor combinations and transformations with Lag2, Volume, and log transformation of Volume were tried. Also, a value of K for KNN was determined on the training data using repeated cross-validation. The K producing the highest accuracy (on the repeated cross validated training data) was chosen. Then the KNN model with that K was used to predict values for the test data following the same procedure as the other models.\n\nEven with trying to add Volume, interactions, and transformations, the case of just using Lag2 as the sole predictor for Direction with a logistic regression or LDA model produced the best accuracy values of 62.5%.\n\nI hope you enjoyed going through the Kernel! Let me know what you think via a comment (and/or an upvote)!\n\n# References and Sources\n\n[1] James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning with applications in R, www.StatLearning.com, Springer-Verlag, New York\n\n[2] https://stackoverflow.com/questions/23891140/r-how-to-visualize-confusion-matrix-using-the-caret-package\n\n[3] http://dataaspirant.com/2017/01/09/knn-implementation-r-using-caret-package/"},{"metadata":{"_uuid":"7e3b69a588151069640f2c38980f995689e2b2e8"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}