{"cells":[{"metadata":{"_uuid":"0450ad16376835efb69d93a1405abf9bf89d7f3c","_cell_guid":"6e530d4f-93bb-4e10-921d-14165acb57fd"},"cell_type":"markdown","source":"Natural Language Processing (usually shortened to \"NLP\") is the task of automatically extracting and summarizing information from text data. There are many different tasks in NLP and this tutorial will focus on just one of them: topic modeling. (There are some links at the end of this lesson to other tutorials for doing common NLP tasks in R.)\n\n> **Topic modeling**: The NLP task of identifying automatically identifying major themes in a text, usually by identifying informative words.\n\nThere are two main uses for topic modeling. The first is to help in identifying major topics in unlabeled texts. You can think of it as a replacement for word clouds to help you understand the recurring themes in a text. The second use is to identify which words are important for text that is labeled for topic. In this tutorial we'll practice both kinds of topic modeling.\n______\n\n### What will I learn?\n\nBy the time you finish this tutorial, you will have:\n\n* Built an unsupervised topic model using Latent Dirichlet Allocation (LDA)\n* Explored the impact of text pre-processing, including removing stop words and stemming\n* Built a supervised topic model using term frequency–inverse document frequency (TF-IDF)\n\n### What do I need to know before I get started?\n\nYou may find some of the discussion here hard to understand if you don't have a basic familiar with the R language before you begin. I would recommend [this series of lessons](https://www.kaggle.com/rtatman/getting-started-in-r-first-steps/) if you've never used R or never programmed before.\n\n### What will I need to do?\n\nAs you work through this tutorial, you'll have a number of exercise to complete. You are of course free to fork this notebook and do them here, but the easiest option is to fork a version of [this workbook for this lesson](https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling-workbook/).\n\n____\n\n## Table of Contents\n\n* [Unsupervised topic modeling with LDA](#Unsupervised-topic-modeling-with-LDA)\n* [Pre-processing text for more informative models](#Pre-processing-text-for-more-informative-models)\n* [Supervised topic modeling with TF-IDF](#Supervised-topic-modeling-with-TF-IDF)\n_____"},{"metadata":{"_uuid":"a961bdd6d4835642e4ff0e562886145bc1781c5e","_cell_guid":"a3b8c040-ffac-47e5-abec-f4fdac822b89"},"cell_type":"markdown","source":"# Setting up our workspace\n_____\n\nBefore we can get started on topic modelling, we need to get our environment set up. First, let's load in the packages we're going to use. Here, I've added a comment letting you know why we need each package.  "},{"metadata":{"scrolled":true,"_kg_hide-output":true,"_uuid":"63f364a2113cb597b27289c0729339e6cff19b1b","trusted":true,"_cell_guid":"ff0f5314-1632-4ca2-8458-d89ac86b9c74"},"cell_type":"code","source":"# read in the libraries we're going to use\nlibrary(tidyverse) # general utility & workflow functions\nlibrary(tidytext) # tidy implimentation of NLP methods\nlibrary(topicmodels) # for LDA topic modelling \nlibrary(tm) # general text mining functions, making document term matrixes\nlibrary(SnowballC) # for stemming","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"af134d35d3c00202170202d9c324e9b285b17a6a","_cell_guid":"e973e4ec-d68e-479c-adaa-179913302c75"},"cell_type":"markdown","source":"Now let's read in our data. For this tutorial we're going to be using a corpus of hotel reviews put together by Myle Ott and co-authors. It contains positive and negative hotel reviews. Some are real reviews written by actual customers, while others are fake deceptive reviews. For more details, check out this paper on this dataset: \n\n> M. Ott, C. Cardie, and J.T. Hancock. 2013. Negative Deceptive Opinion Spam. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies."},{"metadata":{"_kg_hide-output":true,"_uuid":"0f9ee4cc24af10cc345f4dadb3c3165f1ceffa22","trusted":true,"_cell_guid":"d6fe1b3c-e52e-46e5-9b2a-5c04ed010a11"},"cell_type":"code","source":"# read in our data\nreviews <- read_csv(\"../input/deceptive-opinion.csv\")","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"7f1aab6e2ad770e35070c7374a79255191bc5bd4","_cell_guid":"fe5d9cd5-20cb-4064-9261-943f1ceea698"},"cell_type":"markdown","source":"Now that we've got our workspace set up, let's get started with some topic modelling! \n\n## Your turn:\nFor your convenience, I've gone ahead and set up your [workbook environment](https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling-workbook/) as well. You can take this oppertunity to make sure you've forked a copy of it and run the cells you need to read in your libraries and data."},{"metadata":{"_uuid":"b84e06d042cf0e74875184c0c2f023e9db435438","_cell_guid":"b2ca07a2-f26f-403c-907c-6152860c1377"},"cell_type":"markdown","source":"# Unsupervised topic modeling with LDA \n____\n\nLatent Dirichlet (said like deer-ih-CLAY) allocation, more commonly shortened to LDA, is one unsupervised way of doing topic modeling.\n\n> **Unsupervised:** In machine learning, a problem is unsupervised if you don't have labeled examples of the thing you're trying to figure out.\n\nLDA rests on the assumption that each document is going to have some subset of topics in it, and that each topic is associated with a certain subset of words. Imagine, for example, that you have three text ads:\n\n* One is for Chow-Chow Crunch, a new dog food\n* One is for Danny's Downhome Diner, a new restaurant\n* One is for Fifi's Chic Puppy Boutique, a store that sells dog accessories\n\nBoth the diner and the dog food ad will probably have words associated with the topic of food in them, like \"tasty\", \"filling\" or \"delicious\". Both the dog food and the pet store will probably have words associated with the topic of pets, like \"canine\", \"pet\" or \"owner\". And, because they're all ads, it likely that all three documents will have words associated with advertising in them, like \"sale\", \"price\" or \"special\".\n\nThe tricky part is that words can also belong to more than one topic. I just said that \"special\" could be associated with advertising... but it can also be associated with food, especially in terms like \"blue plate special\" or \"chef's special\". \n\nWhat LDA outputs, then, is two estimates:\n\n1. An estimate of how much each topic contributes to each document\n2. An estimate of how much each word contributes to each topic\n\nFor exploratory analysis, especially if you have a large number of documents, the second estimate is usually more interesting. In order to make our lives easier (since we're going to ran an LDA several times) let's write a function that takes a text column from a data frame and returns a  plot of the most informative words for a given number of topics."},{"metadata":{"scrolled":true,"_uuid":"c9534f67ffeb3f1bc9636f7a0f8c2c6ce3792285","trusted":true,"_cell_guid":"f0b2b6e9-68e2-47c5-986e-e27270180b8b"},"cell_type":"code","source":"# function to get & plot the most informative terms by a specificed number\n# of topics, using LDA\ntop_terms_by_topic_LDA <- function(input_text, # should be a columm from a dataframe\n                                   plot = T, # return a plot? TRUE by defult\n                                   number_of_topics = 4) # number of topics (4 by default)\n{    \n    # create a corpus (type of object expected by tm) and document term matrix\n    Corpus <- Corpus(VectorSource(input_text)) # make a corpus object\n    DTM <- DocumentTermMatrix(Corpus) # get the count of words/document\n\n    # remove any empty rows in our document term matrix (if there are any \n    # we'll get an error when we try to run our LDA)\n    unique_indexes <- unique(DTM$i) # get the index of each unique value\n    DTM <- DTM[unique_indexes,] # get a subset of only those indexes\n    \n    # preform LDA & get the words/topic in a tidy text format\n    lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))\n    topics <- tidy(lda, matrix = \"beta\")\n\n    # get the top ten terms for each topic\n    top_terms <- topics  %>% # take the topics data frame and..\n      group_by(topic) %>% # treat each topic as a different group\n      top_n(10, beta) %>% # get the top 10 most informative words\n      ungroup() %>% # ungroup\n      arrange(topic, -beta) # arrange words in descending informativeness\n\n    # if the user asks for a plot (TRUE by default)\n    if(plot == T){\n        # plot the top ten terms for each topic in order\n        top_terms %>% # take the top terms\n          mutate(term = reorder(term, beta)) %>% # sort terms by beta value \n          ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme\n          geom_col(show.legend = FALSE) + # as a bar plot\n          facet_wrap(~ topic, scales = \"free\") + # which each topic in a seperate plot\n          labs(x = NULL, y = \"Beta\") + # no x label, change y label \n          coord_flip() # turn bars sideways\n    }else{ \n        # if the user does not request a plot\n        # return a list of sorted terms instead\n        return(top_terms)\n    }\n}","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"dca325b249411a578c89578d513a6933b53a3fe4","_cell_guid":"1aa49706-c6be-41ca-a82e-d5a81f8d6e19"},"cell_type":"markdown","source":"Alright, now let's try out our function on our dataset! \n\n> **But how do we know how many topics we should have?** You can't be sure ahead of time unless you're very familiar with the dataset and know how many topics you expect to see. Probably the most common way to figure out how many topics there are automatically is to train a lot of different models with different numbers of topics and then see which has the least uncertainty (generally measured by perplexity). This can be a bit resource-intensive, especially if you have a large corpus, so I'm not going to cover it in detail here. You can check out [this guide](http://cfss.uchicago.edu/fall2016/text02.html#perplexity) for more details if you're curious. \n\nSince I know that this dataset contains deceptive and truthful reviews, I'm going to specify that I want to know about two topics."},{"metadata":{"_uuid":"6410a5e14b7915abb9874a268fb0dc7cf18a2005","trusted":true,"_cell_guid":"68543f5c-c7fc-4d60-b5d2-03f189def956"},"cell_type":"code","source":"# plot top ten terms in the hotel reviews by topic\ntop_terms_by_topic_LDA(reviews$text, number_of_topics = 2)","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"1357de49f5af1aebb756c201192828a85860126d","_cell_guid":"7f415dfc-44cf-49a1-aa92-19a14ab28646"},"cell_type":"markdown","source":"And that's it! We're all done! \n\nJust kidding: these topics aren't actually very interesting or informative. This is because they include a lot of very common words that play important grammatical roles but don't actually convey a lot of information. Our topic modelling will be a lot more useful if we remove these words.\n\n___\n\n## Your turn!\n\nBefore we move on, however, why don't you head over to your workbook and try your hand at LDA on [a dataset of fake news stories](https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling-workbook/)? I've even done all the data cleaning for you so you can just jump right in. :)\n\n___"},{"metadata":{"_uuid":"1456e54a7d072b688b9e4464cd0fcfd4f1c1555a","_cell_guid":"3de6b821-5189-46d6-8c92-06dfe634b0ec"},"cell_type":"markdown","source":"# Pre-processing text for more informative models\n______\n\nIn the last section, we tried our hand at topic modelling uisng LDA and found that our models were not very informative becuase they had a lot of very common stopwords in them.\n\n> **Stopwords:** Words that are very commonly used in a language but are not very informative. Examples of English stopwords include \"the\", \"of\" and \"and\". These are often removed during NLP tasks using a hand-curated list. We have stopword lists for many languages on Kaggle, including [this one](https://www.kaggle.com/rtatman/stopword-lists-for-19-languages) which has stopword lists for 19 languages. \n\nLet's see if our topics are any more interesting if we try pre-processing our text data to remove stopwords. (I'm also going to remove both \"hotel\" and \"room\", where are very common in hotel reviews.)"},{"metadata":{"scrolled":true,"_uuid":"741f40cc92ad842b3b40c509e1ac372ceb8026d0","trusted":true,"_cell_guid":"5104fb1f-1787-4861-9ea9-4155b7e8dadf"},"cell_type":"code","source":"# create a document term matrix to clean\nreviewsCorpus <- Corpus(VectorSource(reviews$text)) \nreviewsDTM <- DocumentTermMatrix(reviewsCorpus)\n\n# convert the document term matrix to a tidytext corpus\nreviewsDTM_tidy <- tidy(reviewsDTM)\n\n# I'm going to add my own custom stop words that I don't think will be\n# very informative in hotel reviews\ncustom_stop_words <- tibble(word = c(\"hotel\", \"room\"))\n\n# remove stopwords\nreviewsDTM_tidy_cleaned <- reviewsDTM_tidy %>% # take our tidy dtm and...\n    anti_join(stop_words, by = c(\"term\" = \"word\")) %>% # remove English stopwords and...\n    anti_join(custom_stop_words, by = c(\"term\" = \"word\")) # remove my custom stopwords\n\n# reconstruct cleaned documents (so that each word shows up the correct number of times)\ncleaned_documents <- reviewsDTM_tidy_cleaned %>%\n    group_by(document) %>% \n    mutate(terms = toString(rep(term, count))) %>%\n    select(document, terms) %>%\n    unique()\n\n# check out what the cleaned documents look like (should just be a bunch of content words)\n# in alphabetic order\nhead(cleaned_documents)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"236343781e357ca9f18269db977bc0e42518b800","_cell_guid":"9b606d94-1341-4d55-9aa2-24635af71d4f"},"cell_type":"markdown","source":"OK, that's our preprocessing is complete, let's check out our new (hopefully more informative) topic models."},{"metadata":{"_uuid":"1da61a9b271f33fe051f54359f4fdf0c2468c15c","trusted":true,"_cell_guid":"0ba978e5-2a26-41d8-90ce-28a3c71c8944"},"cell_type":"code","source":"# now let's look at the new most informative terms\ntop_terms_by_topic_LDA(cleaned_documents$terms, number_of_topics = 2)","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"c3a36e77b655bb3fd1cb05519816acd717ec652d","_cell_guid":"3f03b159-6d28-4fbc-a758-ff100ea29f3b"},"cell_type":"markdown","source":"As you can see, these are much more helpful! We can now tell the difference between these topics. While they appear pretty similar (which isn't surprising given that there's a limited number of topics we'd expect to see discussed in hotel reviews) it looks like the first topic is more about people's stay and the second topic is more about the city of Chicago. This isn't too surprising, since all of the hotels reviewed were in Chicago and it's a major tourist destination.\n\nYou may notice something about these topics, however. In the just the first ten words we see a little bit of repetition. Topic one contains both \"stay\" and \"stayed\". This is a little bit less informative than it could be, because \"stay\" and \"stayed\" are different forms of the same underlying word. We can collapse all the different forms of the same word with a process called \"stemming\". \n\n> **Stemming**: Removing all the inflection from words. For instance, the root form of \"horses\", \"horse\", and \"horsing [around]\" is the same word: \"horse\". For some NLP tasks, we want to count all of these words as the same word.\n\nLet's try our LDA topic modeling again, this time after stemming our data."},{"metadata":{"_uuid":"62e54186c460faf2b532b25328d3170a42a6fa26","trusted":true,"_cell_guid":"cb438cfa-9814-4248-8cae-2b7382856bf0"},"cell_type":"code","source":"# stem the words (e.g. convert each word to its stem, where applicable)\nreviewsDTM_tidy_cleaned <- reviewsDTM_tidy_cleaned %>% \n    mutate(stem = wordStem(term))\n\n# reconstruct our documents\ncleaned_documents <- reviewsDTM_tidy_cleaned %>%\n    group_by(document) %>% \n    mutate(terms = toString(rep(stem, count))) %>%\n    select(document, terms) %>%\n    unique()\n\n# now let's look at the new most informative terms\ntop_terms_by_topic_LDA(cleaned_documents$terms, number_of_topics = 2)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"6ed43abc4f83dbc7be44ac43b720796af6e56aff","_cell_guid":"c4a07b8d-6790-48c2-acd8-e14935f8b289"},"cell_type":"markdown","source":"> **What's up with the weird looking words look weird, like \"locat\" and \"stai\"?** These are the stems of the words, which in this case are generated using the [Porter Stemming Algorithm](https://tartarus.org/martin/PorterStemmer/). If we look at our \"horse\" example above, the stem of  \"horses\", \"horse\", and \"horsing [around]\" is actually \"hors\", not \"horse\". That's because \"hors\" is the longest string that occurs in all three words.\n\nIn this instance, it doesn't actually look like stemming was actually that helpful in terms of generating informative topics. We also don't know which (if either) of these topics are associated with deceptive reviews and which are associated with truthful ones. \n\nFortunately, we actually do have the labels for whether each review is truthful or deceptive, which means that we can use a supervised method of topic modeling instead. In general, unsupervised methods (like LDA) are helpful for data exploration but supervised methods (like TF-IDF, which we're going to learn next) are usually a better choice if you have access to labeled data. We've started with ab unsupervised method here in order to give you the chance to directly compare LDA and TF-IDF on the same dataset.\n\n___\n\n## Your turn!\n\nNow it's your turn to try out training an LDA model. [Head on over to your workbook and give it a shot](https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling-workbook/).\n___"},{"metadata":{"_uuid":"7971a4cbf5265c83d0d30980c8e248b8d9a7fd22","_cell_guid":"a298e5f9-3578-441d-ad0c-4a6e2f846986"},"cell_type":"markdown","source":"# Supervised topic modeling with TF-IDF\n___\n\nNow that we've given unsupervised topic modelling a go, let's try supervised topic modeling. For this, we're going to use something call TF-IDF, which stands for \"term frequency-inverse document frequency\".\n\nThe general idea behind how TF-IDF works is this:\n\n* Words that are very common in a specific document are probably important to the topic of that document\n* Words that are very common in all documents probably aren't important to the topics of any of them\n\nSo a term will recieve a high weight if it's common in a specific document and also uncommon across all documents. \n\nIn order to streamline our analysis, I'm going to write a function that takes in a dataframe, the name of the column that has the texts in it and the name of the column that has the topic labels in it. "},{"metadata":{"scrolled":false,"_uuid":"8f0aecc81a4b34d76cb61744046927d6a3bb655e","trusted":true,"_cell_guid":"9ce90747-c5c9-4f88-af51-e6d29e7b940f"},"cell_type":"code","source":"# function that takes in a dataframe and the name of the columns\n# with the document texts and the topic labels. If plot is set to\n# false it will return the tf-idf output rather than a plot.\ntop_terms_by_topic_tfidf <- function(text_df, text_column, group_column, plot = T){\n    # name for the column we're going to unnest_tokens_ to\n    # (you only need to worry about enquo stuff if you're\n    # writing a function using using tidyverse packages)\n    group_column <- enquo(group_column)\n    text_column <- enquo(text_column)\n    \n    # get the count of each word in each review\n    words <- text_df %>%\n      unnest_tokens(word, !!text_column) %>%\n      count(!!group_column, word) %>% \n      ungroup()\n\n    # get the number of words per text\n    total_words <- words %>% \n      group_by(!!group_column) %>% \n      summarize(total = sum(n))\n\n    # combine the two dataframes we just made\n    words <- left_join(words, total_words)\n\n    # get the tf_idf & order the words by degree of relevence\n    tf_idf <- words %>%\n      bind_tf_idf(word, !!group_column, n) %>%\n      select(-total) %>%\n      arrange(desc(tf_idf)) %>%\n      mutate(word = factor(word, levels = rev(unique(word))))\n    \n    if(plot == T){\n        # convert \"group\" into a quote of a name\n        # (this is due to funkiness with calling ggplot2\n        # in functions)\n        group_name <- quo_name(group_column)\n        \n        # plot the 10 most informative terms per topic\n        tf_idf %>% \n          group_by(!!group_column) %>% \n          top_n(10) %>% \n          ungroup %>%\n          ggplot(aes(word, tf_idf, fill = as.factor(group_name))) +\n          geom_col(show.legend = FALSE) +\n          labs(x = NULL, y = \"tf-idf\") +\n          facet_wrap(reformulate(group_name), scales = \"free\") +\n          coord_flip()\n    }else{\n        # return the entire tf_idf dataframe\n        return(tf_idf)\n    }\n}","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"4615ccbdccab10c6ec92c096e84179935c67c9f1","_cell_guid":"0bdf0580-ffb3-43b9-b90f-899f139ea8ae"},"cell_type":"markdown","source":"Now that I've done that, I can quickly and easily check out the most informative words for truthful and deceptive reviews."},{"metadata":{"_uuid":"d379fe01ac7d404817ee7cd1f864cf098164e4ae","trusted":true,"_cell_guid":"bf5b1783-50d9-4706-b425-a18f3f0844e7"},"cell_type":"code","source":"# let's see what our most informative deceptive words are\ntop_terms_by_topic_tfidf(text_df = reviews, # dataframe\n                         text_column = text, # column with text\n                         group_column = deceptive, # column with topic label\n                         plot = T) # return a plot","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"85d95776906525bfc640d752c4f6355f1e6dd8e4","_cell_guid":"49720063-ef6a-4e30-b94c-bed76cace269"},"cell_type":"markdown","source":"Interesting! So it looks like false reviews tend to use a lot of glowing praise in their reviews (\"pampered\", \"exquisite\"), while truthful reviews tend to talk about how they booked thier room (\"priceline\", \"hotwire\"). \n\nSince we have a function written, we can easily check out the most informative words for other topics. This dataset is also annotated for whether the reveiw was positive or negative, so let's see which words are associated with which polarity."},{"metadata":{"_uuid":"dea029ce975c483c9b78c18e1033eb83f157e1ba","trusted":true,"_cell_guid":"e37f0ad9-7061-4ee2-9c32-6a67028ff9ef"},"cell_type":"code","source":"# look for the most informative words for postive and negative reveiws\ntop_terms_by_topic_tfidf(text_df = reviews, \n                         text_column = text, \n                         group = polarity, \n                         plot = T)","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"b2b6b9b78202e538a55f7f4c42e428b5da18a00d","_cell_guid":"2e504a70-25fa-44df-9070-c83c0cad79bf"},"cell_type":"markdown","source":"From this, we can see that negative reviews include words like \"worst\", \"broken\", \"odor\" and \"stains\", while positive reviews really harp on the bathrobes (both \"robes\" and \"bathrobes\" show up in the top ten words). \n\nJust for fun/as a sanity check, let's check out the reviews for each hotel individually. I'm going to use the function I wrote above to return just the TF-IDF output and then do my own plotting, since I don't think the default plot will look good with so many different categories (there are 20 different hotels in this dataset)."},{"metadata":{"scrolled":true,"_uuid":"270ac28597bcf053b78af23ec0aa621b4b26e346","trusted":true,"_cell_guid":"d51810c3-2b8a-4798-a168-652d6cb76da3"},"cell_type":"code","source":"# get just the tf-idf output for the hotel topics\nreviews_tfidf_byHotel <- top_terms_by_topic_tfidf(text_df = reviews, \n                         text_column = text, \n                         group = hotel, \n                         plot = F)\n\n# do our own plotting\nreviews_tfidf_byHotel  %>% \n          group_by(hotel) %>% \n          top_n(5) %>% \n          ungroup %>%\n          ggplot(aes(word, tf_idf, fill = hotel)) +\n          geom_col(show.legend = FALSE) +\n          labs(x = NULL, y = \"tf-idf\") +\n          facet_wrap(~hotel, ncol = 4, scales = \"free\", ) +\n          coord_flip()","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"83bdbd292a0bdd8168b8ed5b04317149699520aa","_cell_guid":"61df7d20-4161-4109-a135-1fc4d4663a4c"},"cell_type":"markdown","source":"Unsurprisingly, the most informative word for reviews about a hotel is the name of the hotel itself. We can also tell a little bit about the features of different hotels: the Monaco has goldfish, Homewood has a kitchen with a dishwasher and Hardrock has musical memorabilia.\n\n___\n\n## Your turn!\n\nNow it's your turn to try out training a TF-IDF model. [Head on over to your workbook and give it a shot](https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling-workbook/).\n___"},{"metadata":{"_uuid":"4043e030b8c4a2226a0f8e6130dace2b9f2e7560","_cell_guid":"964cde79-c969-4895-977f-76d74fb0afea"},"cell_type":"markdown","source":"# And that's all there is to it!\n____\n\nIn this tutorial you have:\n\n* Built an unsupervised topic model using Latent Dirichlet Allocation (LDA)\n* Explored the impact of text pre-processing, including removing stop words and stemming\n* Built a supervised topic model using term frequency–inverse document frequency (TF-IDF)\n\nTopic modelling is just one task in Natural Language Processing, however. Luckily, we have several other beginner tutorials for text processing in R. Some topics include: \n\n* [Tokenization](https://www.kaggle.com/rtatman/tokenization-tutorial) \n* [Getting n-grams](https://www.kaggle.com/rtatman/tutorial-getting-n-grams)\n* [Sentiment analysis](https://www.kaggle.com/rtatman/tutorial-sentiment-analysis-in-r/)\n\nNice work making it through the tutorial, and have fun analyzing text!"}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}