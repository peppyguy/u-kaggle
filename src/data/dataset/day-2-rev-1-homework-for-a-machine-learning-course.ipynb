{"cells":[{"metadata":{"_uuid":"fc86405c539f44ba44070cebf7910bbe833be4f5"},"cell_type":"markdown","source":"# Kickstarter Projects\n# Day 2 (Rev-1) Homework for a Machine Learning Course\nAuthorï¼šHiroki Miyamoto"},{"metadata":{"_uuid":"ea436ad11ed63a5aae7200d3f5f81af3c09d63be"},"cell_type":"markdown","source":"# Links to my homeworks\n- Kaggle\n    - Day1\n        - Objective of Day 1 : Build a supervised machine learning model based on the lecture on Day 1. Don't care about the accuracy for now.\n        - https://www.kaggle.com/hmiyamoto/day-1-homework-for-a-machine-learning-course/\n    - Day2\n        - Objective of Day 2 : Improve the accuracy of your supervised machine learning model applying the algorithms introduced on Day 2.\n        - https://www.kaggle.com/hmiyamoto/day-2-homework-for-a-machine-learning-course/\n    - Day2 Appendix-1\n        - Check the contribution of explanatory variables for prediction by investigating the weight\n        - https://www.kaggle.com/hmiyamoto/day-2-homework-appendix-1\n    - Day2 Appendix-2\n        - Check the contribution of explanatory variables for prediction by investigating the weight again\n            - \"backers\" and \"usd_pledged_real\" are removed from explanatory variables for success prediction because these variables are results of funding.\n        - https://www.kaggle.com/hmiyamoto/day-2-homework-appendix-2\n- GitHub\n    - https://github.com/hmiyamoto1/skillupai_ml"},{"metadata":{"_uuid":"7ed194155b755585d2ddad10f605fa63dacc5a9b"},"cell_type":"markdown","source":"## Objective of Day 2 : Improve the accuracy of your supervised machine learning model applying the algorithms introduced on Day 2.\n\n### \"backers\" and \"usd_pledged_real\" are removed from explanatory variables for success prediction because these variables are results of funding, as examined in the notebook Day 2 Appendix-2. \n\n### 2-variables 'category_dummy' and 'usd_goal_real_log10' are applied as explanatory variables because these variables are highly contributed to predict, as examined in the notebook Day 2 Appendix-2. \n\n### Table of Contents (Day 2)\n1. Devide dataframe into train data(train & validation) and test data(final check)\n1. Parameter study for Logistic Regression (L1)\n1. Parameter study for Logistic Regression (L2)\n1. Parameter study for SVM (Linear)\n1. Parameter study for SVM (RBF)"},{"metadata":{"_uuid":"3abcc76fa79fea0da08cf97baca6780d564677f2"},"cell_type":"markdown","source":"# Summary\n\nSVM was not so as good accuracy as logistic regression in this case.  As can be seen 2D graphs in this notebook, SVM couldn't create a distinct border surface because labels of the objective variable data are overlapped on a map created by selected explanatory variables.\n\n| Model | Final Test Accuracy   |\n|------|------|\n|   Logistic Regression (L1)  | 68.929% |\n|   Logistic Regression (L2)  | 68.932% |\n|   SVM (Linear)  | 61.953% |\n|   SVM (RBF)  | 60.256% |\n\n- Objective variable\n    - state_dummy (successful = 1, other = 0)\n- Explanatory variables\n    - usd_goal_real_log10\n        - Second highly contributed variable to predict\n    - category_dummy\n        - is created by aggregating success rate of each category in training phase\n        - First highly contributed variable to predict\n        \n"},{"metadata":{"_uuid":"a86944c349c98dd758fecf6d1a83572a1b2c1091"},"cell_type":"markdown","source":"# 0. Preparation"},{"metadata":{"_uuid":"7933319a21ec65adb373c9e0f3a6c8b8ea18e8fd","trusted":false},"cell_type":"code","source":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import log_loss, accuracy_score, confusion_matrix, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96b5685618e183f681834ff47c561b6e6c7ec3eb","trusted":false},"cell_type":"code","source":"from matplotlib.colors import ListedColormap\n\n\ndef plot_decision_regions(X, y, classifier, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = ( 'x', '.', 'o', 's', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n                       \n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot class samples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n                    alpha=0.8, c=colors[idx],\n                    edgecolor=None,\n                    marker=markers[idx], \n                    label=cl)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"287c22765d902c22534c9afe506ca94ce9fe1106"},"cell_type":"markdown","source":"# 1. Devide dataframe into train data(trian & validation) and test data(final check)\n- DataFrame\n    - df_TRAIN : 80%\n        - This will be further devided into train and validation data in holdout or cross-validation training phase.\n            - train : 80%\n            - valid : 20%\n    - df_TEST : 20%\n        - This is blind data for final test."},{"metadata":{"_uuid":"20c9c5cf4b2f09a8873df4c500a9fc6eb8a9a8a3"},"cell_type":"markdown","source":"## Acquire data"},{"metadata":{"_uuid":"057075077bc73bc90e4b7b7e76d6f2b9cdeb4374","scrolled":false,"trusted":false},"cell_type":"code","source":"df_kick = pd.read_csv(\"../input/ks-projects-201801.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a87e7f5db23262b0d2cef8b72b81036c2f897f18"},"cell_type":"markdown","source":"## Preview the data"},{"metadata":{"_uuid":"03301b354a362d3e2be2306dce4a62ef67dce871","trusted":false},"cell_type":"code","source":"display(df_kick.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"723d1327ceabadcdd34e40bf2525b1a0a4d610a6"},"cell_type":"markdown","source":"## Create dataframe with selected features"},{"metadata":{"_uuid":"6a010c7eb1b6b9f4ff5036b5a3758e471023aee6","trusted":false},"cell_type":"code","source":"df_kick['state_dummy'] = df_kick['state']\ndf_kick['state_dummy'].loc[df_kick['state_dummy'] != 'successful'] = 0\ndf_kick['state_dummy'].loc[df_kick['state_dummy'] == 'successful'] = 1\n\n# display(df_kick.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6d9e63a19271bb404aef8bd5e0681d0e88e92e7","trusted":false},"cell_type":"code","source":"# epsilon = 1e-8\nepsilon = 1\n\ndf_kick['usd_goal_real_log10'] = np.log10(df_kick['usd_goal_real'] + epsilon)\n\ndisplay(df_kick.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"055973f2b820f0b8281dcf20a9f4bb94d40c7b27","trusted":false},"cell_type":"code","source":"df_ALL = df_kick.loc[:, ['state_dummy', 'usd_goal_real_log10', 'category']]\ndf_ALL.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f36a1a42dbefe7cf48919675f256d56463093e0","trusted":false},"cell_type":"code","source":"df_TRAIN, df_TEST = train_test_split(df_ALL, test_size=0.2, random_state=1234)\ndf_TRAIN.head()\ndisplay(df_TRAIN.describe())\ndisplay(df_TEST.describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cac319a96039a4f88438da713ac321055749e8c5"},"cell_type":"markdown","source":"# 2. Parameter study for Logistic Regression (L1)\n- Model\n    - Variables\n        - Objective variable\n            - state_dummy (successful = 1, other = 0)\n        - Explanatory variables\n            - usd_goal_real_log10\n                - Second highly contributed variable to predict\n            - category_dummy\n                - will be created by aggregating success rate of each category in training phase\n                - First highly contributed variable to predict\n    - Cross validation (Train:80%/Valid:20% - 5 times) is applied.\n    - **L1 regularization is applied here.**\n- Cross Validation Result\n    - Best parameter\n        - **alpha = 1e-3  (1e-8 <= alpha <= 1e-1)**\n        - Actually, alpha = 1e-2 was best prediction score. However, alpha = 1e-3 would be more stable.\n    - Best score\n        - **Cross Validation Log-likelihood = -10.886**\n        - **Cross Validation Accuracy = 68.483%**\n- Final Test Result (Applied best parameter)\n    - Test score\n        - **Test Log-likelihood = -10.732**\n        - **Test Accuracy = 68.929%**"},{"metadata":{"_uuid":"1b64921d8ca361ad8e674a507eb4fb7781bfa831"},"cell_type":"markdown","source":"## Cross-Validation"},{"metadata":{"_uuid":"16bab7d1cdfb9db476c066deac50b8eccaee2c00","trusted":false},"cell_type":"code","source":"penalty = 'l1'\n\nalphas_multiply = np.array(range(-8,0))\nalphas = 10.0 ** alphas_multiply\n\nL1_accuracy = []\nL1_log_likelihood = []\nL1_weight_abs_max = []\nL1_weight_abs_min = []\n\n\nfor alpha in alphas:\n    \n    print('='*100)\n    print('penalty =', penalty)\n    print('alpha =', alpha)\n    print()\n\n    y = df_TRAIN[\"state_dummy\"].values\n    X = df_TRAIN[[\"usd_goal_real_log10\", \"category\"]].values\n\n    n_split = 5 # Number of group\n\n    cross_valid_log_likelihood = 0\n    cross_valid_accuracy = 0\n    split_num = 1\n\n    # Cross Validation\n    for train_idx, valid_idx in KFold(n_splits=n_split, random_state=1234).split(X, y):\n        X_train, y_train = X[train_idx], y[train_idx] # Train data\n        X_valid, y_valid = X[valid_idx], y[valid_idx] # Validation data\n\n        df_X_train = pd.DataFrame(X_train,\n                                 columns=[\"usd_goal_real_log10\", \"category\"])\n        df_y_train = pd.DataFrame(y_train,\n                                 columns=[\"state_dummy\"])\n\n        df_X_valid = pd.DataFrame(X_valid,\n                                 columns=[\"usd_goal_real_log10\", \"category\"])\n        df_y_valid = pd.DataFrame(y_valid,\n                                 columns=[\"state_dummy\"])\n\n\n\n\n        # Create dummy variables for category using train data\n        # Replace category to category_success_rate\n        category_success_rate = {}\n        df_category_successful_count = df_X_train['category'][df_y_train['state_dummy'] == 1].value_counts()\n        df_category_all_count = df_X_train['category'].value_counts()\n        for category in df_category_all_count.keys():\n            category_success_rate[category] = df_category_successful_count[category] / df_category_all_count[category]\n        df_X_train['category_dummy'] = df_X_train['category'].replace(category_success_rate)\n        df_X_valid['category_dummy'] = df_X_valid['category'].replace(category_success_rate)\n\n\n        print(\"Fold %s\"%split_num)\n\n        X_train = df_X_train[[\"usd_goal_real_log10\", \"category_dummy\"]].values\n        X_valid = df_X_valid[[\"usd_goal_real_log10\", \"category_dummy\"]].values\n        \n        # Normaliztion\n        stdsc = StandardScaler()\n        X_train = stdsc.fit_transform(X_train)\n        X_valid = stdsc.transform(X_valid)\n\n        clf = SGDClassifier(loss='log', penalty=penalty, alpha=alpha, max_iter=100, fit_intercept=True, random_state=1234)\n        clf.fit(X_train, y_train)\n\n        # Weight\n        w0 = clf.intercept_[0]\n        w1 = clf.coef_[0, 0]\n        w2 = clf.coef_[0, 1]\n        print('w0 = {:.3f}, w1 = {:.3f}, w2 = {:.3f}'.format(w0, w1, w2))\n\n\n        # Predict labels\n        y_est_valid = clf.predict(X_valid)\n\n        # Log-likelihood\n        log_likelihood = - log_loss(y_valid, y_est_valid)    \n        cross_valid_log_likelihood += log_likelihood    \n        print('Log-likelihood = {:.3f}'.format(log_likelihood))\n\n        # Accuracy\n        accuracy = accuracy_score(y_valid, y_est_valid)\n        cross_valid_accuracy += accuracy   \n        print('Accuracy = {:.3f}%'.format(100 * accuracy))\n        print()\n        \n        if split_num == n_split:\n            plot_decision_regions(X_valid, y_valid, classifier=clf)\n            plt.title('(Fold %s)  L1 alpha = %s' %(split_num,alpha))\n            plt.xlabel('usd_goal_real_log10_stdsc')\n            plt.ylabel('category_dummy_stdsc')\n            plt.axes().set_aspect('equal', 'datalim')\n            plt.legend(loc='upper right')\n            plt.tight_layout()\n            plt.show()\n\n        split_num += 1\n\n    # Generalization performance\n    final_log_likelihood = cross_valid_log_likelihood / n_split\n    print(\"Cross Validation Log-likelihood = %s\"%round(final_log_likelihood, 3))\n    final_accuracy = cross_valid_accuracy / n_split\n    print('Cross Validation Accuracy = {:.3f}%'.format(100 * final_accuracy))\n    \n    L1_accuracy.append(final_accuracy)\n    L1_log_likelihood.append(final_log_likelihood)\n    L1_weight_abs_max.append(np.max(np.abs(clf.coef_)))\n    L1_weight_abs_min.append(np.min(np.abs(clf.coef_)))\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98e5f7653cfa1ad6fca7af348c36738f2388157a","trusted":false},"cell_type":"code","source":"plt.plot(alphas_multiply, L1_accuracy, marker='o')\nplt.title(\"L1 alpha\")\nplt.xlabel(\"Log10(alpha)\")\nplt.ylabel(\"Accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f82d075521f410bc79e4a64265df754e92435e39","trusted":false},"cell_type":"code","source":"plt.plot(alphas_multiply, L1_log_likelihood, marker='o')\nplt.title(\"L1 alpha\")\nplt.xlabel(\"Log10(alpha)\")\nplt.ylabel(\"Log-likelihood\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b328fda005bac3c6f030fdffa6fca1ef824caf8d","scrolled":true,"trusted":false},"cell_type":"code","source":"plt.plot(alphas_multiply, L1_weight_abs_max, marker='o', label='Weight_abs_max')\nplt.plot(alphas_multiply, L1_weight_abs_min, marker='o', label='Weight_abs_min')\nplt.title(\"L1 alpha\")\nplt.xlabel(\"Log10(alpha)\")\nplt.ylabel(\"Weight_abs_max_min\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6087e74aab743b4df0d9143ad607a4dc06f4a7d"},"cell_type":"markdown","source":"## Final Test"},{"metadata":{"_uuid":"23cdc813c3df93da9aa90b89b2bc7984ed03e479","scrolled":true,"trusted":false},"cell_type":"code","source":"penalty = 'l1'\n\n# Best Parameter\nalpha = 1e-3\n\n\nprint('penalty =', penalty)\nprint('alpha =', alpha)\nprint()\n\n# TRAIN data\ndf_y_train = df_TRAIN[[\"state_dummy\"]]\ndf_X_train = df_TRAIN[[\"usd_goal_real_log10\", \"category\"]]\n\n# TEST data\ndf_y_test = df_TEST[[\"state_dummy\"]]\ndf_X_test = df_TEST[[\"usd_goal_real_log10\", \"category\"]]\n\n\n# df_X_train = pd.DataFrame(X_train,\n#                          columns=[\"usd_goal_real_log10\", \"category\"])\n# df_y_train = pd.DataFrame(y_train,\n#                          columns=[\"state_dummy\"])\n\n# df_X_test = pd.DataFrame(X_test,\n#                          columns=[\"usd_goal_real_log10\", \"category\"])\n# df_y_test = pd.DataFrame(y_test,\n#                          columns=[\"state_dummy\"])\n\n\n# Create dummy variables for category using train data\n# Replace category to category_success_rate\ncategory_success_rate = {}\ndf_category_successful_count = df_X_train['category'][df_y_train['state_dummy'] == 1].value_counts()\ndf_category_all_count = df_X_train['category'].value_counts()\nfor category in df_category_all_count.keys():\n    category_success_rate[category] = df_category_successful_count[category] / df_category_all_count[category]\ndf_X_train['category_dummy'] = df_X_train['category'].replace(category_success_rate)\ndf_X_test['category_dummy'] = df_X_test['category'].replace(category_success_rate)\n\n\n\nX_train = df_X_train[[\"usd_goal_real_log10\", \"category_dummy\"]].values\nX_test = df_X_test[[\"usd_goal_real_log10\", \"category_dummy\"]].values\n\ny_train = df_y_train[[\"state_dummy\"]].values\ny_test = df_y_test[[\"state_dummy\"]].values\n\n# Normaliztion\nstdsc = StandardScaler()\nX_train = stdsc.fit_transform(X_train)\nX_test = stdsc.transform(X_test)\n\nclf = SGDClassifier(loss='log', penalty=penalty, alpha=alpha, max_iter=100, fit_intercept=True, random_state=1234)\nclf.fit(X_train, y_train)\n\n# Weight\nw0 = clf.intercept_[0]\nw1 = clf.coef_[0, 0]\nw2 = clf.coef_[0, 1]\nprint('w0 = {:.3f}, w1 = {:.3f}, w2 = {:.3f}'.format(w0, w1, w2))\n\n\n# Predict labels\ny_est_test = clf.predict(X_test)\n\n# Log-likelihood\nlog_likelihood = - log_loss(y_test, y_est_test)       \nprint('Test Log-likelihood = {:.3f}'.format(log_likelihood))\n\n# Accuracy\naccuracy = accuracy_score(y_test, y_est_test)  \nprint('Test Accuracy = {:.3f}%'.format(100 * accuracy))\nprint()\n\nplot_decision_regions(X_test, y_test.flatten(), classifier=clf)\nplt.title('(Final Test)  L1 alpha = %s' %alpha)\nplt.xlabel('usd_goal_real_log10_stdsc')\nplt.ylabel('category_dummy_stdsc')\nplt.axes().set_aspect('equal', 'datalim')\nplt.legend(loc='upper right')\nplt.tight_layout()\nplt.show()\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5211e258019100601182f471d6ceae495b3f72ee"},"cell_type":"markdown","source":"# 3. Parameter study for Logistic Regression (L2)\n- Model\n    - Variables\n        - Objective variable\n            - state_dummy (successful = 1, other = 0)\n        - Explanatory variables\n            - usd_goal_real_log10\n                - Second highly contributed variable to predict\n            - category_dummy\n                - will be created by aggregating success rate of each category in training phase\n                - First highly contributed variable to predict\n    - Cross validation (Train:80%/Valid:20% - 5 times) is applied.\n    - **L2 regularization is applied here.**\n- Cross Validation Result\n    - Best parameter\n        - **alpha = 1e-3  (1e-8 <= alpha <= 1e-1)**\n    - Best score\n        - **Cross Validation Log-likelihood = -10.885**\n        - **Cross Validation Accuracy = 68.484%**\n- Final Test Result (Applied best parameter)\n    - Test score\n        - **Test Log-likelihood = -10.731**\n        - **Test Accuracy = 68.932%**"},{"metadata":{"_uuid":"1d2acf3a0ca9c7cc72ab1d62ed3447df06375c52"},"cell_type":"markdown","source":"## Cross-Validation"},{"metadata":{"_uuid":"41f0f5529625cd67b40978904c6104b02b93d92f","trusted":false},"cell_type":"code","source":"penalty = 'l2'\n\nalphas_multiply = np.array(range(-8,0))\nalphas = 10.0 ** alphas_multiply\n\nL2_accuracy = []\nL2_log_likelihood = []\nL2_weight_abs_max = []\nL2_weight_abs_min = []\n\n\nfor alpha in alphas:\n    \n    print('='*100)\n    print('penalty =', penalty)\n    print('alpha =', alpha)\n    print()\n\n    y = df_TRAIN[\"state_dummy\"].values\n    X = df_TRAIN[[\"usd_goal_real_log10\", \"category\"]].values\n\n    n_split = 5 # Number of group\n\n    cross_valid_log_likelihood = 0\n    cross_valid_accuracy = 0\n    split_num = 1\n\n    # Cross Validation\n    for train_idx, valid_idx in KFold(n_splits=n_split, random_state=1234).split(X, y):\n        X_train, y_train = X[train_idx], y[train_idx] # Train data\n        X_valid, y_valid = X[valid_idx], y[valid_idx] # Validation data\n\n        df_X_train = pd.DataFrame(X_train,\n                                 columns=[\"usd_goal_real_log10\", \"category\"])\n        df_y_train = pd.DataFrame(y_train,\n                                 columns=[\"state_dummy\"])\n\n        df_X_valid = pd.DataFrame(X_valid,\n                                 columns=[\"usd_goal_real_log10\", \"category\"])\n        df_y_valid = pd.DataFrame(y_valid,\n                                 columns=[\"state_dummy\"])\n\n\n\n\n        # Create dummy variables for category using train data\n        # Replace category to category_success_rate\n        category_success_rate = {}\n        df_category_successful_count = df_X_train['category'][df_y_train['state_dummy'] == 1].value_counts()\n        df_category_all_count = df_X_train['category'].value_counts()\n        for category in df_category_all_count.keys():\n            category_success_rate[category] = df_category_successful_count[category] / df_category_all_count[category]\n        df_X_train['category_dummy'] = df_X_train['category'].replace(category_success_rate)\n        df_X_valid['category_dummy'] = df_X_valid['category'].replace(category_success_rate)\n\n\n        print(\"Fold %s\"%split_num)\n\n        X_train = df_X_train[[\"usd_goal_real_log10\", \"category_dummy\"]].values\n        X_valid = df_X_valid[[\"usd_goal_real_log10\", \"category_dummy\"]].values\n        \n        # Normaliztion\n        stdsc = StandardScaler()\n        X_train = stdsc.fit_transform(X_train)\n        X_valid = stdsc.transform(X_valid)\n\n        clf = SGDClassifier(loss='log', penalty=penalty, alpha=alpha, max_iter=100, fit_intercept=True, random_state=1234)\n        clf.fit(X_train, y_train)\n\n        # Weight\n        w0 = clf.intercept_[0]\n        w1 = clf.coef_[0, 0]\n        w2 = clf.coef_[0, 1]\n        print('w0 = {:.3f}, w1 = {:.3f}, w2 = {:.3f}'.format(w0, w1, w2))\n\n\n        # Predict labels\n        y_est_valid = clf.predict(X_valid)\n\n        # Log-likelihood\n        log_likelihood = - log_loss(y_valid, y_est_valid)    \n        cross_valid_log_likelihood += log_likelihood    \n        print('Log-likelihood = {:.3f}'.format(log_likelihood))\n\n        # Accuracy\n        accuracy = accuracy_score(y_valid, y_est_valid)\n        cross_valid_accuracy += accuracy   \n        print('Accuracy = {:.3f}%'.format(100 * accuracy))\n        print()\n        \n        if split_num == n_split:\n            plot_decision_regions(X_valid, y_valid, classifier=clf)\n            plt.title('(Fold %s)  L2 alpha = %s' %(split_num,alpha))\n            plt.xlabel('usd_goal_real_log10_stdsc')\n            plt.ylabel('category_dummy_stdsc')\n            plt.axes().set_aspect('equal', 'datalim')\n            plt.legend(loc='upper right')\n            plt.tight_layout()\n            plt.show()\n\n        split_num += 1\n\n    # Generalization performance\n    final_log_likelihood = cross_valid_log_likelihood / n_split\n    print(\"Cross Validation Log-likelihood = %s\"%round(final_log_likelihood, 3))\n    final_accuracy = cross_valid_accuracy / n_split\n    print('Cross Validation Accuracy = {:.3f}%'.format(100 * final_accuracy))\n    \n    L2_accuracy.append(final_accuracy)\n    L2_log_likelihood.append(final_log_likelihood)\n    L2_weight_abs_max.append(np.max(np.abs(clf.coef_)))\n    L2_weight_abs_min.append(np.min(np.abs(clf.coef_)))\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dd3e0ac9edda217f583f728eee7e8aaace8d590","trusted":false},"cell_type":"code","source":"plt.plot(alphas_multiply, L2_accuracy, marker='o')\nplt.title(\"L2 alpha\")\nplt.xlabel(\"Log10(alpha)\")\nplt.ylabel(\"Accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4a07abf06b11507d580563cfea4a6ae75ebeceb","trusted":false},"cell_type":"code","source":"plt.plot(alphas_multiply, L2_log_likelihood, marker='o')\nplt.title(\"L2 alpha\")\nplt.xlabel(\"Log10(alpha)\")\nplt.ylabel(\"Log-likelihood\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab9a9acb0bed61b07e7c2ea2374b29421231e81b","trusted":false},"cell_type":"code","source":"plt.plot(alphas_multiply, L2_weight_abs_max, marker='o', label='Weight_abs_max')\nplt.plot(alphas_multiply, L2_weight_abs_min, marker='o', label='Weight_abs_min')\nplt.title(\"L2 alpha\")\nplt.xlabel(\"Log10(alpha)\")\nplt.ylabel(\"Weight_abs_max_min\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ee4384c55b9c330d61d4c12f4d26b72f609f723"},"cell_type":"markdown","source":"## Final Test"},{"metadata":{"_uuid":"a4886b18a7243d632086ed4b022f1ba232742e9f","trusted":false},"cell_type":"code","source":"penalty = 'l2'\n\n# Best Parameter\nalpha = 1e-3\n\n\nprint('penalty =', penalty)\nprint('alpha =', alpha)\nprint()\n\n# TRAIN data\ndf_y_train = df_TRAIN[[\"state_dummy\"]]\ndf_X_train = df_TRAIN[[\"usd_goal_real_log10\", \"category\"]]\n\n# TEST data\ndf_y_test = df_TEST[[\"state_dummy\"]]\ndf_X_test = df_TEST[[\"usd_goal_real_log10\", \"category\"]]\n\n\n# df_X_train = pd.DataFrame(X_train,\n#                          columns=[\"usd_goal_real_log10\", \"category\"])\n# df_y_train = pd.DataFrame(y_train,\n#                          columns=[\"state_dummy\"])\n\n# df_X_test = pd.DataFrame(X_test,\n#                          columns=[\"usd_goal_real_log10\", \"category\"])\n# df_y_test = pd.DataFrame(y_test,\n#                          columns=[\"state_dummy\"])\n\n\n# Create dummy variables for category using train data\n# Replace category to category_success_rate\ncategory_success_rate = {}\ndf_category_successful_count = df_X_train['category'][df_y_train['state_dummy'] == 1].value_counts()\ndf_category_all_count = df_X_train['category'].value_counts()\nfor category in df_category_all_count.keys():\n    category_success_rate[category] = df_category_successful_count[category] / df_category_all_count[category]\ndf_X_train['category_dummy'] = df_X_train['category'].replace(category_success_rate)\ndf_X_test['category_dummy'] = df_X_test['category'].replace(category_success_rate)\n\n\n\nX_train = df_X_train[[\"usd_goal_real_log10\", \"category_dummy\"]].values\nX_test = df_X_test[[\"usd_goal_real_log10\", \"category_dummy\"]].values\n\ny_train = df_y_train[[\"state_dummy\"]].values\ny_test = df_y_test[[\"state_dummy\"]].values\n\n# Normaliztion\nstdsc = StandardScaler()\nX_train = stdsc.fit_transform(X_train)\nX_test = stdsc.transform(X_test)\n\nclf = SGDClassifier(loss='log', penalty=penalty, alpha=alpha, max_iter=100, fit_intercept=True, random_state=1234)\nclf.fit(X_train, y_train)\n\n# Weight\nw0 = clf.intercept_[0]\nw1 = clf.coef_[0, 0]\nw2 = clf.coef_[0, 1]\nprint('w0 = {:.3f}, w1 = {:.3f}, w2 = {:.3f}'.format(w0, w1, w2))\n\n\n# Predict labels\ny_est_test = clf.predict(X_test)\n\n# Log-likelihood\nlog_likelihood = - log_loss(y_test, y_est_test)       \nprint('Test Log-likelihood = {:.3f}'.format(log_likelihood))\n\n# Accuracy\naccuracy = accuracy_score(y_test, y_est_test)  \nprint('Test Accuracy = {:.3f}%'.format(100 * accuracy))\nprint()\n\nplot_decision_regions(X_test, y_test.flatten(), classifier=clf)\nplt.title('(Final Test)  L2 alpha = %s' %alpha)\nplt.xlabel('usd_goal_real_log10_stdsc')\nplt.ylabel('category_dummy_stdsc')\nplt.axes().set_aspect('equal', 'datalim')\nplt.legend(loc='upper right')\nplt.tight_layout()\nplt.show()\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0edfb91fff0dfef36276c4de79d63e337dd6599"},"cell_type":"markdown","source":"# 4. Parameter study for SVM (Linear)\n- Model\n    - Variables\n        - Objective variable\n            - state_dummy (successful = 1, other = 0)\n        - Explanatory variables\n            - usd_goal_real_log10\n                - Second highly contributed variable to predict\n            - category_dummy\n                - will be created by aggregating success rate of each category in training phase\n                - First highly contributed variable to predict\n    - **SVM (Linear) is applied here.**\n    - **Holdout method is applied for SVM parameter study instead of cross validation to shorten the calculation time.**\n- Cross Validation Result\n    - Best parameter\n        - **C = 1e-4  (1e-8 <= C <= 1e3)**\n    - Best score\n        - **Holdout Log-likelihood = -13.752**\n        - **Holdout Accuracy = 60.184%**\n- Final Test Result (Applied best parameter)\n    - Test score\n        - **Test Log-likelihood = -13.141**\n        - **Test Accuracy = 61.953%**"},{"metadata":{"_uuid":"373de4a98596d20e2c98a9c03c59e38eb213e577"},"cell_type":"markdown","source":"## Holdout Method"},{"metadata":{"_uuid":"56dfa76cfe2513d22f0097ca3e51ac3a861c1825","trusted":false},"cell_type":"code","source":"kernel = 'linear'\n# Cs = [0.001, 0.01, 0.1, 1, 10]\nCs = np.logspace(-8, 3, 12, base=10)\n\nSVM_accuracy = []\nSVM_log_likelihood = []\nSVM_weight_abs_max = []\nSVM_weight_abs_min = []\n\n\nfor C in Cs:\n    \n    print('='*100)\n    print('kernel =', kernel)\n    print('C =', C)\n    print()\n    \n    y = df_TRAIN[\"state_dummy\"].values\n    X = df_TRAIN[[\"usd_goal_real_log10\", \"category\"]].values\n\n    # Holdout method\n    test_size = 0.2        # 20%\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=1234) # Holdout\n\n\n    df_X_train = pd.DataFrame(X_train,\n                             columns=[\"usd_goal_real_log10\", \"category\"])\n    df_y_train = pd.DataFrame(y_train,\n                             columns=[\"state_dummy\"])\n\n    df_X_valid = pd.DataFrame(X_valid,\n                             columns=[\"usd_goal_real_log10\", \"category\"])\n    df_y_valid = pd.DataFrame(y_valid,\n                             columns=[\"state_dummy\"])\n\n\n    # Create dummy variables for category using train data\n    # Replace category to category_success_rate\n    category_success_rate = {}\n    df_category_successful_count = df_X_train['category'][df_y_train['state_dummy'] == 1].value_counts()\n    df_category_all_count = df_X_train['category'].value_counts()\n    for category in df_category_all_count.keys():\n        category_success_rate[category] = df_category_successful_count[category] / df_category_all_count[category]\n    df_X_train['category_dummy'] = df_X_train['category'].replace(category_success_rate)\n    df_X_valid['category_dummy'] = df_X_valid['category'].replace(category_success_rate)\n\n    X_train = df_X_train[[\"usd_goal_real_log10\", \"category_dummy\"]].values\n    X_valid = df_X_valid[[\"usd_goal_real_log10\", \"category_dummy\"]].values\n    \n    # Normaliztion\n    stdsc = StandardScaler()\n    X_train = stdsc.fit_transform(X_train)\n    X_valid = stdsc.transform(X_valid)\n\n    clf = SVC(C=C,kernel=kernel, max_iter=2000, random_state=1234)\n    clf.fit(X_train, y_train)\n\n    # Predict labels\n    y_est_valid = clf.predict(X_valid)\n\n    # Log-likelihood\n    holdout_log_likelihood = - log_loss(y_valid, y_est_valid)         \n\n    # Accuracy\n    holdout_accuracy = accuracy_score(y_valid, y_est_valid)\n    \n    plot_decision_regions(X_valid, y_valid, classifier=clf)\n    plt.title('(Holdout)  SVM Linear C = %s' %C)\n    plt.xlabel('usd_goal_real_log10_stdsc')\n    plt.ylabel('category_dummy_stdsc')\n    plt.axes().set_aspect('equal', 'datalim')\n    plt.legend(loc='upper right')\n    plt.tight_layout()\n    plt.show()\n\n    # Generalization performance\n    final_log_likelihood = holdout_log_likelihood\n    print(\"Holdout Log-likelihood = %s\"%round(final_log_likelihood, 3))\n    final_accuracy = holdout_accuracy\n    print('Holdout Accuracy = {:.3f}%'.format(100 * final_accuracy))\n    \n    SVM_accuracy.append(final_accuracy)\n    SVM_log_likelihood.append(final_log_likelihood)\n    SVM_weight_abs_max.append(np.max(np.abs(clf.coef_)))\n    SVM_weight_abs_min.append(np.min(np.abs(clf.coef_)))\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01f39277b9814507998d0ac5e7b80939df6ce6cc","trusted":false},"cell_type":"code","source":"plt.plot(Cs, SVM_accuracy, marker='o')\nplt.title(\"SVM kernel = linear\")\nplt.xlabel(\"C\")\nplt.xscale('log')\nplt.ylabel(\"Accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8dd237188376319d74d72012441b125c2d623824","trusted":false},"cell_type":"code","source":"plt.plot(Cs, SVM_log_likelihood, marker='o')\nplt.title(\"SVM kernel = linear\")\nplt.xlabel(\"C\")\nplt.xscale('log')\nplt.ylabel(\"Log-likelihood\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23598315459b4444da4699277e12941c65c07c4d","trusted":false},"cell_type":"code","source":"plt.plot(Cs, SVM_weight_abs_max, marker='o', label='Weight_abs_max')\nplt.plot(Cs, SVM_weight_abs_min, marker='o', label='Weight_abs_min')\nplt.title(\"SVM kernel = linear\")\nplt.xlabel(\"C\")\nplt.xscale('log')\nplt.ylabel(\"Weight_abs_max_min\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a9cf1ebc2df3b310f5b7223c722388dccae686d"},"cell_type":"markdown","source":"## Final Test"},{"metadata":{"_uuid":"73c2cc57cb1f174964efe02331f28bd5957100f5","trusted":false},"cell_type":"code","source":"kernel = 'linear'\n\n# Best Parameter\nC = 1e-4\n\n\nprint('kernel =', kernel)\nprint('C =', C)\nprint()\n\n# TRAIN data\ndf_y_train = df_TRAIN[[\"state_dummy\"]]\ndf_X_train = df_TRAIN[[\"usd_goal_real_log10\", \"category\"]]\n\n# TEST data\ndf_y_test = df_TEST[[\"state_dummy\"]]\ndf_X_test = df_TEST[[\"usd_goal_real_log10\", \"category\"]]\n\n# Create dummy variables for category using train data\n# Replace category to category_success_rate\ncategory_success_rate = {}\ndf_category_successful_count = df_X_train['category'][df_y_train['state_dummy'] == 1].value_counts()\ndf_category_all_count = df_X_train['category'].value_counts()\nfor category in df_category_all_count.keys():\n    category_success_rate[category] = df_category_successful_count[category] / df_category_all_count[category]\ndf_X_train['category_dummy'] = df_X_train['category'].replace(category_success_rate)\ndf_X_test['category_dummy'] = df_X_test['category'].replace(category_success_rate)\n\n\n\nX_train = df_X_train[[\"usd_goal_real_log10\", \"category_dummy\"]].values\nX_test = df_X_test[[\"usd_goal_real_log10\", \"category_dummy\"]].values\n\ny_train = df_y_train[[\"state_dummy\"]].values\ny_test = df_y_test[[\"state_dummy\"]].values\n\n# print(X_test.shape)\n# print(y_test.flatten().shape)\n\n# print(X_valid.shape)\n# print(y_valid.shape)\n\n# Normaliztion\nstdsc = StandardScaler()\nX_train = stdsc.fit_transform(X_train)\nX_test = stdsc.transform(X_test)\n\nclf = SVC(C=C,kernel=kernel, max_iter=2000, random_state=1234)\nclf.fit(X_train, y_train)\n\n# Predict labels\ny_est_test = clf.predict(X_test)\n\n# Log-likelihood\nlog_likelihood = - log_loss(y_test, y_est_test)       \nprint('Test Log-likelihood = {:.3f}'.format(log_likelihood))\n\n# Accuracy\naccuracy = accuracy_score(y_test, y_est_test)  \nprint('Test Accuracy = {:.3f}%'.format(100 * accuracy))\nprint()\n\nplot_decision_regions(X_test, y_test.flatten(), classifier=clf)\nplt.title('(Final Test)  SVM Linear C = %s' %C)\nplt.xlabel('usd_goal_real_log10_stdsc')\nplt.ylabel('category_dummy_stdsc')\nplt.axes().set_aspect('equal', 'datalim')\nplt.legend(loc='upper right')\nplt.tight_layout()\nplt.show()\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5c431da1901f6f11cd7fd009ab99e3489fe1347"},"cell_type":"markdown","source":"# 5. Parameter study for SVM (RBF)\n- Model\n    - Variables\n        - Objective variable\n            - state_dummy (successful = 1, other = 0)\n        - Explanatory variables\n            - usd_goal_real_log10\n                - Second highly contributed variable to predict\n            - category_dummy\n                - will be created by aggregating success rate of each category in training phase\n                - First highly contributed variable to predict\n    - **SVM (RBF) is applied here.**\n    - **Holdout method is applied for SVM parameter study instead of cross validation to shorten the calculation time.**\n- Cross Validation Result\n    - Best parameter\n        - **C = 1e2  (1e-4 <= C <= 1e2)**\n        - **gamma = 1e-8  (1e-10 <= gamma <= 1e-4)**\n    - Best score\n        - **Holdout Log-likelihood = -10.894**\n        - **Holdout Accuracy = 68.460%**\n- Final Test Result (Applied best parameter)\n    - Test score\n        - **Test Log-likelihood = -13.727**\n        - **Test Accuracy = 60.256%**"},{"metadata":{"_uuid":"385888376aec783ef031953075c75b6061f57d68"},"cell_type":"markdown","source":"## Holdout Method"},{"metadata":{"_uuid":"8241ba4ae43a1a79c28f80f5f62ce4d572d8d90d","trusted":false},"cell_type":"code","source":"kernel = 'rbf'\nCs = np.logspace(-4, 2, 7, base=10)\ngammas = np.logspace(-10, -4, 7, base=10)\n\nSVM_accuracy = []\nSVM_log_likelihood = []\nSVM_weight_abs_max = []\nSVM_weight_abs_min = []\n\n\nfor C in Cs:\n    for gamma in gammas:\n\n        print('='*100)\n        print('kernel =', kernel)\n        print('C =', C)\n        print('gamma =', gamma)\n        print()\n\n        y = df_TRAIN[\"state_dummy\"].values\n        X = df_TRAIN[[\"usd_goal_real_log10\", \"category\"]].values\n\n        # Holdout method\n        test_size = 0.2        # 20%\n        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=1234) # Holdout\n\n\n        df_X_train = pd.DataFrame(X_train,\n                                 columns=[\"usd_goal_real_log10\", \"category\"])\n        df_y_train = pd.DataFrame(y_train,\n                                 columns=[\"state_dummy\"])\n\n        df_X_valid = pd.DataFrame(X_valid,\n                                 columns=[\"usd_goal_real_log10\", \"category\"])\n        df_y_valid = pd.DataFrame(y_valid,\n                                 columns=[\"state_dummy\"])\n\n\n        # Create dummy variables for category using train data\n        # Replace category to category_success_rate\n        category_success_rate = {}\n        df_category_successful_count = df_X_train['category'][df_y_train['state_dummy'] == 1].value_counts()\n        df_category_all_count = df_X_train['category'].value_counts()\n        for category in df_category_all_count.keys():\n            category_success_rate[category] = df_category_successful_count[category] / df_category_all_count[category]\n        df_X_train['category_dummy'] = df_X_train['category'].replace(category_success_rate)\n        df_X_valid['category_dummy'] = df_X_valid['category'].replace(category_success_rate)\n\n        X_train = df_X_train[[\"usd_goal_real_log10\", \"category_dummy\"]].values\n        X_valid = df_X_valid[[\"usd_goal_real_log10\", \"category_dummy\"]].values\n\n        # Normaliztion\n        stdsc = StandardScaler()\n        X_train = stdsc.fit_transform(X_train)\n        X_valid = stdsc.transform(X_valid)\n\n        clf = SVC(C=C,kernel=kernel, gamma=gamma, max_iter=2000, random_state=1234)\n        clf.fit(X_train, y_train)\n\n        # Predict labels\n        y_est_valid = clf.predict(X_valid)\n\n        # Log-likelihood\n        holdout_log_likelihood = - log_loss(y_valid, y_est_valid)         \n\n        # Accuracy\n        holdout_accuracy = accuracy_score(y_valid, y_est_valid)\n\n        plot_decision_regions(X_valid, y_valid, classifier=clf)\n        plt.title('(Holdout)  SVM RBF C = %s, gamma = %s' %(C, gamma))\n        plt.xlabel('usd_goal_real_log10_stdsc')\n        plt.ylabel('category_dummy_stdsc')\n        plt.axes().set_aspect('equal', 'datalim')\n        plt.legend(loc='upper right')\n        plt.tight_layout()\n        plt.show()\n\n        # Generalization performance\n        final_log_likelihood = holdout_log_likelihood\n        print(\"Holdout Log-likelihood = %s\"%round(final_log_likelihood, 3))\n        final_accuracy = holdout_accuracy\n        print('Holdout Accuracy = {:.3f}%'.format(100 * final_accuracy))\n\n        SVM_accuracy.append(final_accuracy)\n        SVM_log_likelihood.append(final_log_likelihood)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2b92283fcf07249c00f6457aadb315ad9fb6335","trusted":false},"cell_type":"code","source":"SVM_accuracy_2d = np.array(SVM_accuracy).reshape(7, 7)\nCs_str = list(map(str, Cs))\ngammas_str = list(map(str, gammas))\ndf_SVM_accuracy_2d = pd.DataFrame(data=SVM_accuracy_2d, index=Cs_str, columns=gammas_str)\ndf_SVM_accuracy_2d","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5530329c7b7572f65f6d7ee04b6fee55d1fc666","trusted":false},"cell_type":"code","source":"sns.heatmap(df_SVM_accuracy_2d, cmap='rainbow')\nplt.ylabel(\"C\")\nplt.xlabel(\"gamma\")\nplt.title('Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dad3c386f025278cfebabfd427f894cd74e650b1"},"cell_type":"markdown","source":"## Final Test"},{"metadata":{"trusted":false,"_uuid":"5d549d219bed60fdac94adca6b0f4273242ae126"},"cell_type":"code","source":"kernel = 'rbf'\n\n# Best Parameter\nC = 100\ngamma = 1e-8\n\n\nprint('kernel =', kernel)\nprint('C =', C)\nprint('gamma =', gamma)\nprint()\n\n# TRAIN data\ndf_y_train = df_TRAIN[[\"state_dummy\"]]\ndf_X_train = df_TRAIN[[\"usd_goal_real_log10\", \"category\"]]\n\n# TEST data\ndf_y_test = df_TEST[[\"state_dummy\"]]\ndf_X_test = df_TEST[[\"usd_goal_real_log10\", \"category\"]]\n\n# Create dummy variables for category using train data\n# Replace category to category_success_rate\ncategory_success_rate = {}\ndf_category_successful_count = df_X_train['category'][df_y_train['state_dummy'] == 1].value_counts()\ndf_category_all_count = df_X_train['category'].value_counts()\nfor category in df_category_all_count.keys():\n    category_success_rate[category] = df_category_successful_count[category] / df_category_all_count[category]\ndf_X_train['category_dummy'] = df_X_train['category'].replace(category_success_rate)\ndf_X_test['category_dummy'] = df_X_test['category'].replace(category_success_rate)\n\n\n\nX_train = df_X_train[[\"usd_goal_real_log10\", \"category_dummy\"]].values\nX_test = df_X_test[[\"usd_goal_real_log10\", \"category_dummy\"]].values\n\ny_train = df_y_train[[\"state_dummy\"]].values\ny_test = df_y_test[[\"state_dummy\"]].values\n\n# print(X_test.shape)\n# print(y_test.flatten().shape)\n\n# print(X_valid.shape)\n# print(y_valid.shape)\n\n# Normaliztion\nstdsc = StandardScaler()\nX_train = stdsc.fit_transform(X_train)\nX_test = stdsc.transform(X_test)\n\nclf = SVC(C=C,kernel=kernel, gamma=gamma, max_iter=2000, random_state=1234)\nclf.fit(X_train, y_train)\n\n# Predict labels\ny_est_test = clf.predict(X_test)\n\n# Log-likelihood\nlog_likelihood = - log_loss(y_test, y_est_test)       \nprint('Test Log-likelihood = {:.3f}'.format(log_likelihood))\n\n# Accuracy\naccuracy = accuracy_score(y_test, y_est_test)  \nprint('Test Accuracy = {:.3f}%'.format(100 * accuracy))\nprint()\n\nplot_decision_regions(X_test, y_test.flatten(), classifier=clf)\nplt.title('(Final Test)  SVM RBF C = %s, gamma = %s' %(C, gamma))\nplt.xlabel('usd_goal_real_log10_stdsc')\nplt.ylabel('category_dummy_stdsc')\nplt.axes().set_aspect('equal', 'datalim')\nplt.legend(loc='upper right')\nplt.tight_layout()\nplt.show()\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"24e2b9b7853d128a3c74a77f8426b1a6473dd8b8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}