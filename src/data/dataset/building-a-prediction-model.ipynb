{"cells":[
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "# Making predictions over amazon recommendation dataset\n\n## Predictions\nThe purpose of this analysis is to make up a prediction model where we will be able to predict whether a recommendation is positive or negative. In this analysis, we will not focus on the Score, but only the positive/negative sentiment of the recommendation. \n\nTo do so, we will work on Amazon's recommendation dataset, we will build a Term-doc incidence matrix using term frequency and inverse document frequency ponderation. When the data is ready, we will load it into predicitve algorithms, mainly naïve Bayesian and regression.\n\nIn the end, we hope to find a \"best\" model for predicting the recommendation's sentiment.\n\n## Loading the data\nIn order to load the data, we will use the SQLITE dataset where we will only fetch the Score and the recommendation summary. \n\nAs we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score id above 3, then the recommendation wil be set to \"postive\". Otherwise, it will be set to \"negative\". \n\nThe data will be split into an training set and a test set with a test set ratio of 0.2"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "%matplotlib inline\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\ncon = sqlite3.connect('../input/database.sqlite')\n\nmessages = pd.read_sql_query(\"\"\"\nSELECT Score, Summary\nFROM Reviews\nWHERE Score != 3\n\"\"\", con)\n\ndef partition(x):\n    if x < 3:\n        return 'negative'\n    return 'positive'\n\nScore = messages['Score']\nScore = Score.map(partition)\nSummary = messages['Summary']\nX_train, X_test, y_train, y_test = train_test_split(Summary, Score, test_size=0.2, random_state=42)"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## Brief Exploratory analysis\n\nAfter loading the data, here is what it looks like :"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "print(messages.head(20))"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "After splitting our dataset into X_train, X_test, Y_train, Y_test, we have no more integer score, but an appreciation of it : positive or negative :"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "tmp = messages\ntmp['Score'] = tmp['Score'].map(partition)\nprint(tmp.head(20))"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## Cleaning the data\n\nTo format our data and build the Term-doc incidence matrix, many operations will be performed on the data :\n- Stemming\n- Stop words removal\n- Lowering\n- Tokenization\n- Pruning (numbers and punctuation)"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "stemmer = PorterStemmer()\nfrom nltk.corpus import stopwords\n\ndef stem_tokens(tokens, stemmer):\n    stemmed = []\n    for item in tokens:\n        stemmed.append(stemmer.stem(item))\n    return stemmed\n\ndef tokenize(text):\n    tokens = nltk.word_tokenize(text)\n    #tokens = [word for word in tokens if word not in stopwords.words('english')]\n    stems = stem_tokens(tokens, stemmer)\n    return ' '.join(stems)\n\nintab = string.punctuation\nouttab = \"                                \"\ntrantab = str.maketrans(intab, outtab)\n\n#--- Training set\n\ncorpus = []\nfor text in X_train:\n    text = text.lower()\n    text = text.translate(trantab)\n    text=tokenize(text)\n    corpus.append(text)\n        \ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(corpus)        \n        \ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\n#--- Test set\n\ntest_set = []\nfor text in X_test:\n    text = text.lower()\n    text = text.translate(trantab)\n    text=tokenize(text)\n    test_set.append(text)\n\nX_new_counts = count_vect.transform(test_set)\nX_test_tfidf = tfidf_transformer.transform(X_new_counts)\n\nfrom pandas import *\ndf = DataFrame({'Before': X_train, 'After': corpus})\nprint(df.head(20))\n\nprediction = dict()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## Applying Multinomial Naïve Bayes learning method"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "from sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB().fit(X_train_tfidf, y_train)\nprediction['Multinomial'] = model.predict(X_test_tfidf)"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## Applying Bernoulli Naïve Bayes learning method"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "from sklearn.naive_bayes import BernoulliNB\nmodel = BernoulliNB().fit(X_train_tfidf, y_train)\nprediction['Bernoulli'] = model.predict(X_test_tfidf)"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## Applying Logistic regression learning method"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(C=1e5)\nlogreg.fit(X_train_tfidf, y_train)\nprediction['Logistic'] = logreg.predict(X_test_tfidf)"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## Results\n\nIn order to compare our learning algorithms, let's build the ROC curve. The curve with the highest AUC value will show our \"best\" algorithm.\n\nIn first data cleaning, stop-words removal has been used, but the results were much worse. Reason for this result could be that when people want to speak about what is or is not good, they use many small words like \"not\" for instance, and these words will typically be tagged as stop-words, and will be removed. This is why in the end, it was decided to keep the stop-words. For those who would like to try it by themselves, I have let the stop-words removal as a comment in the cleaning part of the analysis."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "def formatt(x):\n    if x == 'negative':\n        return 0\n    return 1\nvfunc = np.vectorize(formatt)\n\ncmp = 0\ncolors = ['b', 'g', 'y', 'm', 'k']\nfor model, predicted in prediction.items():\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test.map(formatt), vfunc(predicted))\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.plot(false_positive_rate, true_positive_rate, colors[cmp], label='%s: AUC %0.2f'% (model,roc_auc))\n    cmp += 1\n\nplt.title('Classifiers comparaison with ROC')\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "After plotting the ROC curve, it would appear that the Logistic regression method provides us with the best results, although the AUC value for this method is not outstanding... \n\nLet's focus on logistic regression, and vizualise the accuracy, recall and confusion matrix of this model:"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "print(metrics.classification_report(y_test, prediction['Logistic'], target_names = [\"positive\", \"negative\"]))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(set(Score)))\n    plt.xticks(tick_marks, set(Score), rotation=45)\n    plt.yticks(tick_marks, set(Score))\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n# Compute confusion matrix\ncm = confusion_matrix(y_test, prediction['Logistic'])\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm)    \n\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nplt.figure()\nplot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n\nplt.show()"
 }
],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}, "nbformat": 4, "nbformat_minor": 0}