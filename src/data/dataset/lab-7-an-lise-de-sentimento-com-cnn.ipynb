{"cells":[{"metadata":{"_uuid":"35460bad94c4646a9a2a27566be99ee492bd8cc3"},"cell_type":"markdown","source":"# Laboratório 7 - Análise de Sentimento utilizando Redes Neuronais Convolucionais"},{"metadata":{"_uuid":"0d9c1564ff58fd1e3b0a4616dd136aee8130ab3e"},"cell_type":"markdown","source":"Nesse laboratório vamos realizar análise de sentimento utilizando redes neuronais convolucionais, como proposto nesse <a href=\"http://arxiv.org/pdf/1408.5882v2.pdf\">artigo</a>. A arquitetura da solução pode ser visualizada na figura abaixo:\n\n<img src=\"http://debajyotidatta.github.io/assets/images/Zhang.png\" align=\"center\"/>"},{"metadata":{"trusted":true,"_uuid":"b9fe6782676850ac92dc11439a9969e5c59f9c38"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nnp.random.seed(5)\nimport os\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Concatenate\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.convolutional import Convolution1D, MaxPooling1D\nfrom keras.optimizers import Adam\n\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss, classification_report","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93da44700e90a71021d7fb678f6906dea1a546ab"},"cell_type":"markdown","source":"## Parte 1 - Carregando e manipulando os dados"},{"metadata":{"_uuid":"07198133127a2e0ba2f54fe095d5b3ec6fbbc5ee"},"cell_type":"markdown","source":"Os dados que utilizaremos referem-se a dados de avaliação do atendimento do Service Desk da Petrobras. Queremos classificar os comentários feitos pelos usuários entre elogio, neutro ou reclamação."},{"metadata":{"trusted":true,"_uuid":"cc040743479ae981d9096626fd7d3336dfc81ced"},"cell_type":"code","source":"dfAvaliacoesAnalisadas = pd.read_csv('../input/all.csv')\ndfAvaliacoesAnalisadas.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f41fbf3caa181f3f0925c4b2714eecb363477719"},"cell_type":"markdown","source":"Convertemos a manifestação (elogio, neutro, reclamação) em códigos:"},{"metadata":{"trusted":true,"_uuid":"cf9e10a51b07e12e4cdb2c9f39dc7f1de17b7dec"},"cell_type":"code","source":"def converteCategoria(df, coluna):\n    le = preprocessing.LabelEncoder()\n    le.fit(df[coluna])\n    df[coluna] = le.transform(df[coluna])\n    return le\n\nnum_classes = len(dfAvaliacoesAnalisadas.manifest_atendimento.unique())\n\nlabelEncoderManifAtendimento = converteCategoria(dfAvaliacoesAnalisadas, 'manifest_atendimento')\nprint(num_classes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2158e33e694bd5ed5f6144ec731879f88decdd22"},"cell_type":"markdown","source":"Nesse ponto podemos começar a trabalhar com nossos comentários. O primeiro passo é remover caracteres indesejados e separar pontuação das palavras, pois a pontuação será tratada no nosso modelo. Também converteremos alguns números, emails e incidentes em uma palavra única."},{"metadata":{"trusted":true,"_uuid":"55529bb9cf72502ff828e6be1e06e9b1ad8afebe"},"cell_type":"code","source":"import re\ndef clean_str(string):\n    \"\"\"\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    \"\"\"\n    string = re.sub(r\"[^a-zA-Z0-9ÀÁÂÃÄÅÇÈÉÊËÌÍÎÏÒÓÔÕÖÙÚÛÜÝàáâãäåçèéêëìíîïðòóôõöùúûüýÿ,!?\\'\\`\\.\\(\\)]\", \" \", string)\n    string = re.sub(r\"INC[0-9]{7,}\", \" <INCIDENTE> \", string)\n    string = re.sub(r\"[+-]?\\d+(?:\\.\\d+)?\", \" <NUMERO> \", string)\n\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"\\.\", \" \\. \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" ( \", string)\n    string = re.sub(r\"\\)\", \" ) \", string)\n    string = re.sub(r\"\\?\", \" ? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip() #.lower()\n\ndfAvaliacoesAnalisadas['coment'] = dfAvaliacoesAnalisadas['coment'].apply(clean_str)\ndfAvaliacoesAnalisadas['coment'] = dfAvaliacoesAnalisadas['coment'].apply(lambda x : x.split(' '))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d282cf68f74359fb07be8fa40f1bba36a96752ba"},"cell_type":"markdown","source":"Na nossa arquitetura, todas as sentenças devem ter o mesmo número de palavras. Temos de escolher um bom tamanho. Para isso, vamos visualizar um histograma dos tamanhos das sentenças:"},{"metadata":{"trusted":true,"_uuid":"e5bee980bbfc0f04117297054da647478bf62dd0"},"cell_type":"code","source":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\n\nsequence_length = dfAvaliacoesAnalisadas['coment'].apply(len).values\nprint(np.percentile(sequence_length, 99.9))\nprint(np.max(sequence_length))\nplt.hist(sequence_length)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"226a0ba2f9a6915f8f5f8745209a5f400bf72ea5"},"cell_type":"markdown","source":"Como mostrado acima, 99,9% das sentenças (avaliações) possuem menos de 37 palavras e 100% são menores que 40 palavras . Então vamos colocar um ponto de corte  em 40. As avaliações com mais de 40 palavras serão truncadas e as com menos, terão a palavra <PAD/\\> completando o final até chegar a 40 palavras."},{"metadata":{"trusted":true,"_uuid":"ec220cfc0bac2c02595b81cc5ee3a06e55a0092a"},"cell_type":"code","source":"def pad_sentence(sentence, sequence_length, padding_word=\"<PAD/>\"):\n    if len(sentence) > sequence_length:\n        sentence = sentence[:sequence_length]\n    num_padding = sequence_length - len(sentence)\n    new_sentence = sentence + [padding_word] * num_padding\n    return new_sentence\n\ncorte = np.max(sequence_length)\ndfAvaliacoesAnalisadas['coment'] = dfAvaliacoesAnalisadas['coment'].apply(lambda x : pad_sentence(x, corte))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"212209e8f6ee120fd939514c421ff7ec7aae342e"},"cell_type":"markdown","source":"Agora estamos prontos para gerar nossas  *Word embeedings* para nosso vocabulário. Várias pesquisas tem demonstrado que usar um modelo baseado no corpus específica da tarefa (no caso, avaliações de atendimento), funciona melhor do que usar corpus genéricos e grandes como a wikipédia."},{"metadata":{"_uuid":"8fa09b2859bf05a753618dffc1f7bb59b5bc091b"},"cell_type":"markdown","source":"## Parte 2 - Gerando as *Word Embeddings*"},{"metadata":{"trusted":true,"_uuid":"a32c8124811a763203d92c65366449e2b83ca504"},"cell_type":"code","source":"import itertools\nfrom collections import Counter\n\ndef build_vocab(sentences):\n    \"\"\"\n    Builds a vocabulary mapping from word to index based on the sentences.\n    Returns vocabulary mapping and inverse vocabulary mapping.\n    \"\"\"\n    # Build vocabulary\n    word_counts = Counter(itertools.chain(*sentences))\n    # Mapping from index to word\n    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n    # Mapping from word to index\n    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n    return (vocabulary, vocabulary_inv)\n\ndef build_input_data(sentences, labels, vocabulary):\n    \"\"\"\n    Maps sentencs and labels to vectors based on a vocabulary.\n    \"\"\"\n    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n    y = np.array(labels)\n    return (x, y)\n\ncomments = dfAvaliacoesAnalisadas['coment'].values\nlabels = dfAvaliacoesAnalisadas['manifest_atendimento'].values\n\nvocabulary, vocabulary_inv = build_vocab(comments)\nX, ylabels = build_input_data(comments, labels, vocabulary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efb219fe71f5e0165279cc2747d92745592452b6"},"cell_type":"code","source":"from gensim.models import word2vec\nfrom os.path import join, exists, split\n\ndef train_word2vec(sentence_matrix, vocabulary_inv,\n                   num_features=300, min_word_count=1, context=10):\n    \"\"\"\n    Trains, saves, loads Word2Vec model\n    Returns initial weights for embedding layer.\n   \n    inputs:\n    sentence_matrix # int matrix: num_sentences x max_sentence_len\n    vocabulary_inv  # dict {str:int}\n    num_features    # Word vector dimensionality                      \n    min_word_count  # Minimum word count                        \n    context         # Context window size \n    \"\"\"\n    model_dir = 'word2vec_models'\n    model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n    model_name = join(model_dir, model_name)\n    if exists(model_name) and False:\n        embedding_model = word2vec.Word2Vec.load(model_name)\n        print('Loading existing Word2Vec model \\'%s\\'' % split(model_name)[-1])\n    else:\n        # Set values for various parameters\n        num_workers = 4       # Number of threads to run in parallel\n        downsampling = 1e-3   # Downsample setting for frequent words\n        \n        # Initialize and train the model\n        print(\"Training Word2Vec model...\")\n        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n                            size=num_features, min_count = min_word_count, \\\n                            window = context, sample = downsampling)\n        \n        # If we don't plan to train the model any further, calling \n        # init_sims will make the model much more memory-efficient.\n        embedding_model.init_sims(replace=True)\n        \n        # Saving the model for later use. You can load it later using Word2Vec.load()\n        if not exists(model_dir):\n            os.mkdir(model_dir)\n        print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n        embedding_model.save(model_name)\n    \n    #  add unknown words\n    embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n                                                        for w in vocabulary_inv])]\n    return embedding_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b546b9514ce11501a75ed4a98915182f581b9975"},"cell_type":"code","source":"embedding_dim = 50\nmin_word_count = 1\ncontext = 10\n\nembedding_weights = train_word2vec(X, vocabulary_inv, embedding_dim, min_word_count, context)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"742a53bc4ac26f2b93ead1506ed71bee63836c67"},"cell_type":"code","source":"print(\"Tamanho do vocabulário: {:d}\".format(len(vocabulary)))\nprint(embedding_weights[0].shape) # número de palavras x tamanho do vetor definido.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9354c84f4a76056cc1405f00fe22d615e9a228c6"},"cell_type":"markdown","source":"Agora que temos nosso modelo de palavras transformado em vetores (*word embeddings*), podemos usá-lo na nossa rede convolucional."},{"metadata":{"_uuid":"20a40622eb1f34c06c9d811edd3c85b277e29b24"},"cell_type":"markdown","source":"## Parte 3 - Montando a rede neuronal convolucional"},{"metadata":{"_uuid":"54a1f740738372b7f3f754242ff2cd8b05813f67"},"cell_type":"markdown","source":"Primeiro definimos os parâmetros"},{"metadata":{"trusted":true,"_uuid":"9262d13ad5e5e658a165f8bf7d205fc6c0844efb"},"cell_type":"code","source":"filter_sizes = (3, 4, 5) # cada item da lista representa os tamanhos de filtro que usaremos\nnum_filters = 128 # quantidade de filtro para cada um dos tamanhos acima\ndropout_prob = (0.3, 0.5) # probabilidade de cada camada de Dropout\nhidden_dims = 64 # número de neurônios na camada densa final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d9abe3796ccb7da2511e903325cb1ff7d5f4fef"},"cell_type":"code","source":"def build_model():\n\n    for fsz in filter_sizes:\n        conv = Convolution1D(nb_filter=num_filters,\n                             filter_length=fsz,\n                             border_mode='valid',\n                             activation='relu',\n                             subsample_length=1)\n        pool = MaxPooling1D(pool_length=2)\n        \n    model = Sequential()\n    model.add(Embedding(len(vocabulary), embedding_dim, input_length=corte, weights=embedding_weights))\n    model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim)))\n\n    model.add(Dense(hidden_dims))\n    model.add(Dropout(dropout_prob[1]))\n    model.add(Activation('relu'))\n    model.add(Dense(hidden_dims))\n    model.add(Dropout(dropout_prob[1]))\n    model.add(Activation('relu'))\n    model.add(Flatten())\n    model.add(Dense(num_classes))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b0ef0ef0581a806871298776b35e3cf1ad0d6e0"},"cell_type":"code","source":"model = build_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e260284575966c297cc434b4bdc31e4e628df840"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9dccc883c395a1bc1263e4633fe2e5c36394a65"},"cell_type":"code","source":"plot_model(model, to_file='plot_model.png', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7b99e3220804de5a68899880635b68475887993"},"cell_type":"markdown","source":"![Estrutura do Modelo](plot_model.png \"Estrutura do Modelo\")"},{"metadata":{"_uuid":"a2076611de13ecc2c0dd2a8ea15d619755ed8edf"},"cell_type":"markdown","source":"Separando os dados de treino e testes."},{"metadata":{"trusted":true,"_uuid":"1491c1836c5a13ee013aa28e3d47bc6cff130116"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny = to_categorical(ylabels)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ceaf36c60f39a2a0ca59b378609eb7572e30c158"},"cell_type":"code","source":"y = to_categorical(ylabels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8b5a789f19fc791c9723f8cb4925c4f809e6967"},"cell_type":"markdown","source":"Treinando nosso modelo e verificando o desempenho"},{"metadata":{"trusted":true,"_uuid":"3d2cb36d9dfe1d224b37869cb39d24b00b204317","scrolled":true},"cell_type":"code","source":"model.fit(X_train, y_train, batch_size=64, epochs=5, validation_data=(X_valid, y_valid), verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c11ed05379c8b68f7ada909b220447fd798ab8c7"},"cell_type":"code","source":"preds = model.predict_proba(X_valid) # as previsões são probabilidades para cada uma das 3 classes\n\n#conta o número de acertos, considerando a classe de maior probabilidade\nacc_score = np.sum(np.argmax(preds,1)==np.argmax(y_valid,1))/float(len(y_valid))\n#calcula o categorical log-loss\nlog_loss_score= log_loss(y_valid, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87c011b72a50fc25d2553a50aca4b67d18c4847c"},"cell_type":"code","source":"print('Accuracy: %.4f Categorical log-loss: %.4f' % (acc_score, log_loss_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61a6ef5863e5938d5144f50e09edbbd0d3e3a4db"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}