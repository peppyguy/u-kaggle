{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41f517d4db356d2c0a4e04cbacec919c1878d4f7"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# Logistic Regression\n* When we have binary classification( 0 and 1 outputs) we can use logistic regression"},{"metadata":{"_uuid":"be50c8851e96a1cba56804dc867f7b01a52bcdb9"},"cell_type":"markdown","source":"* Computation graph of logistic regression\n<a href=\"http://ibb.co/c574qx\"><img src=\"http://preview.ibb.co/cxP63H/5.jpg\" alt=\"5\" border=\"0\"></a>\n    * Parameters to be found are weights and bias\n    * Initial values of weight and bias parameters can be chosen arbitrarily\n    * For every iteration, we are going to calculate loss function\n    * Sum of the loss function will be our cost function\n    * We are going to update weight and bias parameters using derivative of cost function and a learning rate\n    * Learning rate is a hyperparameter that is chosen randomly and tuned afterward.\n    * After many iteratios, the cost wil be minimized and we will obtain final weight and bias parameters to be used (our machine will learn them)\n    * Using these final weight and bias parameters we are going to predict a given test data"},{"metadata":{"_uuid":"cdf17c94b0122a1be015bae7b39a721828c5721f"},"cell_type":"markdown","source":"* Mathematical expression of log loss(error) function is: \n    <a href=\"https://imgbb.com/\"><img src=\"https://image.ibb.co/eC0JCK/duzeltme.jpg\" alt=\"duzeltme\" border=\"0\"></a>"},{"metadata":{"_uuid":"51bb0d607130e664f35a762ae33ec8caa06c13cf"},"cell_type":"markdown","source":"  * Example of a cost function vs weight\n   <a href=\"http://imgbb.com/\"><img src=\"http://image.ibb.co/dAaYJH/7.jpg\" alt=\"7\" border=\"0\"></a>"},{"metadata":{"_uuid":"ae3eb463841ebc2fffcf013de00e953d4f68d83e"},"cell_type":"markdown","source":"**Updating Weight and Bias**\n\n* alpha = learning rate\n* J: cost function\n* w: weight\n* b: bias\n    <a href=\"http://imgbb.com/\"><img src=\"http://image.ibb.co/hYTTJH/8.jpg\" alt=\"8\" border=\"0\"></a>\n*  Using similar way, we update bias"},{"metadata":{"_uuid":"f4d28d75d85093e6e77ec01173c8c0fabc64f794"},"cell_type":"markdown","source":"**Derivatives of cost function wrt w and b**\n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}x(  y_head - y)^T$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (y_head-y)$$"},{"metadata":{"_uuid":"4e8dea96d7f3d769c72e7792cc1dadb2ddc4cd80"},"cell_type":"markdown","source":"* For sigmoid function please visit\nhttps://en.wikipedia.org/wiki/Sigmoid_function"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/voice.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3b78e6f2de61da505606f12cbb8277805775784"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc146604faa5faec813d0a2cbfab843199052556"},"cell_type":"code","source":"data.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7f14905df63c4043655794912026b83d0558274"},"cell_type":"code","source":"# Convert label feature: female = 0 male = 1\ndata['label'] = [1 if i=='male' else 0 for i in data.label]\ndata.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cd560a04659b8436876b684d992bd563f3a332d"},"cell_type":"code","source":"# data selection\nx_data = data.drop(['label'], axis=1) # it is a matrix excluding label feature\ny = data.label.values # it is a vector wich contains only label feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12991557fea447051ceec61facc53824df582f87"},"cell_type":"code","source":"x_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05c9108f2be56a1169a71d712aab35a5de919970"},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4689bdcc012d9939eadaa6745a61047b0a9efa0b"},"cell_type":"code","source":"# normalization of x_data and obtaining x\nx = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"132fda370e30cb49a7641e6c2b9c1736873b3c12"},"cell_type":"code","source":"# train test split (we split our data into 2 parts: train and test. Test part is 20% of all data)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42) # 0.2=20%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00cbb0e20e631a10425d8f210e09aeb6709af4cc"},"cell_type":"code","source":"# take transpose of all these partial data\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79554987823c6cd69f78ab541780c96b22ceff82"},"cell_type":"code","source":"# initialize w: weight and b: bias\ndimension = 20\ndef initialize(dimension):\n    w = np.full((dimension,1), 0.01)\n    b = 0.0\n    return w,b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d2a13f3a7797afec8667390eb04c6304bc1c483"},"cell_type":"code","source":"# sigmoid function\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z))\n    return y_head\n\n# check sigmoid function\nsigmoid(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3da426b69ea76a502b6549ed1f5c7281e8822583"},"cell_type":"code","source":"def cost(y_head, y_train):\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost_value = np.sum(loss)/x_train.shape[1] # for scaling\n    return cost_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70d0225b4d63a3571c3d475d7f842c4aa2791ca4"},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train)+b\n    y_head = sigmoid(z)\n    cost_value = cost(y_head, y_train)\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train, ((y_head-y_train).T)))/x_train.shape[1]\n    derivative_bias = (np.sum(y_head-y_train))/x_train.shape[1]\n    \n    return cost_value, derivative_weight, derivative_bias","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0691ff57a2455c218d3e74dd09cd506edeba1c0"},"cell_type":"code","source":"def logistic_regression(x_train, x_test, y_train, y_test, learning_rate, num_iteration):\n    w,b = initialize(dimension)\n    cost_list = []\n    index = []\n    for i in range(num_iteration):\n        cost_value, derivative_weight, derivative_bias = forward_backward_propagation(w,b,x_train,y_train)\n        \n        # updating weight and bias\n        w = w-learning_rate*derivative_weight\n        b = b-learning_rate*derivative_bias\n\n        if i % 10 == 0:\n            index.append(i)\n            cost_list.append(cost_value)\n            print('cost after iteration {}: {}'.format(i,cost_value))\n    \n    # in for loop above, we have obtained final values of parameters(weight and bias): machine has learnt them \n           \n    z_final = np.dot(w.T,x_test)+b\n    z_final_sigmoid = sigmoid(z_final) #z_final value after sigmoid function\n    \n    # prediction\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z_final_sigmoid is bigger than 0.5, our prediction is sign 1 (y_head_=1)\n    # if z_final_sigmoid is smaller than 0.5, our prediction is sign 0 (y_head_=0)\n    for i in range(z_final_sigmoid.shape[1]):\n        if z_final_sigmoid[0,i]<= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n            \n    # print test errors\n    print('test accuracy: {} %'.format(100-np.mean(np.abs(y_prediction-y_test))*100))\n    \n    # plot iteration vs cost function\n    plt.figure(figsize=(15,10))\n    plt.plot(index, cost_list)\n    plt.xticks(index, rotation='vertical')\n    plt.xlabel('number of iteration', fontsize=14)\n    plt.ylabel('cost', fontsize=14)\n    plt.show()          ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5df7394b899dd432c6a0377189c52e7865e083f1"},"cell_type":"code","source":"# run the program\n# Firstly, learning_rate and num_iteration are chosen randomly. Then it is tuned accordingly\nlogistic_regression(x_train, x_test, y_train, y_test, learning_rate=1.5, num_iteration=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f92af9c0f874d3f5b7e0c3d3c443604e8e0a1f0f"},"cell_type":"markdown","source":"# LOGISTIC REGRESSION USING SKLEAR"},{"metadata":{"trusted":true,"_uuid":"89dd536390bf1de4a24261d9f1c8fac8fb7efa94"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train.T,y_train.T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d36f139a1526b16c6d695e903f282bd6c24c4b79"},"cell_type":"code","source":"# prediction of test data\nlog_reg.predict(x_test.T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5859b76eaefe9a5b6d18519b49e6ac8e7793b354"},"cell_type":"code","source":"# actual values\ny_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81c82f3f68a8b49a6aeb9549ba6d061d5ba0f698"},"cell_type":"markdown","source":"At first glance, we see that 21th value of predicted data is 1, however it is 0 in actual data (y_test). There can be also other wrong predictions"},{"metadata":{"trusted":true,"_uuid":"d0796b5aad55b25651e73a93f272c1911e9c248a"},"cell_type":"code","source":"print('test_accuracy: {}'.format(log_reg.score(x_train.T,y_train.T)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}