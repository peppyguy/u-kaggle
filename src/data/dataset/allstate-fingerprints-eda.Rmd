---
title: "Allstate Fingerprints (EDA)"
author: "dmi3kno"
date: "October 20, 2016"
output: 
     html_document:
            toc: true
            highlight: zenburn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 9.475, fig.height = 5)
```

> Motivation for this post is an attempt to gain some clarity about the data for the purpose of feature engineering. I think it is sad to see the next two months spent on trying to beat Faron's excellent xgboost tuning skills. Not only do I think that it would be a waste of time, but also quite frustrating and painful waste of time (remember Santander). Instead, discovering new ways of looking at the data and, hopefully, building better and more informed features could be a differentiation point. I guarantee you, it will be a lot more fun (remember Rossmann).

## Data provenance

When working with Allstate dataset, one can not get away from asking himself/herself: "What is this data? What is it all about? What does the `loss` represent and what sort of features could used to predict it?" The data is anonimyzed and I must admit, it is anonimyzed quite well. You will soon see what I mean. Kudos to Allstate data team!

```{r, echo=FALSE, warning=FALSE, message=FALSE}

#rm(list = ls())

library(data.table)
library(ggplot2)
library(forecast)


train = fread("../input/train.csv", showProgress = F)
test = fread("../input/test.csv", showProgress = F)

train <- train[, loss:=log(loss)]; test <- test[, loss:=NA]
train_ids <- train$id; test_ids <- test$id;
y_train <- train$loss

```


## Continuous features {.tabset}

We will be only looking at continious features today. Lets load the data and have a look again. I will be looking at the cross-plots of `cont` features against the label and then deeper into `cont` features on a joined dataset. Non-interactive charts today. Plotly would take forever to load.

```{r, echo=FALSE, warning=FALSE, message=FALSE}

train_test = rbindlist(list(train, test), use.names = T)

plotCrossPlots <- function(df, x_string, y_string="loss") {
   p1 <- ggplot(df, aes_string(x = x_string, y = y_string)) + geom_point(alpha=0.5) + xlab(x_string) + ylab(y_string) +  theme_bw()
    p1
 }

```

### Cont1


```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont1")

```

Do you notice vertical lines on this chart? there are some coinciding datapoint sharing the same value of `cont1`. How could we see it closer? For now, lets just remember that `cont1` is a "barcode"-shaped feature with `r length(unique(train_test$cont1))` unique values, but also quite a lot of noise.

### Cont2


```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont2")

```

This is very clear-cut ordinal feature (if not categorical). Only `r length(unique(train_test$cont2))` unique values. Somewhat unclear towards the lower end, but otherwise quite manageable as a factor (if you would decide to implement it as such). Also note, that although most features are "stretched" between `0` and `1`, this one does not go all the way to the maximum of the scale. Maximum value here is `r round( max(train_test$cont2), 4)`.

### Cont3

```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont3")

```

Not as clear as previous one (`cont2`), but they are quite similar. Also, note that there are a lot of values either taking `0` or `1`. They also exibit somewhat different behaviour. Could this feature be split into two (or even three?) Total of `r length(unique(train_test$cont3))` unique values here.

### Cont4

```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont4")

```

Wow, also quite a few similar values. Total of `r length(unique(train_test$cont4))` unique values, a bit noisy toward the higher end.

### Cont5

```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont5")

```

Quite ok in the beginning, quite crammed in the higher quarter of the data. `r length(unique(train_test$cont5))` unique values here.

### Cont6

```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont6")

```

This is a different type of animal. Notice the gap between 0 and 0.1. The rest is quite noisy. I am not going to calculate unique values here. Small positive slope with Y, but it rarely (or never) comes up very high in the xgboost feature importance. Interesting.

### Cont7

```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont7")

```

Most promising non-ordinal feature (along with cont14). Notice the gap (which implies that cont6 and cont7 probably measure the same thing). Also quite some values crammed at 0 and 1.

### Cont8

```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont8")

```

This one is somehow similar to the ordinal features we saw earlier, but it also has that "noisy" feel to it, like we saw in `cont1`. Quite a lot of unique values here: `r length(unique(train_test$cont8))`. There's also some break at around 0.85. What could that mean?

### Cont9

```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont9")

```

Similar to `cont8` with `r length(unique(train_test$cont9))` unique values. Some non-even pattern here and the gap towards the lower end.

### Cont10

```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont10")

```

Similar to `cont8` and `cont9` with `r length(unique(train_test$cont10))` unique values. Larger gaps.


### Cont11

```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont11")

```

Seems to be in the same group with `cont8`, `cont9` and `cont10` with `r length(unique(train_test$cont11))` unique values. Curving upwards towards the end (predicting higher loss).

### Cont12

```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont12")

```

Same story: `r length(unique(train_test$cont12))` unique values. Slighly curving

### Cont13

```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont13")

```

That's a funny shape. Wonder how it can be useful. Looks noisy. `r length(unique(train_test$cont13))` values.

### Cont14

```{r, echo=FALSE, warning=FALSE, message=FALSE}

plotCrossPlots(train_test, "cont14")

```

How could you expect that this feature would pop up so often on xgboost importance ranking tables?  Some outliers, noisy and irregular. `r length(unique(train_test$cont14))` unique values in total.


## Counts and diffs {.tabset}

Lets now make counts by unique values and look at the patterns. I will also make a plot of diffs, as well as ACF and PACF (we're after temporal patterns here). 
```{r, echo=FALSE, warning=FALSE, message=FALSE}
plot4plots <- function(df, x_char){ 
  layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
  count_df <- df[,.(count=.N), by=.(get=get(x_char))]
  n_unique <- length(unique(df[[x_char]]))
  hist(df[[x_char]], n_unique/2, main=x_char)
  plot(count_df[order(get)][,.(diff_order=seq_along(diff(get)),diff_value=diff(get))], main=paste0("diff(sort(unique(", x_char, "))"))
  acf(count_df[order(get)][["count"]], oma=c(1,1,1,1), main="") 
  pacf(count_df[order(get)][["count"]], oma=c(1,1,1,1), main="")
  } 
```

Base R plots will be used for speed and compactness.

### Cont1

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont1")
```
Notice the pattern in the Sorted Diff chart? This data has been distorted with some sort of algorithm, but the time pattern is clearly there. Autocorrelation and significant lags at 7 and 14. I bet that after transformation the time pattern will become even more visible. We will look into it a bit later.

### Cont2

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont2")
```

Clearly autocorrelated, but no significant lags. This is ordinal feature distorted with some soft of smooth function. looks like Lognormal PDF now, but it is really an ordinal feature, so this Sorted Diff graph should have been flat. Something special though about the range. We will talk about it as well.

### Cont3

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont3")
```

Looks ordinal to me. Softed diff should have been flat.

### Cont4

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont4")
```

Again, looks like ordinal feature. Distorted. There's no reason why lags should deteriorate in such a nice fashion.

### Cont5

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont5")
```

Same story. Needs work. No significant lags at all. Most values are 0

### Cont6

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont6")
```

Now, this is random. This is how money looks. My hypothesis is that this is some sort of value feature.

### Cont7

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont7")
```

Same here. Value. Perhaps premiums paid. Remember cont7 was correlated with Per Capita Income per state?

### Cont8

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont8")
```

Again, some soft of manipulated ordinal feature.

### Cont9

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont9")
```

Ah! Date again! This has more noise, but still quite clear pattern.


### Cont10

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont10")
```

No idea. Pretty flat Sorted Diff. I wonder if the feature is very predictive (dont remember seeing it high in ranks)

### Cont11

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont11")
```

Something with period of 5. Years? Insurance plan? Slightly distorted to mask the pattern.


### Cont12

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont12")
```

Same thing. 5 year data.

### Cont13

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont13")
```
That strange feature. Noisy. Not clear.


### Cont14

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot4plots(train_test, "cont14")
```

Autocorrelated. Random. Smells like money to me.

## Surgery {.tabset}

The Sorted Diff had some "fingerprints" from data manipulation. Lets look at one of the features closer. I am interested in dates. Lets pick feature `cont1`and that that chart up close (removing the outliers).

There were many good suggestions in the forum today about how to transform this feature. I seem to have found a very simple solution that gets me 90% where I want to be. Thanks to a very helpful article on [StackOverflow](http://stackoverflow.com/questions/25159962/correcting-barrel-lens-distortion-in-an-x-y-series-in-r). Alternative (and quite plausible) interpretation could be in the area of polar coordinates, but I did not have time to dwell on it enough to verify this hypothesis.

```{r}
undistortFun <- function(X, Y, dX, dY, radius, a, b, c, d = 1, imWidth = 640, imHeight = 480) {

    normX <- X - dX
    normY <- Y - dY

    radius_u <- radius #sqrt(imWidth^2 + imHeight^2)

    r <- sqrt(normX^2 + normY^2) /  radius_u

    Rsrc <- r * (a*r^3 + b*r^2 + c*r + d)

    theta <- ifelse(Rsrc == 0, 1, 1 / atan(Rsrc) * Rsrc)

    newX <-  theta * normX + (imWidth / 2) 
    newY <-  theta * normY #+ (imHeight / 2)

    return(data.frame(X = newX, Y = newY))
}

orig_cont1_order <- base::sort(unique(train_test$cont1))
diffs <- diff(orig_cont1_order)
df1 <- data.frame(X=1:length(diffs), Y=diffs)

df2 <- undistortFun(X=df1$X, Y=df1$Y, dX=650/2, dY=0, radius = 650/2, a=2, b=1, c=1, d=1, 
                    imWidth = length(diffs), imHeight = max(diffs))
```

Plot and compare. There's no rocket science on the parameters - they are mostly hand-picked, but I bet you can find a better algorithm!

### Original

```{r echo=FALSE, warning=FALSE, message=FALSE}
ggplot(df1[-1:-3,], aes(X, Y))+geom_point()
```
Removed couple of points in the beginning

### Corrected

```{r echo=FALSE, warning=FALSE, message=FALSE}
ggplot(df2[-1:-3,], aes(X, Y))+geom_point()
```
Data gets compressed towards the center, but hopefully does not loose the order of points. So that now, when I take `cumsum(Y)` I will have timeline of transactions.


## What is the effect?

Lets test if this in fact was useful. Make timeline, attach it to dataset, take counts and look at ACF/PACF again. Uncomment to see the charts.

```{r echo=FALSE, warning=FALSE, message=FALSE}
cat("Number of rows in unchanged:", nrow(df1) == nrow(df2))
# How else would you check if we have ruined the order of some dates?
cat("Order of observations is intact:", all(diff(df2$X)>0)) # because if we did, the indices would get messed up and diff would contain negative values.

#Original dates
orig_dates <-orig_cont1_order
new_dates <- as.numeric(cumsum(c(0,  df2$Y)))
summary(new_dates); length(new_dates)

cat("Number of dates is equivalent:", length(orig_dates)==length(new_dates))
# some dates will be negative, but that should not bother us, should it?

dates_lookup_df <- data.frame(cont1=orig_dates, cont1_corr=new_dates)

train_test_corr <- merge(train_test, dates_lookup_df, by=c("cont1"))
#plot4plots(train_test_corr, "cont1_corr")
#plotCrossPlots(train_test_corr, "cont1_corr")

```
Looking more or less the same. A bit more "crowded" in the center of the data. Still unclear why we bothered processing this feature?

## Converting to dates

Now, what we are going to do is create a function that will go through the new_dates vector and convert it to dates (or rather to the days from the beginning of the dataset). Here's an example how you would do it for the first two datapoints.

#### Observation 2
Lets take observation #2 in the `new_dates` vector. Value is `r new_dates[2]`, lag from previous value is `r new_dates[2]-new_dates[1]`.  How many actual days is it from the previous date? Well in order to find it out, we need to look at the date lags around it. Here's a tabulation of the lags within 100 observations from the current one. Note, I am defining a precision cutoff here, to group similar values together.

```{r echo=FALSE, warning=FALSE, message=FALSE}

tab1 <- as.data.frame(table(round(diff(new_dates[1:100]),4)), stringsAsFactors = F)
names(tab1) <- c("value", "freq")
ggplot(tab1, aes(x=value, y=freq))+geom_bar(stat = "identity") + coord_flip()
```

so by far the most frequent value here is `r tab1$value[which.max(tab1$freq)]`. We shall assume it is a value equivalent to *1 period*. Su our current value needs to be divided by this value and rounded to the nearest decimal (eventually to an integer, but we might want to keep precision at this point in time).

So, normalized lag is `r (new_dates[2]-new_dates[1])/as.numeric(tab1$value[which.max(tab1$freq)])`. That's like over two weeks away. Lets take the next one. 

#### Observation 3
Value is `r new_dates[3]`, a lag of `r (new_dates[3]-new_dates[2])`. Again, tabulating 100 days (which happens to be the same table). Dividing one by the other is `r (new_dates[3]-new_dates[2])/as.numeric(tab1$value[which.max(tab1$freq)])` This one is even larger! 

#### Observation 4
Value is `r new_dates[4]`, a lag of `r (new_dates[4]-new_dates[3])` . Dividing by the most frequent value from our table is `r (new_dates[4]-new_dates[3])/as.numeric(tab1$value[which.max(tab1$freq)])`. So this one is couple of months away from the previous one. 

#### Observation 5
Value is `r new_dates[5]`, a lag of `r (new_dates[5]-new_dates[4])` . Equivalent to `r (new_dates[5]-new_dates[4])/as.numeric(tab1$value[which.max(tab1$freq)])` days. A bit less than previous. 

I hope you get the idea. We will convert everything into the unit of days based on 100 nearest neighboring lags (to eliminate the remaining "bias" in the curvature of the points) and then string it back into the date sequence by `cumsum()`. At this stage we might want to round it to nearest integer to be able to convert it to time series class (`zoo` or `ts`). Hope it makes better sense now. 

All of it can be automated with a single a function.

```{r}
makeMyDates <- function(date_vector, i, window_size, digits){
  # some checks of inputs
  stopifnot(i>1, length(date_vector)>1, window_size<length(date_vector), is.null(digits)==FALSE)
  # you need to pick window size that is not too small (avoid local bias), but not too big (to avoid warning messages from previous(non-relevant) dates)
  beginning_of_window <- max(1, floor(i-window_size/2)) # half the window size but not less then 1
  beginning_of_window <- min(beginning_of_window, length(date_vector)-window_size) # avoid shrinking of window
  end_of_window <- beginning_of_window + window_size-1 
  tab1 <- as.data.frame(table(round(diff(date_vector[beginning_of_window:end_of_window]),digits=digits)), stringsAsFactors = F)
  names(tab1) <- c("value", "freq")
  tab1$value <- as.numeric(tab1$value)
  most_freq_value <- tab1$value[which.max(tab1$freq)]
  # check that most_frew_value is also within the "walking distance" from the smallest non-zero value
  min_nz_value <- min(tab1[tab1$value>0, "value"])
  # most frequent value should not be more than 2 times larger then the minimum non-zero value.
  if (most_freq_value/min_nz_value >=2) 
    warning(paste0("Warning! Most frequent value for element ", i, " seems too big. Frequencies from min_nz_value to most_freq_value are", paste0(tab1[which(tab1$value==min_nz_value): which(tab1$value==most_freq_value), "freq"], collapse=","), "\n"))
  current_lag <- date_vector[i]-date_vector[i-1]
  days_equivalent <- current_lag / most_freq_value
  return(days_equivalent)
 }

my_new_lags <- sapply(2:length(new_dates), function(x) makeMyDates(new_dates, x, 45, 4))
norm_new_dates <- as.integer(round(cumsum(c(0,  my_new_lags))),0) # return integer dates
cat("Number of dates is equivalent:", length(orig_dates)==length(norm_new_dates))
```

**UPDATE:**

Thanks to excellent [contribution from @Tony S](https://www.kaggle.com/c/allstate-claims-severity/forums/t/24551/data-pre-processing-techniques-and-how-to-work-with-them/140310#post140310) it is possible to fit a sine function (or rather a family of functions with different amplitude and vertical offset). Basics of sine and cosine functions can be referenced [here](https://www.mathsisfun.com/algebra/amplitude-period-frequency-phase-shift.html). The format of sine function is: y = A sin(Bx + C) + D , where amplitude is *A*, period is *2π/B*, phase shift is *−C/B*, vertical shift is *D*.

There are a few constants that might be useful in this case:

 - we are fitting half a period, so the constant B will be equal to π/length(df_original)
 - we are interested in varying both amplitude and vertical shift, preferrably in proportionate manner
 - through previous excercise with the de-barelling function, it was apparent that data has residual trend (tilted to the right), so we will add a linear coefficient
 - it also seems that the data is not perfectly centered (so we will add phase shift constant)
 
 We could solve all of these coefficients automatically if we could separate the points into "layers". Perhaps very clever clustering could do it, but for this size of task visual estimation by trial and error might do as well. Lets print a contour of first dozen of functions. 

```{r}
orig_cont1_order <- base::sort(unique(train_test$cont1))
df1_diffs <- diff(orig_cont1_order)
df1_original <- data.frame(X=1:length(df1_diffs), Y=df1_diffs)

plot(df1_original$Y[-1:-3]~df1_original$X[-1:-3])
for (i in 1:12){
lines( (i*(0.0012*sin(pi/630*df1_original$X)+0.0003)-5e-08*df1_original$X) ~ (df1_original$X),col=i+1,lty=2)   
}

my_new_lags_a <- apply(df1_original, 1, function(z) (z[2]+-5e-08*z[1]) / (0.0012*sin(pi/630*z[1])+0.0003))

norm_new_dates_a <- as.integer(round(cumsum(c(0,  my_new_lags_a))),0) # return integer dates
cat("Number of dates is equivalent:", length(orig_dates)==length(norm_new_dates_a))

```
Not bad at all for visual curve fitting! Correlation with the results of de-barrelling function is `r cor(norm_new_dates_a ,norm_new_dates)`. However, looking closer at what is different between the _de-barrelling_ and _sine-contour_ aproaches, one can notice up to 25 days disagreement in the beginning of dataset and up to 5 days disagreement towards the end.

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot(my_new_lags-my_new_lags_a)
```

This can probably be reduced with extra automated parameter tuning, but it seems rather difficult, since any small change of the parameters will have an enormous effect on those few outliers in the beginnig of the dataset. The outer edges of the contour get enormously compressed and it will become exceedingly difficult to discern between layers as we go up in the number of estimated curves. Lets see how good of a time-series we get by applying each of the two approaches and their mix.

## Show time!

Now we are going to look at what was this whole effort for. Time series! Again, remeber, this time series is representing number of activities per date, nothing else. It just measures how many observations fall on the same date within `cont1`

```{r echo=FALSE, warning=FALSE, message=FALSE}
make_new_train_test_df <- function(orig_dates, new_dates, train_test){
dates_lookup_df <- data.frame(cont1=orig_dates, cont1_corr=new_dates)
train_test_corr <- merge(train_test, dates_lookup_df, by=c("cont1"))
return(train_test_corr)
}

train_test_1 <- make_new_train_test_df(orig_dates = orig_dates, new_dates=norm_new_dates, train_test)
train_test_2 <- make_new_train_test_df(orig_dates = orig_dates, new_dates=norm_new_dates_a, train_test)

count_as_ts_1 <- ts(train_test_1[,.N, by=.(cont1_corr)][["N"]],f=7)
count_as_ts_2 <- ts(train_test_2[,.N, by=.(cont1_corr)][["N"]],f=7)

#plot4plots(train_test_corr, "cont1_corr")
#plotCrossPlots(train_test_corr, "cont1_corr")
par(mfrow=c(1,2)) 
Pacf(count_as_ts_1, main = "De-Barrelling Function")
Pacf(count_as_ts_2, main = "Sine-Contour")
par(mfrow=c(1,1)) 
  

```

And the winner is the _de-barrelling_ function. It seems intuitive that lags of 7, 14, and especially 21 should be well maintained throughout the series. After all, this is weekly data. In the _sine-contour_ approach we are off by a few days and it seems to be runining the sequence of weekdays and weekends. Here's ETS model using results of _de-barrelling_ function

```{r}

plot(fit<- ets(count_as_ts_1))

```

Here, it seems like we start with a weekend and maintain the weekly seasonality fairly well throughout the dataset.
How about another continuous feature? `cont7`for example. This is ETS model for mean of `cont7` by date

```{r echo=FALSE, warning=FALSE, message=FALSE}
cont7_as_ts <- ts(train_test_1[loss>0,.(mean_cont7=mean(cont7)), by=.(cont1_corr)][["mean_cont7"]], f=7)
plot(fit<- ets(cont7_as_ts))
```
Slightly underwhelming, because univariate model could not pick up any seasonality in the data.
How about median loss per date? if there's a trend, date feature can be useful.

```{r echo=FALSE, warning=FALSE, message=FALSE}
loss_as_ts <- ts(train_test_1[loss>0,.(median_loss=median(loss)), by=.(cont1_corr)][["median_loss"]],f=10)
plot(fit<- ets(loss_as_ts))
```
Again, no season. It is unlikely that either `cont7` or `loss` are happening on `cont1`
Now, it is your turn! Try another date!