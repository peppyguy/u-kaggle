{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Predicting whether a developer uses R or Python\nMyself being an avid Python user, I thought it'd be fun to see if based on this survey I could predict whether a given developer uses R or Python - and of course if so, which features allow the classifier to determine that. I'll try to keep the analysis as simple as possible and focus on clarity of code and analysis rather than on creating anything overly complex and detailed.\n\n## Conclusions?\nIf you do not want to go through the notebook, the quick conclusion is that among data scientists and analysts, Python and R users are pretty similar. It is however possible to create pretty decent classifiers for predicting whether a user uses R or Python, and there are a few funny conclusions and reasonings to be found within those classifiers.\n\n## Table of Contents:\n* [Step 1: Preprocessing the data](#preprocessing)\n    * [Step 1.1. Dealing with multiple-choice columns](#pre1)\n    * [Step 1.2. Dealing with NaNs](#nan)\n    * [Step 1.3. Dealing with Collinearity](#pre2)\n    * [Step 1.4. Creating and splitting into X and y](#pre3)\n* [Step 2: Supervised prediction of R / Python](#supervised)\n    * [Step 2.1. Random Forest classifier](#rf)\n    * [Step 2.2. Logistic Regression classifier](#lr)\n    * [Step 2.3. xgBoost Classifier](#xgb)\n    * [Step 2.4. Linear Discriminant Analysis](#lda)\n* [Step 3: Unsupervised learning with R vs. Python](#unsupervised)\n    * [Step 3.1. Clustering Analysis](#clustering)\n    * [Step 3.2. PCA Biplots](#pca)\n    * [Step 3.3. ICA Biplots](#pca)\n    * [Step 3.4. Manifold learning](#manifold)\n    * [Step 3.5. Network-based Correlation Analysis](#manifold)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"_kg_hide-output":false,"collapsed":true},"cell_type":"code","source":"from copy import deepcopy\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom tqdm import tqdm\n\nimport hdbscan\nfrom sklearn import manifold\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.base import clone\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scikitplot as skplt\nfrom matplotlib.ticker import NullFormatter","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f16d5a3a39aa73c945e5dbdd02bb6325a3309a2"},"cell_type":"markdown","source":"First things first, the data has to be prepared for analysis. In this dataset we have some columns where people have been able to select multiple values; this has to be extracted and put on a format which can be interpreted by the machine learning model. Since we're on kaggle, I've chosen to only look at data analysts and data scientists, ignoring entries from all other types of developers (e.g. it would be too easy to classify all backend developers as more likely being Python users). On the same note, I've chosen to ignore certain columns such as \"FrameworksWorkedWith\", since e.g. any user working with Django would be easily recognizable as a Python user."},{"metadata":{"trusted":true,"_uuid":"3027f5bcfe13ed05492293d0b660dacfad4bd140","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# Read in the survey results, shuffle results\ndf = pd.read_csv('../input/survey_results_public.csv', low_memory=False).sample(frac=1)\n\n# Columns with multiple choice options\nMULTIPLE_CHOICE = [\n    'CommunicationTools','EducationTypes','SelfTaughtTypes','HackathonReasons', \n    'DatabaseWorkedWith','DatabaseDesireNextYear','PlatformWorkedWith',\n    'PlatformDesireNextYear','Methodology','VersionControl',\n    'AdBlockerReasons','AdsActions','ErgonomicDevices','Gender',\n    'SexualOrientation','RaceEthnicity', 'LanguageWorkedWith'\n]\n\n# Dev types - let's only look at data scientists\nDEV_TYPES = [\n    'Data or business analyst',\n    'Data scientist or machine learning specialist'\n]\n\n# Columns which we are not interested in (predicting Python/R would be too easy with them)\nDROP_COLUMNS = [\n    'IDE', 'FrameworkWorkedWith', 'FrameworkDesireNextYear',\n    'LanguageDesireNextYear', 'DevType', 'CurrencySymbol',\n    'Salary', 'SalaryType', 'Respondent', 'Currency'\n]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ad89ea6027fc61109ef5c81c3e5b55a7dd5b0ef"},"cell_type":"markdown","source":"# Step 1: Preprocessing data<a class=\"anchor\" id=\"preprocessing\"></a>\n\n## 1.1. Dealing with multiple-choice columns<a class=\"anchor\" id=\"pre1\"></a>\nSome of the columns allow multiple options; e.g. which methodologies people have worked with etc. For all these,  I'll create new one-hot-encoding columns for each option. For all the rest of the categorical variables, I simply create dummy columns using the convenience function included in pandas. Nice and easy."},{"metadata":{"trusted":true,"_uuid":"47f9391b31d3daea85fad565683b1758a4e73719","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"%%time\n\n# Pick off data science types\ndf = df.loc[df.DevType.str.contains('|'.join(DEV_TYPES)).fillna(False)]\n\n# Drop too easy columns\nprint(f\">> Deleting columns with simple Python/R relations: {DROP_COLUMNS}\")\ndf.drop(DROP_COLUMNS, axis=1, inplace=True)\n\n# Go through all object columns\nfor c in MULTIPLE_CHOICE:\n    \n    # Check if there are multiple entries in this column\n    temp = df[c].str.split(';', expand=True)\n\n    # Get all the possible values in this column\n    new_columns = pd.unique(temp.values.ravel())\n    for new_c in new_columns:\n        if new_c and new_c is not np.nan:\n            \n            # Create new column for each unique column\n            idx = df[c].str.contains(new_c, regex=False).fillna(False)\n            df.loc[idx, f\"{c}_{new_c}\"] = 1\n\n    # Info to the user\n    print(f\">> Multiple entries in {c}. Added {len(new_columns)} one-hot-encoding columns\")\n\n    # Drop the original column\n    df.drop(c, axis=1, inplace=True)\n        \n# For all the remaining categorical columns, create dummy columns\ndf = pd.get_dummies(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2873bcdf1f4e971305c2d93ada2ea3841c7a459"},"cell_type":"markdown","source":"## 1.2. Dealing with missing values\nWe ahve quite a few missing values. For all dummy features I'll replace NaN with 0, and otherwide I'll replace with median."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5fcd99fde1d688f72e4290e1e92d2d596c42e8c2","collapsed":true},"cell_type":"code","source":"# Fill in missing values\ndf.dropna(axis=1, how='all', inplace=True)\ndummy_columns = [c for c in df.columns if len(df[c].unique()) == 2]\nnon_dummy = [c for c in df.columns if c not in dummy_columns]\ndf[dummy_columns] = df[dummy_columns].fillna(0)\ndf[non_dummy] = df[non_dummy].fillna(df[non_dummy].median())\n\nprint(f\">> Filled NaNs in {len(dummy_columns)} OHE columns with 0\")\nprint(f\">> Filled NaNs in {len(non_dummy)} non-OHE columns with median values\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc57b8865c765acc5beed340c52e9f8f6fe02263"},"cell_type":"markdown","source":"## 1.3. Dealing with Collinearity<a class=\"anchor\" id=\"pre2\"></a>\nGiven the nature of our dataset, we have several features / columns which are highly collinear, which may throw off subsequent analysis. I'll just go with a very simply option of dropping columns that have >0.75 correlations with other columns"},{"metadata":{"trusted":true,"_uuid":"c853d709563f76ad73a5427a35a8e7aa8637e10e","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = df.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n\n# Drop those columns\nprint(f\">> Dropping the following columns due to high correlations: {to_drop}\")\ndf = df.drop(to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b583bfe0d6c4793feef93acf253d23aea2ee65d9"},"cell_type":"markdown","source":"## 1.4. Creating and splitting into X and y<a class=\"anchor\" id=\"pre3\"></a>\nHaving performed the encoding of all the categorical data, the next step is to get our labels (users using only R and users using only Python), and then split our dataset into X and y, which can then be fed to the classifier."},{"metadata":{"trusted":true,"_uuid":"b40438c3e2477cc43819d1ca8dc9b8765b8c1602","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# Remove outliers for converted salary\nprint(\">> Removing salary outliers\")\ndf = df[df.ConvertedSalary < df.ConvertedSalary.mean() + df.ConvertedSalary.std()*3]\n\n# Scale dataframe\nprint(\">> Scaling non-dummy columns\")\nnondummy_columns = [c for c in df.columns if df[c].max() > 1]\nscaled_df = deepcopy(df)\nscaled_df.loc[:, nondummy_columns] = scale(df[nondummy_columns])\n\n# Create target - ignore all users who do not use either\nprint(\">> Getting R, Python and R&Python user indexes\")\nR_only_idx = (df.LanguageWorkedWith_R == 1) & (df.LanguageWorkedWith_Python == 0)\nPython_only_idx = (df.LanguageWorkedWith_R == 0) & (df.LanguageWorkedWith_Python == 1)\nR_and_Python_idx = (df.LanguageWorkedWith_R == 1) & (df.LanguageWorkedWith_Python == 1)\n\n# Set the classes\nscaled_df.loc[R_only_idx, 'RorPython'] = 0\nscaled_df.loc[Python_only_idx, 'RorPython'] = 1\nscaled_df.loc[R_and_Python_idx, 'RorPython'] = 2\nscaled_df.dropna(subset=['RorPython'], axis=0, inplace=True)\n\n# Split into X and y (with all Python / R users)\nprint(\">> Storing subset with all Python and R users\")\ny_all = scaled_df['RorPython']\nX_all = scaled_df.drop(['LanguageWorkedWith_Python', 'LanguageWorkedWith_R', 'RorPython'], axis=1)\n\n# Split into X and y (with all Python-only and R-only users)\nprint(\">> Storing subset with all Python-only and R-only users\")\ndf_only = scaled_df[scaled_df.RorPython != 2]\ny_only = df_only['RorPython']\nX_only = df_only.drop(['LanguageWorkedWith_Python', 'LanguageWorkedWith_R', 'RorPython'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c3fdaa22e5dde6f3c73e5009fb564037e94504f"},"cell_type":"markdown","source":"# Step 2: Supervised prediction of R / Python<a class=\"anchor\" id=\"supervised\"></a>\n## 2.1. Random Forest Classification<a class=\"anchor\" id=\"rf\"></a>\nNow that the data is ready, we can start trying to predict if a user prefers Python or R. I'm implementing the following small convenience function for performing 10-fold stratified cross-validation, and visualizing the results in terms of ROC, feature importance, and for the most important features, I also visualize the difference in fractions of R and Python users for each of those features."},{"metadata":{"trusted":true,"_uuid":"0afa740abac724dcaec0c30e52990fd0846c4980","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"def evaluate(clf, X, y, ignore_columns=[], plot_features=15):\n\n    # Columns to use in classification\n    use_cols = np.array([c for c in X.columns if c not in ignore_columns])\n\n    # Create 10-fold cross validated predictions\n    predicted_probas = cross_val_predict(\n        clone(clf),        \n        X[use_cols], y,\n        cv=10,\n        n_jobs=1, verbose=0,\n        method='predict_proba'\n    )\n\n    # Fit classifier on all data\n    full_clf = clone(clf)\n    full_clf.fit(X[use_cols], y)\n\n    # Create feature importance and explanations next to each other\n    _, axes = plt.subplots(1, 3, figsize=(20, 5))\n    \n    # Show ROC plot\n    skplt.metrics.plot_roc(y, predicted_probas, ax=axes[0])   \n    \n    # Extract feature importances from the random forest\n    if hasattr(full_clf, 'feature_importances_'):\n        axes[1].set_title(\"Feature importances\")\n        importances = full_clf.feature_importances_\n        \n        indices = np.argsort(importances)[::-1][:plot_features]        \n    else:\n        axes[1].set_title(\"Feature Coefficients\")\n        importances = full_clf.coef_[0]\n        indices = np.argsort(np.abs(full_clf.coef_[0]))[::-1][:plot_features]        \n    \n    # Can we put on stds?\n    if hasattr(full_clf, 'estimators_'):\n        std = np.std([tree.feature_importances_ for tree in full_clf.estimators_], axis=0)        \n    else:\n        std = np.zeros(len(importances))    \n    \n    # Create coefficient or importances plot\n    axes[1].bar(range(plot_features), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n    axes[1].set_xticks(range(plot_features))\n    axes[1].set_xticklabels(use_cols[indices], rotation=90)\n    axes[1].set_xlim([-1, plot_features])\n    \n    # Plot mean values\n    X[use_cols[indices]]. \\\n        groupby(y).mean().T. \\\n        rename(columns={0.0: \"R\", 1.0: \"Python\"}). \\\n        plot(kind='bar', ax=axes[2], title='Mean Value for Users')\n    plt.show()\n        \n# Evaluate a RF model on all the data columns\nclf_rf = ExtraTreesClassifier(n_jobs=-1, n_estimators=100, class_weight='balanced')\nevaluate(clf_rf, X_only, y_only)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a0fa912a848771984f4bc08530f25e925412914"},"cell_type":"markdown","source":"Wow, at first glance that seems to be quite the amazing classifier with an ROC of of around 0.85. Looking at the feature importances though, it's clear that it has a pretty easy job, in that almost all developers who work with Ruby or Rust are R users. Looking closer, it seems that more users are using different languages (Java, C++, Linux, Bash, etc.), the more likely it is to be a Python user (not that surprising I reckon), and if you're on a Windows machine, you're more likely to be using R. Let's try to remove some of all these columns from the analysis, and see how good the model does then."},{"metadata":{"trusted":true,"_uuid":"58f68211ea438e6c70bc5e194121493a633a42c3","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# Ignore columns with these prefixes\nignore_prefixes = ['LanguageWorkedWith_', 'PlatformWorkedWith_', 'DatabaseWorkedWith_', 'OperatingSystem_']\nignore_columns = [c for c in X_only.columns if any(check in c for check in ignore_prefixes)]\n\n# Run the model and evaluate classifier\nevaluate(clf_rf, X_only, y_only, ignore_columns=ignore_columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e792e7cf40d4eb42994f3a8636e14d6e1da86d7f"},"cell_type":"markdown","source":"Then we get an AUC_ROC of about 0.7, whic still isn't too bad. Without digging too much deeper, the model qualitatively suggests that:\n\n* If you're looking to move towards Linux next year, you're more likely a Python user\n* If you studied statistics you're more likely R, and if computer science then Python\n* If you're young (18-24 years old), you're more likely Python user\n* If you do code competitions, you're more likely a Python user\n* If you want an android next year, you're more likely a Python user\n* If you want to learn SQL next year, more likely R user\n* If you user MS office, you're more likely an R user\n* If you want an Rasperry Pi next year, you're more likely an Python user\n* If you're a full time student, you're more likely to be a Python user\n* If you're using Agile methodology, you're more likely to be a Python user\n* If you're more worried than excited about AI, then you're more likely to be an R user"},{"metadata":{"_uuid":"b30aab4945a390b7a3c8c7b7c560c0d0b4786e5c"},"cell_type":"markdown","source":"## Step 2.2. Logistic Regression Classification<a class=\"anchor\" id=\"lr\"></a>\nLet's see if a logistic regression classifier agrees with the outcome of the random forest classifier."},{"metadata":{"trusted":true,"_uuid":"54b2148d07d2e20779d1cc0753dae0b98236c84b","collapsed":true},"cell_type":"code","source":"# Evaluate a RF model on all the data columns\nclf_lr = LogisticRegression(class_weight='balanced', C=0.05)\nevaluate(clf_lr, X_only, y_only, ignore_columns=ignore_columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d9f3804858d5b7dc6ce43ce6f41e8c147773648"},"cell_type":"markdown","source":"Seems to agree pretty well with the random forest features, and the classifier performs at about the same ROC"},{"metadata":{"_uuid":"9ca8a20bab69f248c2d8db6acdd1f40a938f3655"},"cell_type":"markdown","source":"## Step 2.3. xgBoost Classifier<a class=\"anchor\" id=\"xgboost\"></a>\nLet us also try a slighly more advanced classifier, to see if we can create a better classifier"},{"metadata":{"trusted":true,"_uuid":"1955331b922554de505cf1b19d592bdd7eb71355","collapsed":true},"cell_type":"code","source":"# Evaluate a RF model on all the data columns\nclf_xgb = XGBClassifier(n_jobs=-1, n_estimators=100)\nevaluate(clf_xgb, X_only, y_only, ignore_columns=ignore_columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99b1a13c28fa4d02a8d4f8eb4eb436e10b14dc48"},"cell_type":"markdown","source":"I'd like to see if we can improve a little bit on the xgBoost classifier, so inspired by [previous notebook](https://www.kaggle.com/nanomathias/bayesian-optimization-of-xgboost-lb-0-9769), I'll perform bayesian optimization to see if we can find some better parameters."},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true,"_uuid":"4d595903d6a7ebb4742ca2fc55e806f72aea1395","collapsed":true},"cell_type":"code","source":"from skopt import BayesSearchCV\n\n# Classifier\nbayes_cv_tuner = BayesSearchCV(\n    estimator = XGBClassifier(\n        n_jobs = 1,\n        objective = 'binary:logistic',\n        eval_metric = 'auc',\n        silent=1,\n        tree_method='approx'\n    ),\n    search_spaces = {\n        'learning_rate': (0.01, 10.0, 'log-uniform'),\n        'min_child_weight': (0, 10),\n        'max_depth': (0, 50),\n        'max_delta_step': (0, 30),\n        'subsample': (0.01, 1.0, 'uniform'),\n        'colsample_bytree': (0.01, 1.0, 'uniform'),\n        'colsample_bylevel': (0.01, 1.0, 'uniform'),\n        'reg_lambda': (1e-9, 10000, 'log-uniform'),\n        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n        'gamma': (1e-9, 0.5, 'log-uniform'),\n        'min_child_weight': (0, 5),\n        'n_estimators': (50, 100),\n        'scale_pos_weight': (0.1, 10, 'log-uniform')\n    },    \n    scoring = 'roc_auc',\n    cv = StratifiedKFold(\n        n_splits=10,\n        shuffle=True,\n        random_state=42\n    ),\n    n_jobs = 4,\n    n_iter = 3,   \n    verbose = 0,\n    refit = True,\n    random_state = 42\n)\n\ndef status_print(optim_result):\n    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n    \n    # Get all the models tested so far in DataFrame format\n    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n    \n    # Get current parameters and the best parameters    \n    best_params = pd.Series(bayes_cv_tuner.best_params_)\n    print('Model #{}\\nBest ROC-AUC: {}\\nBest params: {}\\n'.format(\n        len(all_models),\n        np.round(bayes_cv_tuner.best_score_, 4),\n        bayes_cv_tuner.best_params_\n    ))\n    \n# Fit the model\nresult = bayes_cv_tuner.fit(\n    X_only[[c for c in X_only.columns if c not in ignore_columns]].values,\n    y_only.values,\n    callback=status_print\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c32aa9d57dd4dc531ae13826b43851864082205"},"cell_type":"markdown","source":"Running this for about 200 iterations, I found that it wasn't finding any better models. The results from the best model is then as follows:"},{"metadata":{"trusted":true,"_uuid":"ab7e77a09f12574e1a48528702366695c658c0bf","collapsed":true},"cell_type":"code","source":"best_params = {\n    'colsample_bylevel': 0.24573122383897958, \n    'colsample_bytree': 0.6265238053481696,\n    'gamma': 1.3485673446209135e-06,\n    'learning_rate': 0.035385067445099304,\n    'max_delta_step': 18,\n    'max_depth': 12, \n    'min_child_weight': 4,\n    'n_estimators': 93,\n    'reg_alpha': 1.9910199304506005e-05,\n    'reg_lambda': 0.0021525020638473143,\n    'scale_pos_weight': 0.1,\n    'subsample': 0.8262150586465882\n}\n# Evaluate a RF model on all the data columns\nclf_xgb = XGBClassifier(n_jobs=-1, **best_params)\nevaluate(clf_xgb, X_only, y_only, ignore_columns=ignore_columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a582cc26e6772c3420757fe0b75d622c32d95e5b"},"cell_type":"markdown","source":"Overall it doesn't seem like there's much to be gained from tuning the model; if one were to go further, one would have to start looking at feature engineering, but for me that's overkill for this notebook."},{"metadata":{"_uuid":"427344e6af196662b954d8b126c8bf8e577bf2c6"},"cell_type":"markdown","source":"## Step 2.4. Linear Discriminant Analysis<a class=\"anchor\" id=\"lda\"></a>\nGiven that we are trying to find differences between Python, R and Python&R users, we should  try to perform Linear Discriminant Analysis, which is very similar to PCA, but in addition to trying to maximize the variance in our component axes, we are also trying to maximize the separation between multiple classes. The technique is well-described in the scikit-documentation [here](http://scikit-learn.org/stable/modules/lda_qda.html)."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ddf7e6d20342aae13d035b900334293547977d2e","_kg_hide-input":true},"cell_type":"code","source":"# Colors\npyColor = '#ff7f0e'\nrColor = '#1f77b4'\nrpyColor = '#2ca02c'\nclasses = ['R', 'Python', 'R & Python']\n\ndef get_angle(v1, v2):\n    \"\"\"Calculate angle between two vectors\"\"\"\n    cosang = np.dot(v1, v2)\n    sinang = np.linalg.norm(np.cross(v1, v2))\n    return np.degrees(np.arctan2(sinang, cosang))\n\ndef annotate_embedding(loadings, pc_x, pc_y, ax, scaling=10, n_features=10, angle_thr=20):\n    \"\"\"Function for adding feature loadings to PCA or LDA embedding\"\"\"\n\n    # Find the [n_features] longest vectors in the feature loading dataframe\n    loadings['VectorLength'] = np.sqrt(loadings[pc_x]**2 + loadings[pc_y]**2)\n    loadings = loadings.sort_values(by='VectorLength', ascending=False)\n    \n    # Plot each of the longest vectors \n    for feature, row in loadings.iloc[0:n_features].iterrows():\n        vector = np.array([row[pc_x]*scaling, row[pc_y]*scaling])\n        ax.arrow(0, 0, vector[0], vector[1], head_width=0.2, head_length=0.3)\n        ax.annotate(feature, xy=(0, 0), xytext=(vector[0], vector[1]), fontsize=8)\n    \n    # Return sorted list of top features\n    top_features = loadings.index.tolist()\n    return top_features\n\ndef descriminant_analysis(clf):\n    \"\"\"Perform discriminant analysis and show results in plot\"\"\"\n    \n    # Get X without countries\n    X_subset = X_all[[c for c in X_all.columns if 'Country' not in c]]\n\n    # Run PCA on scaled numeric dataframe, and retrieve the projected data\n    trafo = clf.fit_transform(X_subset, y_all)\n\n    # The transformed data is in a numpy matrix. This may be inconvenient if we want to further\n    # process the data, and have a more visual impression of what each column is etc. We therefore\n    # put transformed/projected data into new dataframe, where we specify column names and index\n    clf_df = pd.DataFrame(\n        trafo,\n        index=X_all.index,\n        columns=[\"PC\" + str(i + 1) for i in range(trafo.shape[1])]\n    )\n\n    fig = plt.figure(figsize=(20, 10))\n    ax1 = plt.subplot(121)\n    ax2 = plt.subplot(222)\n    ax3 = plt.subplot(224)    \n\n    # How many features to check\n    n_check = 20\n\n    # Show biplots\n    clf_df.loc[y_all==2].plot(\n        kind=\"scatter\", x=\"PC1\", y=\"PC2\", ax=ax1, label=\"R & Python\", c=rpyColor, alpha=0.2\n    )\n    clf_df.loc[y_all==1].plot(\n        kind=\"scatter\", x=\"PC1\", y=\"PC2\", ax=ax1, label=\"Python\", c=pyColor, alpha=0.2\n    )\n    clf_df.loc[y_all==0].plot(\n        kind=\"scatter\", x=\"PC1\", y=\"PC2\", ax=ax1, label=\"R\", c=rColor, alpha=0.2\n    )\n\n    # Plot feature loadings on the biplot\n    scalings = pd.DataFrame(clf.scalings_, columns=['PC1', 'PC2'], index=X_subset.columns)\n    top_features = annotate_embedding(scalings, 'PC1', 'PC2', ax1, scaling=1.5, n_features=n_check)\n    top_features = top_features[0:n_check]\n    ax1.set_title(clf.__class__.__name__ + \" embedding with scalings\")    \n\n    # Show ratio of users\n    scaled_df[top_features+['RorPython']]. \\\n        groupby('RorPython').mean().T. \\\n        rename(columns={0.0: \"R\", 1.0: \"Python\", 2.0: \"Python & R\"}). \\\n        plot(kind='bar', ax=ax2, title='Mean Value for Users - for most important feature')\n    ax2.set_xticklabels([])\n\n    # Show coefficients\n    coef = pd.DataFrame(clf.coef_, columns=X_subset.columns, index=classes).T\n    coef.loc[top_features].plot(kind='bar', ax=ax3)\n    ax3.set_title(clf.__class__.__name__ + \" coefficients - for most important feature\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5d01b7836034f93a698802b0c1ee5a805824269","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# Run analysis\ndescriminant_analysis(LinearDiscriminantAnalysis())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5976a0f839e26714eef6dba7a24c0d3f892ad93c"},"cell_type":"markdown","source":"This looks quite promising. In the left-hand plot I've plotted all the respondees on the LDA subspace; i.e. used LDA for dimensionality reduction. On top of that plot, I've also put the \"loadings\" or \"scalings\", which indicates to some degree how we should intepret this subspace in terms of our original features, using the same function as previously for PCA. On the right-hand side I've picked the most important features, and plotted the mean value of those for the different groups (top) and the LDA coefficients for different groups (bottom). A few of the conclusions from previous re-occur, and a few fun new ones pop up, i.e.\n\n* More python users have had their adblocker disabled during the last month. Also more have adblocker installed though.\n* People who've been coding for 3-11 years more likely Python users, 12+ years more likely R.\n* People using Python&R are more often moderately happy than people using only one of the tools (intersting?)"},{"metadata":{"_uuid":"af7a1258cf22be0c8748449bf4ad3ad35ce9c52c"},"cell_type":"markdown","source":"# Step 3: Unsupervised learning with R vs. Python<a class=\"anchor\" id=\"unsupervised\"></a>\nNow that we have the data processed, we may as well pass it through a few unsupervised learning algorithms to see if we can find any patterns in it, and compare it for R users vs. Python users. \n\n## Step 3.1. Cluster Analyses<a class=\"anchor\" id=\"clustering\"></a>\nThe first thing that springs to mind when we are talking about unsupervised learning is clustering. I'll go for two clustering algorithms; HDBSCAN and K-Means. The reasons for chosing HDBSCAN of other typical clustering algorithms (K-Means, etc.) are nicely summarized [here](http://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html) I'm primarily including K-means as well simply just as a reference, since I'm not too fond of that algorithm."},{"metadata":{"trusted":true,"_uuid":"be9a6a3c26390a8b6328d058c31dc8f3967f1a77","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"def get_cluster_colors(clusterer, palette='Paired'):\n    \"\"\"Create cluster colors based on labels and probability assignments\"\"\"\n    n_clusters = len(np.unique(clusterer.labels_))\n    color_palette = sns.color_palette(palette, n_clusters)\n    cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n    if hasattr(clusterer, 'probabilities_'):\n        cluster_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n    return cluster_colors\n\n# Prepare figure\n_, ax = plt.subplots(1, 3, figsize=(20, 5))\nsettings = {'s':50, 'linewidth':0, 'alpha':0.2}\n\nprint(\">> Clustering using HDBSCAN\")\nclusterer = hdbscan.HDBSCAN(min_cluster_size=10)\nclusterer.fit(X_all)\n\nprint(\">> Calculating elbow plot for KMeans\")\nkmeans = KMeans(random_state=42)\nskplt.cluster.plot_elbow_curve(kmeans, X_all, cluster_ranges=[1, 5, 10, 20], ax=ax[0])\n\nprint(\">> Dimensionality reduction using TSNE\")\nprojection = manifold.TSNE(init='pca', random_state=42).fit_transform(X_all)\n\nprint(\">> Clustering using K-Means\")\nkmeans = KMeans(n_clusters=6).fit(X_all)\n\n# PLot on figure\nax[1].scatter(*projection.T, c=get_cluster_colors(clusterer), **settings)\nax[2].scatter(*projection.T, c=get_cluster_colors(kmeans), **settings)\nax[1].set_title('HDBSCAN Clusters')\nax[2].set_title('K-Means Clusters')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8819a69f3b41022a600ef9071ccf59cea6b6121"},"cell_type":"markdown","source":"In the left-most figure above we can see from the elbow plot that the optimal number of K-Means clusters is probably around 6. In the middle and right-most figures, we see the identified clusters as visualized on a default TSNE embedding (we'll play more with dimensionality reduction in following sections). \n\nIt is observed that HDBSCAN filters out a lot of the datapoints as noise, and only idenfies a few clusters, which actually overall correspond nicely to groups of points in the TSNE embedding.  It is important to remember that TSNE **does not** preserve distances or density from the original dataset, rather it tries to preserve nearest neighbors, and as a result can create \"fake\" patterns, which do not neccesarily have any easily intepretable meaning. Still, the K-Means still manages to pick out the groups of respondees suggested by TSNE, so it is definitely interesting to analyze these groups a bit further. Note, all clustering is performed in the original >700 dimensional space, and are then shown in the embedded 2D space.\n\nLet us try to inspect these identified clusters, and see if we can find what is different within these clusters as compared to the average of all the users. I'll only look at the two clusters identified by the HDBSCAN algorithm for now."},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false,"_uuid":"9b4df08a8c6cf017c37da6f4faf6a5af4ef50093","collapsed":true},"cell_type":"code","source":"# Get number of clusters identified by HDBSCAN\nunique_clusters = [c for c in np.unique(clusterer.labels_) if c > -1]\n\n# Placeholder for our plotting\n_, axes = plt.subplots(1, len(unique_clusters), figsize=(20, 5))\n\n# Go through clusters identified by HDBSCAN\nfor i, label in enumerate(unique_clusters):\n    \n    # Get index of this cluster\n    idx = clusterer.labels_ == label\n    \n    # Identify feature where the median differs significantly\n    median_diff = (X_all.median() - X_all[idx].median()).abs().sort_values(ascending=False)\n    \n    # Create boxplot of these features for all vs cluster\n    top = median_diff.index[0:20]\n    temp_concat = pd.concat([X_all.loc[:, top], X_all.loc[idx, top]], axis=0).reset_index(drop=True)\n    temp_concat['Cluster'] = 'Cluster {}'.format(i+1)\n    temp_concat.loc[0:len(X_all),'Cluster'] = 'All respondees'\n    temp_long = pd.melt(temp_concat, id_vars='Cluster')\n    \n    sns.boxplot(x='variable', y='value', hue='Cluster', data=temp_long, ax=axes[i])\n    for tick in axes[i].get_xticklabels():\n        tick.set_rotation(90)\n    axes[i].set_title(f'Cluster #{i+1} - {idx.sum()} respondees')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6cf648c21a5c6a87c8a0aeca2cc843ee0339504"},"cell_type":"markdown","source":"Going into the schema for the questions, the following things can be said about the two clusters who stick out:\n    \nOn my run, this gave two interesting clusters, both of which consists primarily of female developers, that overall have differences from the average. It's quite fun to go over the figures to see these differences compared to the average user."},{"metadata":{"_uuid":"9846986a3cd34f1a770e9c6d26c4930dbabd72a5"},"cell_type":"markdown","source":"## Step 3.2. Principal Component Analysis<a class=\"anchor\" id=\"pca\"></a>\nLet us start out by performing a PCA analysis on our data; hopefully the first few components will contain a lot of information about the dataset."},{"metadata":{"trusted":true,"_uuid":"0be00c9085c711a618aa2f0bb6b42a718dedc7c8","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# Create a PCA object, specifying how many components we wish to keep\npca = PCA(n_components=50)\n\n# Run PCA on scaled numeric dataframe, and retrieve the projected data\npca_trafo = pca.fit_transform(X_all)\n\n# The transformed data is in a numpy matrix. This may be inconvenient if we want to further\n# process the data, and have a more visual impression of what each column is etc. We therefore\n# put transformed/projected data into new dataframe, where we specify column names and index\npca_df = pd.DataFrame(\n    pca_trafo,\n    index=X_all.index,\n    columns=[\"PC\" + str(i + 1) for i in range(pca_trafo.shape[1])]\n)\n\n# Create two plots next to each other\n_, axes = plt.subplots(2, 2, figsize=(20, 15))\naxes = list(itertools.chain.from_iterable(axes))\n\n# Plot the explained variance# Plot t \naxes[0].plot(\n    pca.explained_variance_ratio_, \"--o\", linewidth=2,\n    label=\"Explained variance ratio\"\n)\n\n# Plot the cumulative explained variance\naxes[0].plot(\n    pca.explained_variance_ratio_.cumsum(), \"--o\", linewidth=2,\n    label=\"Cumulative explained variance ratio\"\n)\n\n# Show legend\naxes[0].legend(loc=\"best\", frameon=True)\n    \n# Feature loadings on each component\nloadings = pd.DataFrame(\n    pca.components_,\n    index=['PC'+str(i+1) for i in range(len(pca.components_))],\n    columns=X_all.columns\n).T\n\n# Show biplots\nfor i in range(1, 4):\n    \n    # Components to be plottet\n    x, y = \"PC\"+str(i), \"PC\"+str(i+1)\n    \n    # Plot biplots\n    settings = {'kind': 'scatter', 'ax': axes[i], 'alpha': 0.2, 'x': x, 'y': y}\n    pca_df.loc[y_all==2].plot(label='Python & R', c=rpyColor, **settings)\n    pca_df.loc[y_all==1].plot(label='Python', c=pyColor, **settings)\n    pca_df.loc[y_all==0].plot(label='R', c=rColor, **settings)\n    \n    # Show annotations on the plot\n    annotate_embedding(loadings, x, y, axes[i], scaling=15, n_features=20, angle_thr=20)\n    \n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22df47ef8297fae909a54fa723dd52fecb6fa832"},"cell_type":"markdown","source":"So with the first few components we still have around 10% of the variation in the dataset in total, which is OK. We do not see any clear patterns in the PCA, and no clear cluster of R and Python users - this is to be expected given that if we had clear clusters, then it'd be much easier to have done the classification in the previous section. Based on the PCA of the first few components, R and Python users qualitatively seem more or less the same, although there are some areas in the PCA where there might be more non-R users. This is something that could be investigated further.\n\n## Step 3.3. Independent Component Analysis<a class=\"anchor\" id=\"pca\"></a>\nLet us also try to perform an independent component analysis (ICA) of the data to see how that fares. This is basically a computational method for separating multivariate signal into additive components (independent components). It's used in pretty much the same manner in sklearn, so let's just see what it does:"},{"metadata":{"trusted":true,"_uuid":"704b96cfd99ba07cd438c7f3c38372e43cc1f01a","collapsed":true},"cell_type":"code","source":"from sklearn.decomposition import FastICA\n\n# Create a ICA object, specifying how many components we wish to keep\nica = FastICA(n_components=50, algorithm='deflation')\n\n# Run ICA on scaled numeric dataframe, and retrieve the projected data\nica_trafo = ica.fit_transform(X_all)\n\n# The transformed data is in a numpy matrix. This may be inconvenient if we want to further\n# process the data, and have a more visual impression of what each column is etc. We therefore\n# put transformed/projected data into new dataframe, where we specify column names and index\nica_df = pd.DataFrame(\n    ica_trafo,\n    index=X_all.index,\n    columns=[\"IC\" + str(i + 1) for i in range(ica_trafo.shape[1])]\n)\n\n# Create two plots next to each other\n_, axes = plt.subplots(2, 2, figsize=(20, 15))\naxes = list(itertools.chain.from_iterable(axes))\n\n\n# Show legend\naxes[0].legend(loc=\"best\", frameon=True)\n    \n# Feature loadings on each component\nloadings = pd.DataFrame(\n    ica.components_,\n    index=['IC'+str(i+1) for i in range(len(ica.components_))],\n    columns=X_all.columns\n).T\n\n# Show biplots\nfor i in range(0, 4):\n    \n    # Components to be plottet\n    x, y = \"IC\"+str(i+1), \"IC\"+str(i+2)\n    \n    # Plot biplots\n    settings = {'kind': 'scatter', 'ax': axes[i], 'alpha': 0.2, 'x': x, 'y': y}\n    ica_df.loc[y_all==2].plot(label='Python & R', c=rpyColor, **settings)\n    ica_df.loc[y_all==1].plot(label='Python', c=pyColor, **settings)\n    ica_df.loc[y_all==0].plot(label='R', c=rColor, **settings)\n    \n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f522b3a7ff45c3464ca3334d7624e2d1334ec4fb"},"cell_type":"markdown","source":"Interestingly, where the PCA didn't find much of any patterns in the first few components, here we actually do seem to find some patterns, although these patterns to not seem to pertain to R or Python users specifically. Hence, let's not go forward with this techniuqe either."},{"metadata":{"_uuid":"cef42b2c78d5a87f27937a7c6271a35979469895"},"cell_type":"markdown","source":"## Step 3.4. Manifold Learning<a class=\"anchor\" id=\"manifold\"></a>\nManifold learning are methods of non-linear dimensionality reduction; i.e. essentially reducing the dimensionality of very high-dimensional datasets while trying to retain certain characteristics between samples, such as their similarity etc. Plenty of these methods are implemented in scikit-learn, so let's try a bunch of them to see their results, as well as see if we can see a split between R and Python users within these embedded dimensions."},{"metadata":{"trusted":true,"_uuid":"39b317eae32566ad8c5c9e64518ba7799d97da59","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# Settings\nn_neighbors = 10\n\n# Figure with all embeddings\n_, axes = plt.subplots(2, 4, figsize=(20, 10))\naxes = list(itertools.chain.from_iterable(axes))\n\n# Other manifold methods\nmodels = {\n    'LLE': manifold.LocallyLinearEmbedding(n_neighbors, method='standard', eigen_solver='dense', n_jobs=4),\n    'LTSA': manifold.LocallyLinearEmbedding(n_neighbors, method='ltsa', eigen_solver='dense', n_jobs=4),\n    'Hessian LLE': manifold.LocallyLinearEmbedding(n_neighbors, method='hessian', eigen_solver='dense', n_jobs=4),\n    'Modified LLE': manifold.LocallyLinearEmbedding(n_neighbors, method='modified', eigen_solver='dense', n_jobs=4),\n    'Isomap': manifold.Isomap(n_neighbors, n_jobs=4),\n    'MDS': manifold.MDS(max_iter=100),\n    'SpectralEmbedding': manifold.SpectralEmbedding(n_neighbors=n_neighbors, n_jobs=4),\n    'TSNE': manifold.TSNE(init='pca')\n}\nfor i, (label, model) in tqdm(enumerate(models.items())):\n    \n    # Create embedding\n    Y = model.fit_transform(X_all)\n    \n    # Add plot\n    axes[i].scatter(Y[y_all==2, 0], Y[y_all==2, 1], label='Python & R', c=rpyColor, alpha=0.5)\n    axes[i].scatter(Y[y_all==1, 0], Y[y_all==1, 1], label='Python', c=pyColor, alpha=0.5)\n    axes[i].scatter(Y[y_all==0, 0], Y[y_all==0, 1], label='R', c=rColor, alpha=0.5)    \n    axes[i].legend(loc='best')\n    axes[i].xaxis.set_major_formatter(NullFormatter())\n    axes[i].yaxis.set_major_formatter(NullFormatter())\n    axes[i].set_title(label)\n    \n# Show figure\nplt.axis('tight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6002bd2c2331a757fb562c5d87a5da572ffa7dca"},"cell_type":"markdown","source":"It's pretty fun to  see how the different algorithms work differently; the code and idea is basically just adapted from [scikit learn example](http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html). For some of the `LocallyLinearEmbedding` methods it seems like certain Python users are put into areas where there are no R users, but that might as well be attributed to the fact that we simply have more Python users in the dataset, and therefore a higher propensity of \"weird\" people that fall outside the normal. MDS, IsoMap and TSNE all show that both Python and R users are very similarly distributed, so again the conclusion is that Python and R users are more or less the same.\n\n## Step 3.5. Correlation-based Network Analysis\nIt could also be fun to do some network analysis on the data - in the following I draw networks where edges are drawn between users that have a correlation coefficient above a given threshold with each other."},{"metadata":{"trusted":true,"_uuid":"998922e511ae17763955b0507b1272e949b28ddb","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# Sample subset of users\nsamples = 500\ndata = pd.concat([\n    X_all[y_all == 0].sample(samples),\n    X_all[y_all == 1].sample(samples),\n    X_all[y_all == 2].sample(samples)\n], axis=1)\n\n# Calculate correlation matrix for users\ncorr = data.T.corr()\n\n# Set colors in dataframe\ndata.loc[y_all == 0, 'color'] = rColor\ndata.loc[y_all == 1, 'color'] = pyColor\ndata.loc[y_all == 2, 'color'] = rpyColor\n\n# Transform it in a links data frame (3 columns only):\nlinks = corr.stack().reset_index()\nlinks.columns = ['from', 'to','value']\n\n# Prepare plot\n_, axes = plt.subplots(1, 5, figsize=(20, 4))\nfor i, thr in enumerate([0.25, 0.3, 0.35, 0.4, 0.45]):\n\n    # Keep only correlation over a threshold and remove self correlation (cor(A,A)=1)\n    links_filtered=links.loc[ (links['value'] > thr) & (links['from'] != links['to']) ]\n    links_filtered\n\n    # Build your graph\n    G=nx.from_pandas_dataframe(links_filtered, 'from', 'to', create_using=nx.Graph())\n\n    # Get colors of users (in proper order)\n    colors_ordered = data.reindex(G.nodes())['color']\n\n    # Plot the network:\n    nx.draw(\n        G, \n        pos=nx.spring_layout(G),\n        with_labels=False, \n        node_color=colors_ordered, \n        node_size=10,\n        edge_color='black',\n        linewidths=5,\n        ax=axes[i]\n    )\n    axes[i].axis('on')\n    axes[i].set_title(f\"Correlation threshold: {thr}\")\n\n# Save figure\nplt.savefig('network_analysis.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"373d3aaa83eeb0ee971f1cdaaf1661bd818826f5"},"cell_type":"markdown","source":"This looks extremely interesting - for low thresholds it seems like three groups are clearly separated by each other; python, R, and Python-R users. Either I did something wrong with the coloring, or... will investigate more when I have time."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"44519047b852915b53808a04366b973c3ed27bc0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}