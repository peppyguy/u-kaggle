{"cells":[{"metadata":{"_uuid":"0a3862034e8a955897ca436f3618fda0807bdfe1"},"cell_type":"markdown","source":"## Can we help foggy gaussians to lift off with bernoulli lighthouses?"},{"metadata":{"_uuid":"30a4b948951ec34cc02252968ca3c124c361be6d"},"cell_type":"markdown","source":"During the data science trainee program of Lea W. we [explored the gaussian mixture model](https://www.kaggle.com/allunia/hidden-treasures-in-our-groceries) using real dirty data of the openfoodfacts app. The treasures we were looking for are given by the hidden product categories that we tried to find by unsupervised clustering using nutrition table information. **This way we found nice clusters that already hold meaningful products like pasta, yoghurts, cookies and much more**. In addition we found an anomalistic cluster that was fully occupied by outliers or seldom products. On the coarse-grained view all results are nice and clustering was succesful.\n\n\nLet's take a look at some of these clusters! But to do so, we need to:\n\n* Load packages\n* Load data"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"455424fc4d34abc992884a4214424588301fb7bb"},"cell_type":"code","source":"########################### Load packages\n\n# This Python 3 environment comes with many helpful analytics libraries installed\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.model_selection import train_test_split\n\n########################### Load the data\n\ndata =pd.read_csv(\"../input/hidden-treasures-in-our-groceries/hidden_treasures_groceries_gmm.csv\",\n                  index_col=0)\ndata[\"product\"] = data[\"product\"].astype(str)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f68aa5e94bde0b84e182ec8974a061ceb1f8d3a"},"cell_type":"markdown","source":"Now, let's take a look at product name words of some of our clusters obtained by Gaussian Mixture:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"6537128dd23c3923746416f3ee35910d05744b6d"},"cell_type":"code","source":"def make_word_cloud(data, cluster, subplotax, title):\n    words = data[data.cluster==cluster][\"product\"].apply(lambda l: l.lower().split())\n    cluster_words=words.apply(pd.Series).stack().reset_index(drop=True)\n\n    text = \" \".join(w for w in cluster_words)\n\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud(max_font_size=30, max_words=30,\n                          background_color=\"white\", colormap=\"YlGnBu\").generate(text)\n\n    # Display the generated image:\n    \n    subplotax.imshow(wordcloud, interpolation='bilinear')\n    subplotax.axis(\"off\")\n    subplotax.set_title(title)\n    return subplotax\n\nfig, ax = plt.subplots(1,2, figsize=(20,5))\nmake_word_cloud(data, 8, ax[0], \"Yoghurt and Milk\")\nmake_word_cloud(data, 9, ax[1], \"Pasta\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76c1377cad5648621d9ec1b411ac243b41c6e76d"},"cell_type":"markdown","source":"These clusters look really nice and they are what we are seeking for.  But diving deeper into cluster statistics and looking at violinplots of nutrient distributions per cluster we revealed **strange behaviours of our model**: Beside the anomalistic cluster we found an mixed type, but not anomalistic, cluster that showed the same nutrient distributions except from 2 features: salt and proteins. In these nutrients these normal cluster showed sharp zeros. Thus is contained products that were typed in by users with zero fat and zero proteins. "},{"metadata":{"trusted":true,"_uuid":"bb303545dc8b218653f969a16d11c14aa02e3b7c","_kg_hide-input":true},"cell_type":"code","source":"nutrients = [\"fat_100g\",\n             \"proteins_100g\",\n             \"carbohydrates_100g\",\n             \"sugars_100g\", \n             \"other_carbs\",\n             \"salt_100g\",\n             \"g_sum\"]\nenergies = [\"energy_100g\", \"reconstructed_energy\"] \n\ntransformed_nutrients = [\"transformed_\" + nutrient for nutrient in nutrients]\ntransformed_energies = [\"transformed_\" + energy for energy in energies]\n\ndef make_violin(subax, cluster, nutrients):\n    pos = np.arange(1, len(nutrients)+1)\n    part = subax.violinplot(\n            data[data.cluster==cluster]\n                [nutrients].values,\n            showmeans=True,\n            showextrema=False)\n    subax.set_title(\"Feature distributions of cluster: \" + str(cluster), size = 20)\n    subax.set_xticks(pos)\n    subax.set_xticklabels(nutrients)\n    set_color(part, len(nutrients))\n    return subax\n\ndef set_color(axes, num_colors):\n    cm = plt.cm.get_cmap('RdYlBu_r')\n    NUM_COLORS=num_colors\n    for n in range(len(axes[\"bodies\"])):\n        pc = axes[\"bodies\"][n]\n        pc.set_facecolor(cm(1.*n/NUM_COLORS))\n        pc.set_edgecolor('black')\n    return axes\n\nfig, ax = plt.subplots(2,2,gridspec_kw = {'width_ratios':[3, 1]}, figsize=(30,10))\npair00 = make_violin(ax[0,0], 6, nutrients)\nax[0,0].set_ylim([0,100])\npair01 = make_violin(ax[0,1], 6, energies)\nax[0,1].set_ylim([0,4000])\npair10 = make_violin(ax[1,0], 12, nutrients)\nax[1,0].set_ylim([0,100])\npair11 = make_violin(ax[1,1], 12, energies)\nax[1,1].set_ylim([0,4000])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72464b2adf691bcf66cb009a08b4c25baa77884a"},"cell_type":"markdown","source":"Both clusters spread widely into the feature space and contain outliers in their features. Looking at the anomalies per cluster, we can see that cluster 6 is highly anomalistic whereas cluster 12 is not. And that is really strange!"},{"metadata":{"trusted":true,"_uuid":"488370f7c99c19d3575db92fd2bce1fbb88ba2da"},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(20,5))\ncounts_6 = data[data.cluster==6].anomaly.value_counts() / data[data.cluster==6].anomaly.count() * 100\ncounts_12 = data[data.cluster==12].anomaly.value_counts() / data[data.cluster==12].anomaly.count() * 100\nsns.barplot(x=counts_6.index, y=counts_6.values, ax=ax[0], palette=\"Set2\")\nax[0].set_title(\"Anomaly detection in cluster 6\")\nax[0].set_xlabel(\"normal: 0 - anomal: 1\")\nax[0].set_ylim([0,100])\nax[0].set_ylabel(\"Percentage of cluster data\")\nsns.barplot(x=counts_12.index, y=counts_12.values, ax=ax[1], palette=\"Set2\")\nax[1].set_title(\"Anomaly detection in cluster 12\")\nax[1].set_xlabel(\"normal: 0 - anomal: 1\")\nax[1].set_ylim([0,100])\nax[1].set_ylabel(\"Percentage of cluster data\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d36517a5937391ae1c6eb11676c6ee02c91b7e5b"},"cell_type":"markdown","source":"### Why did that happen? \n\nWell... for us it's still not easy to understand. Both clusters have means in their features that are close to each other except from proteins and fat. Recapping the definition of anomaly in the view of a gaussian mixture model we can say that **our model detects an anomaly if the sample is placed in a low-density region of our feature space**. But this mixed-type cluster with zero-fat-zero-proteins is very dense in these two features as all samples are placed at only one point: 0 :-). And even if all other nutrient features spread widely into the space, this **heavy sample density on the zero plane may causes the model to say that this is not anomalistic**. \n\n"},{"metadata":{"_uuid":"64350750604aee9df596174907ef1b93e0935cd8"},"cell_type":"markdown","source":"\n### And how can we escape from the mighty zero-plane?\n\nDuring this kernel I will try to improve the model by gluing Bernoullis and Gaussians that try to explain the somehow discrete and continous nature of nutrition table information. Hopefully the bernoullis will guide the gaussians to better results, especially in anomaly detection. In the beginning the kernel might look very crowded with math and code as I need to derive and implement the new model. Unfortunately I can only find tools for either Gaussian or Bernoulli Mixture Models and not for mixed typed as well. Hence I have to roll up my sleeves and put some work into implementations.\n\nI'm excited and curios if this new model can lift off the gaussians! :-)"},{"metadata":{"trusted":true,"_uuid":"54f1daa5014624aada06659790ccac8495b55411"},"cell_type":"markdown","source":"## Building lighthouses \n\nBefore we can start to dive into implementation and analysis, we have to derive the model and its learning procedure:\n\n$$ p(\\hat{x}) = \\sum_{k=1}^{K} \\pi_{k} \\cdot B_{k}(\\chi| \\nu_{k}) \\cdot N_{k}(x| \\mu_{k}, \\Sigma_{k})$$\n\nIn contrast to the gaussian mixture model you can see that the density we need to detect anomalies is now calculated by multiplying gaussians with bernoullis. Let's try to understand this model:\n\n$$ B(\\chi_{n}| \\hat{\\mu}_{k}) = \\prod_{d=1}^{D} \\nu_{k,d}^{\\chi_{n,d}} \\cdot (1-\\nu_{k,d})^{(1-\\chi_{n,d})} $$\n\nThe bernoulli distribution describes the discrete nature of the features. To make it act we need new features $\\chi_{n,d}$:\n* A discrete nutrient feature that holds 1 if a nutrient of a product is zero (for example zero fat) and 0 in the case where it's greater than zero. This way we **describe the zeroness of a nutrient**. In contrast to pure nutrients like fat, proteins etc. the energy features and the g_sum feature should be treated differently:\n* For energies we like to know if there is a **discrepancy between the user given energy and the reconstructed energy**. Now we could set 1 in cases where the reconstructed energy is higher than the user given energy and 0 for the otherway round. Even if almost all samples have discrepancies between these energies you might think that this feature is useless, but I think it acts like a separator that makes it possible to explain some more patterns like products where the nutrients have all zero entries but energy is positive and hence higher than reconstructed. \n* In addition we can try to cover the errors when the sum of all nutrients exceed 100g (g_sum) and the case where energy exceeds 3700. We exclude these kind of errors during fitting but for model performance test on test data these **new features of exceeding nature** could be very helpful. \n\nLet's explore the discreteness of our data :-)"},{"metadata":{"_uuid":"abc134009ca086d90d6e9d5febfa47e3ad5b88f1"},"cell_type":"markdown","source":"## The islands where lighthouses live\n\nFirst of all, let's try to understand couplings, for example one might ask: \"Do products with zero proteins often have zero fat as well?\" and the other way round: \"Do products with zero fat often have zero proteins?\" "},{"metadata":{"_uuid":"5eda77bcb15c047d9b5fca4385039a7ae2af1319"},"cell_type":"markdown","source":"### Can we find some coupled zero-zero features?"},{"metadata":{"trusted":true,"_uuid":"68721af0a7b50e43ef07f58a028dcc6777829d5b","_kg_hide-input":false},"cell_type":"code","source":"lighthouses = pd.DataFrame(index=data.index)\nlighthouses[\"zero_proteins\"] = np.where(data.proteins_100g == 0, 1, 0)\nlighthouses[\"zero_fat\"] = np.where(data.fat_100g == 0, 1, 0)\nlighthouses[\"zero_sugars\"] = np.where(data.sugars_100g == 0, 1, 0)\nlighthouses[\"zero_carbs\"] = np.where(data.carbohydrates_100g == 0, 1, 0)\nlighthouses[\"zero_other_carbs\"] = np.where(data.other_carbs == 0, 1, 0)\nlighthouses[\"zero_salt\"] = np.where(data.salt_100g == 0, 1, 0)\n\nlighthouses[\"energy_separator\"] = np.where(data.reconstructed_energy >= data.energy_100g, 1, 0)\nlighthouses[\"exceeds_energy\"] = np.where(data.energy_100g > 3700, 1, 0)\nlighthouses[\"exceeds_reconstructed_energy\"] = np.where(data.reconstructed_energy > 3700, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac4203c6069737ee50e7802454a7f8b8273eb12e"},"cell_type":"code","source":"cols_of_interest = [col for col in lighthouses.columns if \"zero\" in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32762868679525cca1b38ef36cf43cae67660f8b","_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"def get_percentage(feature):\n    result = lighthouses[lighthouses[feature] == 1].sum(axis=0) \n    result /= lighthouses[lighthouses[feature] == 1][feature].count() \n    return result\n\nzero_heatmap = pd.DataFrame(index=lighthouses.columns)\n\nfor col in lighthouses.columns:\n    zero_heatmap[col] = np.round(get_percentage(col) * 100)\n\nplt.figure(figsize=(10,5))\nsns.heatmap(zero_heatmap.loc[cols_of_interest, cols_of_interest].transpose(),\n            cmap=\"coolwarm\", annot=True, cbar=False, fmt=\"g\")\nplt.ylabel(\"Products with \")\nplt.xlabel(\"contain this percentage of \")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe4a68fe0538cf5f38a6a30984bfa40f21f55326"},"cell_type":"markdown","source":"### Take-Away\n\nOk, first the questions and then in general:\n\n* Products with zero proteins have in 82 % of cases zero fat as well. And the other way round: Products with zero fat have in 62 % of cases zero proteins. We can see an imbalanced coupling of the zeroness of proteins and fat. Think of it - does it make sense? What could be a product with zero fat? .... Water, drinks in general, vegetables, perhaps fruits... these products are likely to have no proteins as well. I'm curious if we can reveal what is hidden behind such groups. \n* The next obvious pattern is between zero carbs, zero sugars and zero other carbs. It make sense that products with zero carbohydrates have zero sugars and zero other carbs as the latter are themselves carbohydrates. \n* Now let's take a look at the smoother patterns: Products with zero salt often have zero fat and proteins as well. And one antipattern: products with zero salt, proteins and fat often consists of carbohydrates. \n\nNext topic to discover: \n\n### Which is the feature with the highest count of zeroness over all products of our data?"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"df485327544051d1dcda8f691b6fe03e7bf7cafb"},"cell_type":"code","source":"zeroness_per_product = lighthouses[cols_of_interest].sum(axis=1).value_counts() / lighthouses.shape[0] * 100\npercentage_zeroness = lighthouses.sum(axis=0) / lighthouses.count(axis=0) * 100\npercentage_zeroness = percentage_zeroness.loc[cols_of_interest]\npercentage_zeroness = percentage_zeroness.sort_values()\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.barplot(x=percentage_zeroness.index, y=percentage_zeroness.values, order=percentage_zeroness.index, \n           palette=\"Reds\", ax=ax[0])\nax[0].set_ylabel(\"% in data\")\nsns.barplot(zeroness_per_product.index, zeroness_per_product.values)\nax[1].set_xlabel(\"Number of zero nutrients per product\")\nax[1].set_ylabel(\"% in data\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad5d8fa782ef3df03c4689397def730a642eff0b"},"cell_type":"markdown","source":"### Take-Away\n\n* We can see that most products have zero fat, zero other carbohydrates or zero proteins. Interestingly carbohydrates themselves and sugars are often higher than zero. Perhaps the data given is full of sweets or perhaps the industry likes sugar in each product? :-)\n* The second plot shows that most products have no zero nutrients. This are all data spots that are not somehow sticked to the mighy zero planes. In addition we can see that multiple zero nutrients higher than 3 are seldom. They would probably not have caused the problem that clusters that are sticked to zero planes are of high density even though they consist of many outliers. Hence the cause of our problem is mainly due to the products with 1, 2 or 3 zero nutrients. "},{"metadata":{"_uuid":"f35e85ca84062d2f3f100fda7b718849b632a1da"},"cell_type":"markdown","source":"## Foggy Gaussians\n\nNow we have gained an impression what discrete features of nutrients can tell us. We have in mind that zero fat does often come with zero proteins and we know that there are roughly 55 % of products with no zero-nutrients and 45 % with one or more. The latter probably causes our anomaly detection problem as high counts on the zero planes or axes lead to very high densities. We changed our model by introducing bernoulli distributions $B_{k}$ that explain the discreteness of each cluster:\n\n$$ p(\\hat{x}) = \\sum_{k=1}^{K} \\pi_{k} \\cdot B_{k}(\\chi| \\nu_{k}) \\cdot N_{k}(x| \\mu_{k}, \\Sigma_{k})$$\n"},{"metadata":{"_uuid":"483b3e5ea18edd5c3802670dd6a3e8019c7bf1c1"},"cell_type":"markdown","source":"Doing so they change the way we calculate densities and hopefully this will change the cluster formation process such that we can obsvere even nicer patterns and detect anomalies suffienctly. Now, let's take a look at the second part of our model, the gaussians $N_{k}$:\n\n$$ N(x_{n}| \\mu_{k}, \\Sigma_{k}) = \\frac{1}{\\sqrt{(2\\pi)^{d}\\det\\Sigma_{k}}} \\cdot \\exp \\left( \\frac{1}{2} \\cdot (x_{n} - \\mu_{k})^{T}  \\Sigma_{k}^{-1} (x_{n} - \\mu_{k}) \\right) $$ \n"},{"metadata":{"_uuid":"a31c71c07128d73d3e8a2054366139ea935d495f"},"cell_type":"markdown","source":"In contrast to the Bernoullis they describe the continuous nature of our nutrients. Each nutrient should be allowed to cover a range of 0 up to 100g. As this is not described by the zero-features we need the original ones. That's the reason why we have $\\chi$ for Bernoulli and $x$ for the original continuous features. \n\nIf you take a look at two nutrients of your choice of the following,\n\n* proteins_100g\n* fat_100g\n* salt_100g\n* sugars_100g\n* carbohydrates_100g\n* other_carbs\n\nyou can recap the different nature of cluster 6 and 12. This way you can see that both clusters contain products that are highly different in the way zeroness occurs in their nutrients. This is the second motivation to extend the old Gaussian Mixture Model: Adding the discreteness as a second aspect for clustering may help to detect more pattern. We expect that products that are similar in their zeroness build own groups! :-) Beside anomaly detection there should be an improvement of clustering as well! "},{"metadata":{"trusted":true,"_uuid":"5386606ff483b8941a96c9294cfbec5812a35dca"},"cell_type":"code","source":"features = [\"sugars_100g\", \"salt_100g\"]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d5a22d6d7ff36154c2c2d3d838c481048144ed91"},"cell_type":"code","source":"sns.set()\nfig, ax = plt.subplots(1,2, figsize=(20,5))\nax[0].scatter(data[(data.cluster==6) & (data.anomaly==1)][\"transformed_\" + features[0]].values,\n              data[(data.cluster==6) & (data.anomaly==1)][\"transformed_\" + features[1]].values, s=1, alpha=0.5, color=\"coral\")\nax[0].set_title(\"Anomalistic cluster of old GMM\")\nax[0].set_xlabel(\"Boxcox transformed \" + features[0])\nax[0].set_ylabel(\"Boxcox transformed \" + features[1])\nax[1].scatter(data[(data.cluster==12) & (data.anomaly==0)][\"transformed_\" + features[0]].values,\n              data[(data.cluster==12) & (data.anomaly==0)][\"transformed_\" + features[1]].values, s=1, alpha=0.5, color=\"mediumaquamarine\")\nax[1].set_title(\"Non-Anomalistic counterpart cluster of old GMM\")\nax[1].set_xlabel(\"Boxcox transformed \" + features[0])\nax[1].set_ylabel(\"Boxcox transformed \" + features[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c285d57508327c2a72b6948a2901642b2d50b7ce"},"cell_type":"markdown","source":"## Expectation maximization\n\nOk, now we will dive deeper and set up the learning process for our model with expectation maximization algorithm. Its a general method to make mixture models learn and has a well-founded theory that is beyond the scope of this kernel. I will heavily build upon explanations of the book \"Pattern recognition and machine learning\" of [Christopher Bishop](https://www.microsoft.com/en-us/research/people/cmbishop/) that is one of my favorites. \n\nThe next lines will be full of math and some of these lines are just a recap for myself. So if you don't want to go into details for yourself, make a jump to the next chapter. ;-)\n\n### Log-Likelihood of incomplete data $D = [x_{n}]_{n=1}^{N}$\n\nLet's take a look at our model again:\n\n$$ p(\\hat{x}) = \\sum_{k=1}^{K} \\pi_{k} \\cdot B_{k}(\\chi| \\nu_{k}) \\cdot N_{k}(x| \\mu_{k}, \\Sigma_{k})$$\n\nIt describes the probability density given by our samples within the feature space over all nutrients we take into account. We want the model to describe our data well. It should fit to our data as much as it can and the only way to shape it as we want is by tuning its parameters: the component probabilities $\\pi_{k}$ the bernoulli centers $\\nu_{k}$ as well as the gaussian centers $\\mu_{k}$  and their covariances $\\Sigma_{k}$. We want to fit our model most likely to our data and we are searching for a maximum likelihood solution with respect to our parameters. As we have gaussians with exponentials we expect that its easier to consider the log of our probability density:\n\n$$ \\ln p(\\hat{x}) = \\ln \\left( \\sum_{k=1}^{K} \\pi_{k} \\cdot B_{k}(\\chi| \\nu_{k}) \\cdot N_{k}(x| \\mu_{k}, \\Sigma_{k}) \\right)  $$\n\nOh, here comes the trouble: As we have a sum over $k$ cluster components the $\\ln$ can't act directly on our bernoullis and gaussians :-( . Maximizing this function with respect to our parameters is not tractable in this case. For this reason we need a new approch!  "},{"metadata":{"_uuid":"310ca72ba307e05481c3f1513b0d4b086c1ce8ce"},"cell_type":"markdown","source":"### Log-Likelihood of complete data $D = [x_{n}, z_{n}]_{n=1}^{N}$\n\nThings would become much nicer if we could place the $\\ln$ inside the sum. But without clear motivation this is idea is not worth to try. In our current state we have given something like that:\n\n$$\\ln p(X|\\theta) = \\ln \\left( \\sum_{Z} p(X,Z|\\theta) \\right) $$\n\nThe Z stands for our hidden latent varibales, the product categories we like to obtain by clustering. For each data spot $x_{n}$ there is one $z_{n,k}$ that describes which component $k$ is related to it. In this case $z_{n,k}$ holds one (hot) wheras all other elements of this vector hold zeros. Now imagine you already know $\\vec{z}_{n}$. In this case you would have given the complete data, the nutrition information and the product categories. In this case it would be easy to compute $\\ln p(X,Z)$. As we don't have it, the least we can do is the following:\n\n* Choose some parameters $\\theta^{*}$ randomly and compute expectations of $\\ln p(X,Z|\\theta)$ under some probability distribution $\\tilde{p}(j)$:\n\n$$E\\left[ \\ln p(X,Z)\\right] = \\sum_{j} \\tilde{p}(j) \\ln p(X,Z|\\theta) = \\sum_{Z} p(Z|X, \\theta^{*}) \\cdot \\ln p(X,Z|\\theta) $$\n\n* Given parameters $\\theta^{*}$ and our data spots $X$ we are able to compute the responsibilities for each component to generate the data spots. This is described by the probability distribution $p(Z|X, \\theta^{*})$. Computing them is part of the **E-Step**.\n* If we know these responsibilites we can than maximize our expectation with respect to the parameters $\\theta$. Now, this part, called **M-Step** is nice because we can compute $\\ln p(X,Z|\\theta)$ as $\\ln$ acts directly on our bernoullis and gaussians. This way we obtain new parameters $\\theta^{*}$ and we can repeat the procedure. "},{"metadata":{"trusted":true,"_uuid":"3883917a3af68b0cc43968d0b0129daa960bdebd"},"cell_type":"markdown","source":"#### E-Step:\n\nOk, first part for us is to derive the E-Step for our model...\n\ncoming soon\n\n....\n\n$$ \\gamma_{nk} = \\frac{\\pi_{k} \\cdot B(x_{n}|\\nu_{k}) \\cdot N(x_{n}|\\mu_{k}, \\Sigma_{k})}{\\sum_{j=1}^{K}\\pi_{j} \\cdot B(x_{n}|\\nu_{j}) \\cdot N(x_{n}|\\mu_{j}, \\Sigma_{j})}$$\n\n\n#### M-Step:\n\n$$ N_{k} = \\sum_{n=1}^{N} \\gamma_{nk}$$"},{"metadata":{"trusted":true,"_uuid":"172f7caaa9bd0f7da271b2e929fde42e60bf2dc9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}