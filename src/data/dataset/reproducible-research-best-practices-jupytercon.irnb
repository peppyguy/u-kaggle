{"cells":[{"metadata":{"_uuid":"bbcdb40f470b2027adac31d6f410e9569845f284"},"cell_type":"markdown","source":"This notebook contains supporting materials for the workshop [Reproducible research best practices (highlighting Kaggle Kernels)](https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68323) at JupyterCon 2018. \n\n <table cellspacing=\"1\" cellpadding=\"1\" border=\"0\" width=\"100%\" bgcolor=\"#000000\">\n  <tr>\n    <td bgcolor=\"#FFFFFF\" font-size=\"30px\">\n      <h1 style=\"font-size:1.5vw\"> <strong>Learning goal</strong>: By the time you finish this workshop, you will have a research project hosted on Kaggle that can be reproduced in one click. (Two if you count logging in.)\n       </h1>\n    </td>\n  </tr>\n</table>"},{"metadata":{"_uuid":"8544f994317169e8bdc72f33b2c8d6c765f997ec"},"cell_type":"markdown","source":"# Introduction\n___\n\n### What’s “reproducibility” anyway?\n\nWhat I mean by \"reproducibility\" here is the ability to exactly re-create an earlier analysis given the same data. \n\nYou can think about reproducibility as a spectrum: the less time it takes to get the same result you did with the resources you provide, the more reproducible your work is. The time to reproduce a research project can range from months (if you need to rebuild a project from scratch) to a few seconds (if the original researchers provide executable code and the environment to run it in). My goal is to get you closer to the “seconds” end of the spectrum and farther away from the “months”. \n\n### What will we do today?\n\nToday, we'll focus on four aspects necessary for reproducible research:\n\n* [**Step 0:** Organizing and planning your project](#Step-0)\n* [**Data**: Format and document your data for easy use and reuse](#Data)\n* [**Code**: Make your code easy for other people (or you in the future) to run and understand](#Code)\n* [**Computing environment**: Standardize the computing environment your code is run in to ensure consistent output](#Environment)\n\nHere's the breakdown of what we'll be doing when:\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n.tg .tg-us36{border-color:inherit;vertical-align:top}\n.tg .tg-dvpl{border-color:inherit;text-align:right;vertical-align:top}\n@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;}}</style>\n<div class=\"tg-wrap\"><table class=\"tg\">\n  <tr>\n    <td class=\"tg-dvpl\">9:00 - 9:30</td>\n    <td class=\"tg-us36\">Introductions &amp; Set-up</td>\n  </tr>\n  <tr>\n    <td class=\"tg-dvpl\">10:00 - 11:00: </td>\n    <td class=\"tg-us36\">Project planning &amp; organization</td>\n  </tr>\n  <tr>\n    <td class=\"tg-dvpl\">11:00 - 12:30:<br></td>\n    <td class=\"tg-us36\">Sharing data</td>\n  </tr>\n  <tr>\n    <td class=\"tg-dvpl\">12:30 - 1:30: </td>\n    <td class=\"tg-us36\"> Lunch! :)</td>\n  </tr>\n  <tr>\n    <td class=\"tg-dvpl\">1:30 - 2:30:</td>\n    <td class=\"tg-us36\">Preprocessing &amp; utility code</td>\n  </tr>\n  <tr>\n    <td class=\"tg-dvpl\">2:30 - 3:30:</td>\n    <td class=\"tg-us36\"> Analysis/modelling code & Figures<br></td>\n  </tr>\n  <tr>\n    <td class=\"tg-dvpl\">3:30 - 4:30:</td>\n    <td class=\"tg-us36\">Work/debugging time</td>\n  </tr>\n  <tr>\n    <td class=\"tg-dvpl\">4:30 - 5:00: </td>\n    <td class=\"tg-us36\">Wrap up &amp; reflection</td>\n  </tr>\n</table></div>"},{"metadata":{"_uuid":"f3a98874992116c668721635642fa99bc1257429"},"cell_type":"markdown","source":"# Step 0\n\n\"Well begun is half done.\" Aristotle\n\n_________________\n\nThe best way to make your work easy to reproduce is to ensure that both your code and data are logically organized and that it's clear how they're supposed to fit together.  The best time to figure out how you want your project to be organized is before you begin. The second best time is now!\n\n> **What about exploratory projects?** For projects where I start off with a lot of exploratory code, I like to have two versions of the project. The first is just a single large file that I use like a lab notebook, just adding more code at the end. Then, once I have a good idea where the project is going, I'll tidy up my project structure so that it's easier to work with. \n\n## Organizing code and data\n\nThere are lots of possible ways to organize code and data, and as long as you have some sort of plan, then can all work just fine. This is my personal favorite way to organize the code and data files for a analysis-based (rather than systems building) research project. (For a system building or evaluation type project, check out [these slides](https://goo.gl/4kqNzq).) \n\nHere, I'm assuming that you'll be generating two different kinds of figures. One which shows the results of a statistical analysis or model (Figures 1 and 2 in this example) and one which summarizes the data itself (Figure 3 in this example). In each case, I'm sharing the input data I used to generate each figure, all the steps necessary to get there, and the figure file itself.\n\n![](https://svgshare.com/i/7s0.svg)\n\nThis architecture is built around two key ideas here: \n\n * Research projects should be as modular as possible\n * Save the output of every step\n\nLet's dig into these. \n\n### Modular code and data\n\nThere are two compelling reasons to break up your code & data into small chunks. One is that it saves you time: if you just want to change the margins on a figure showing the results of a model that took a long time to train, you shouldn't have to re-train the model. If you save your model when you're done training it and do your plotting in a separate file, you can quickly make the change you need.\n\nAnother reason to break up your code and data is that it makes it much easier to find things. I think we've all been at the point where we have a single huge notebook or script and have to spend minutes scrolling and searching to find that one specific line we need to update. Segmenting and organizing your code helps you (and your co-authors) avoid this.\n\nA final reason to modularize your code in this way (which I know won't apply to everyone) is because it makes it much easier to integrate into LaTeX or RMarkdown: each file corresponds to a single chunk so it's very easy to keep things in order. \n\n### Save the output of every step\n\nIn addition to making your life easier, as discussed above, this makes it much easier for you to reproduce your work. If you need to re-generate a specific figure based on your code at a specific point in time, all you need to do is refer to the version you used to generate that figure (assuming you're using version control).\n\n> **Version control:** Version control allows you to track changes to your documents and see what your files looked like at a specific point in time. We're not going to talk about it in depth here because Kaggle automates much of this process for you: every time you update your data files a new version is saved & you can create a stable version of your code that you can refer to later using the \"Commit\" button in the upper right hand corner. \n\nIt also makes life much easier for people reproducing your work: if I just want to do some additional evaluation of your trained model, if you've shared your trained model that means I don't need to train it myself, which saves me money and time.\n\nOf course the structure I've proposed won't work for every project, so I'd like you to spend some time figuring out what sort of structure to use for your data and adding it to your notebook so that you can have it to refer to later. \n\n### Other organizational structures\n\n* Python/Jupyter: [Cookiecutter Data Science: A logical, reasonably standardized, but flexible project structure for doing and sharing data science work.](https://drivendata.github.io/cookiecutter-data-science/)\n* R: [ProjectTemplate is a system for automating the thoughtless parts of a data analysis project](http://projecttemplate.net/architecture.html)\n\nAnother common project structure is to store your custom functions in a utility script that you then import and use in other code as necessary. Personally, I think it's fine to have a few small functions in with the rest of your analysis code, but if you have many larger functions or you re-use them a lot, then you might find this helpful. (See [this project as an example](https://www.kaggle.com/rtatman/reproducing-research-men-also-like-shopping).) This structure looks something like this:\n\n![](https://svgur.com/i/7rs.svg)\n\n## Exercise:\n____\n\n1. Take some time to sketch out a diagram showing the structure you'd like your research code and data to have.\n2. Find a partner and share your proposed structure and diagram. Take turns giving each other feedback. \n3. Revise your structure until you're happy with it and then add it to your notebook. (I used [Google Draw](https://docs.google.com/drawings) to make my figures, but feel free to use whatever you like.\n\nIf you didn't bring your own research for the exercises today, here are some research projects you can work on reproducing:\n\n* Python, Jupyter: [Tracking wakefulness as it fades: micro-measures of Alertness](https://github.com/SridharJagannathan/Jagannathan_Neuroimage2018)\n* Python, Jupyter: [The Flash-Lag Effect as a Motion-Based Predictive Shift](https://github.com/laurentperrinet/Khoei_2017_PLoSCB)\n* R, Rmd: [Jasbi, Jaggi, Frank (2018): Conceptual and prosodic cues assist disjunction acquisition](https://github.com/jasbi/JasbiJaggiFrank_cogsci2018)\n* Python, R, Jupyter: [An Experimental Study of Cryptocurrency Market Dynamics](https://github.com/pkrafft/An-Experimental-Study-of-Cryptocurrency-Market-Dynamics)\n* Python: [Weather conditions determine attenuation and speed of sound: Environmental limitations for monitoring and analyzing bat echolocation](https://onlinelibrary.wiley.com/doi/10.1002/ece3.4088) (Code is in .zip files at the bottom of the page in the \"Supporting Information\" drop down, data is [on Dryad](https://datadryad.org/resource/doi:10.5061/dryad.k5d3280).)\n* Python & R: [Distribution of lifespan gain from primary prevention intervention](https://openheart.bmj.com/content/3/1/e000343). This is a complex one: I'd recommend picking a single figure & focusing on that one.\n* R: [Analysis of sheep saliva peptides with mass spectrometry](https://github.com/jaredraynes/Sheep_Saliva)"},{"metadata":{"_uuid":"b998cce8c2ef69805c2c70a0530e24bb68c04949","_execution_state":"idle","trusted":false},"cell_type":"markdown","source":"# Data\n\n“Data sharing may not be the answer to everything but data hoarding is the answer to nothing.” Harlan Krumholz\n\n___\n\nNow that you know how you're going to structure your project, you're ready to get your data ready to share. \n\n## Data sharing checklist\n\nWhen I'm getting ready to share a dataset, I consider three types of factors: ethical, practical and legal ones. \n\n### Ethical considerations\n\n* If you're sharing human subjects data, do you have permission to share it? (I add a section detailing how data sharing will work and allowing participants to opt in to my consent forms. You can find an example from the NIH [here](https://www.ninds.nih.gov/sites/default/files/sample_data_sharing_consent_form_508C.pdf).)\n* If your data is covered by specific regulations (HIPAA, FERPA, GDPR), are you sharing it in a way that is compliant with those regulations?\n* If you're sharing anonymized data, have you done a last pass to ensure robust anonymization? (E.g. to a pre-specified level of k-anonymity, l-diversity or making sure you've removed all the required information for [Safe Harbor](https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html))\n\n### Practical considerations\n\n* Have you documented your dataset? You should include a description of what data is in your dataset, as well as how and why it was collected. \n* Do you have all the files you want to share in a single directory? Do you know what's in each file?\n* If your dataset is very large (> 20 Gigabytes) you might consider sharing a sample rather than the whole thing. If so, how will you sub-sample your data? \n* Kaggle's datasets platform will take care of a lot of the practical considerations for you, which is nice. :)\n\n### Legal considerations\n\n* Does your dataset already have a license? (If your data is a derivation of another dataset with a license that transfers to derivatives, then it does!) Double check that you're sharing data in a way that's consistent with that license.\n* If not, have you picked a license to release it under? I personally like [CC-BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/) but there are [lots of good open source data licenses](https://project-open-data.cio.gov/open-licenses/).\n\n## Exercise:\n____\n\nRun through the data-sharing checklist above and apply it to your project. Are there any other concerns that come up for you or folks in your discipline? Write about them in your notebook. \n\n## Uploading data to Kaggle\n\nTo upload data to Kaggle, you have two options. You can either upload data through the graphical interface or from the command line through the API. I'm linking to the documentation here instead of retyping it so that this notebook will remain current. :)\n\n* API: [API documentation](https://github.com/Kaggle/kaggle-api), [Uploading a dataset walk-through](https://codelabs.developers.google.com/codelabs/upload-update-data-kaggle-api/index.html?index=..%2F..%2Findex#1)\n* Website: [Website documentation](https://www.kaggle.com/docs/datasets#creating-a-dataset)\n\n### A note on file formats\n\nThere are lots of file formats you can use to share your data, but in my experience some of them are easier to work with, especially after several years have passed. Here are my personal file format Do's and Don'ts:\n\n#### DON'T\n\n**Use a serialized format.** These are things like pickles or .RData files They're designed to be read directly into your current environment as variables. While they can be handy for quickly passing variables back and forth, they are extremely brittle to differences between environments: a single subversion update for a package you're using can mean you're no longer able to read in your data. And, because most formats are stored as binary files, it can be really, really hard to get your data out of them. I would strongly recommend against using a serialized format to share research data. \n\n**Rely on a proprietary data format.** If you need to purchase a specific piece of software in order to open a data file, this represents a significant (in some cases insurmountable) barrier to reproducing your work. Besides financial considerations, what happens when the software needed to open your files is no longer supported or available? Non-proprietary data formats are almost always more portable.\n\n#### DO\n\nI pretty much completely agree with the [Library of Congress](http://www.loc.gov/preservation/resources/rfs/data.html) on this one. (Who am I to argue with folks whose whole job it is to preserve and share data!) In their words, the preferred file formats for sharing data are:\n\n> * Platform-independent, character-based formats are preferred over native or binary formats as long as data is complete, and retains full detail and precision. Preferred formats include well-developed, widely adopted, de facto marketplace standards, e.g.\n>     * Self-describing, e.g. JSON, XML-based data formats using well known schemas, XML-based data formats accompanied by schema employed\n>     * Line-oriented, e.g. TSV, CSV, fixed-width\n>     * Platform-independent open formats, e.g. SQLite (.sqlite, .db, .db3)\n> * Character Encoding, in descending order of preference:\n>     * UTF-8, UTF-16 (with BOM)\n>     * US-ASCII or ISO 8859-1\n>     * Other named encoding\n\nThe only thing that I would add is that, for sharing trained ML models, hdf5 is emerging as a standard within the field, so I would recommend using that.\n\n## Exercise:\n____\n\nUpload your dataset to Kaggle and complete the dataset quality checklist: \n\n* Add a subtitle — Stand out on the listings page with a snappy subtitle\n* Add tags — Make it easy for users to find your dataset in search\n* Add a description — Share specifics about the context, sources, and inspiration behind your dataset\n* Upload an image — Make your dataset pop with an eye-catching cover image and thumbnail (<- I recommend using Unsplash or Wikimedia to find CC0 images)\n* Add file information — Help others navigate your dataset with a description of each file\n* Include column descriptors — Empower others to understand your data by describing its features\n\nYou can also see this checklist under the \"Overview\" tab for your dataset, and it will be automatically updated as you complete each step.\n\n> **Important note:** Your data is by default private unless you make it public or add a contributor, so if you want to share a link to your data you need to make sure you make it public first! If you want to share just code and not data (for privacy reasons, for example) you can make public kernels on a private Dataset which will allow anyone to view your kernel, but not the underlying private data source."},{"metadata":{"_uuid":"15104f8c708e2ecd5821fc6e78f4ed87e9c0569a"},"cell_type":"markdown","source":"# Code\n\n“An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures” [Buckheit and Donoho, 1995](https://statweb.stanford.edu/~wavelab/Wavelab_850/wavelab.pdf)\n\n____\n\nIn this section I'm going to cover some best practices for writing code that's easy to read and use. I want to make sure you don't feel overwhelmed and like you need to immediately follow every rule and suggestions here, though! Here's a strict ordering of where my priorities are for research code:\n\n1. Make it run\n2. Make it available\n3. Make it pretty \n\nIt's far more important for reproducibility to make your runnable research code available (on any platform!) than to feel that it has to be beautiful before you share it. I know it's easy to get caught in a perfectionist loop of getting everything as tidy as possible before you share it, but available is far better than perfect!\n\nThat said, there are some simple tips that you can follow to make your research code easier to read and use. \n\n### Making your code readable\n\nThere are lots of little changes you can make to make your code easier to read, but these are the things that I find help me the most:\n\n*  **Put all your imports, `import x` or `library(x)` at the top of your notebook**. This is especially helpful for reproducing work because installing packages tends to be the biggest hassle, and getting all that work out of the way up front means that you're not blindsided by it later on.\n*  **Make sure to use white space and use it consistently. **  I think of writing code a little like writing a paper: white space can serve like the breaks between paragraphs to help you group similar concepts together and help your readers follow the logical flow of the text.\n*  **Break up long lines at logical places.** For example, if you have a function that takes a lot of arguments, you can try inserting a line break after each argument. Long lines that wrap in a viewer are very difficult to read and understand. In both Python and R, I also really, *really* dislike the use of `;` to write multiple statements on the same line. \n* **Make your variable names logical and human readable.** If you've parsed the text of the Rubáiyát of Omar Khayyám and saved the parse trees in a variable called \"x_298\", as a newcomer to the project I'll have no idea what's in that variable. Something like \"rubaiyat_parse_trees\", on the other hand, clearly tells me what's in this variable. \n* **Comment your code!** Especially for research code, when your intended audience may not be as fluent in your language of choice, I err on the side of more and more explicit comments. One of my proudest technical accomplishments is the fact that one of my co-authors was actually able to learn R based on my commented research code.  \n* **Use consistent patterns of capitalization.** The prevailing pattern in data science tends to be lower_case_with_underscores for functions and variables, but as long as you're consistent it doesn't matter too much.\n\n####Python example: \n\nThese two chunks both sample 1000 random points from the normal distribution and then plot them as a histogram with a density curve... but one of them is significantly easier to read and understand.  \n\n**Do this:**\n```\nimport numpy as np\nimport matplotlib.pyplot as pltsigma\n\n# generate random normal distribution\nmu, sigma = 0, 0.1 # mean and standard deviation\nsample = np.random.normal(mu, sigma, 1000)\n\n# plot histogram with density curve \ncount, bins, ignored = plt.hist(sample, 30, density=True)\nplt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n         np.exp( - (bins - mu)**2 / (2 * sigma**2) ), \n         linewidth=2, color='r')\nplt.show()\n```\n\n **Not this:**\n```\nimport numpy as np\nM, s1 = 0, 0.1 \n S2 = np.random.normal(M, s1, 1000)\nimport matplotlib.pyplot as plt; x, Y, zZ = plt.hist(S2, 30, density=True); plt.plot(Y, 1/(s1 * np.sqrt(2 * np.pi)) * np.exp( - (Y - M)**2 / (2 * s1**2) ), linewidth=2, color='r'); plt.show()\n```\n\nI *wrote* the code in the cell above and I barely know what parts do what. :|\n\n####R example:\n\n**Do this:**\n```\nlibrary(reshape2)\nlibrary(ggplot2)\n\n# create a matrix with 100 random values sampled\n# from the normal distribution & melt it\nmatrix_normal <- matrix(rnorm(100),\n                       nrow=10, \n                       ncol=10)\nmatrix_melted <- melt(matrix_normal)\n\n# create a heatmap from our matrix\nggplot(data = matrix_melted, aes(x=Var1, y=Var2, fill=value)) + \n    geom_tile()\n```\n\n **Not this:**\n```\n b_1 = matrix(rnorm(100),nrow=10, ncol=10) ; library(reshape2)\n MC <- melt(b_1)\nlibrary(ggplot2)\n  ggplot(data = MC, aes(x=Var1, y=Var2, fill=value)) + geom_tile()\n```\n\n\n### Licensing your code\n\nI'll skim over licensing because this is something Kaggle takes care of for you: if you make your code public, it's automatically covered by the [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) license. However, if you're sharing your code on a different platform, you do want to make sure that you distribute a license with your code. You will probably also want to license your data and code separately, since there are specialized license for each. \n\n### Style guides and you\n\nYou're probably familiar with a style guide for academic writing like MLA, APA or Chicago/Turabian. But you may not have known that there are style guides for code as well! Following a style guide will ensure that your code is consistent both internally and with a larger community. Which style guide you use is a matter of preference, but here are some options:\n\n* (R) [Tidyverse style guide](http://style.tidyverse.org/)\n* (R) [Google style guide](https://google.github.io/styleguide/Rguide.xml)\n* (Python) [PEP 8](https://www.python.org/dev/peps/pep-0008/)\n* (Python) [Google style guide](https://github.com/google/styleguide/blob/gh-pages/pyguide.md)\n\nThe easiest way to make sure your code follows a consistent style guide is to check & fix it automatically using a linter.\n\n> **[Linter](http://en.wikipedia.org/wiki/Lint_%28software%29):** A linter or lint refers to tools that analyze source code to flag programming errors, bugs, stylistic errors, and suspicious constructs. You can think of it like a grammar checker but for your code. \n\nBelow are some options for linters. They can take a little bit of set up, but once you're all set up using a linter can make your life much easier (and your code prettier!). \n\n* (R) [lintr](https://github.com/jimhester/lintr)\n* (Python) [Pylint](https://www.pylint.org/)\n* (Python) [autoflake](https://github.com/myint/autoflake) (only removes unused imports or variables)\n* (Python) [flake8](http://flake8.pycqa.org/)\n\n### Accessing & creating files\n\nWhen you're accessing files in your code, remember to use relative rather than absolute file paths. If you're new to these concepts, I've written an intro notebook [here](https://www.kaggle.com/rtatman/reproducibility-tips-absolute-vs-relative-paths).\n\nWhen creating file, try to be careful about choosing names. I **highly** recommend [Jenny Bryan's guide on naming files](http://www2.stat.duke.edu/~rcs46/lectures_2015/01-markdown-git/slides/naming-slides/naming-slides.pdf), but here are some of her key things to AVOID these things in file names:\n\n - spaces in file names\n - punctuation\n - accented characters\n - different files named “foo” and “Foo” (capitalization is the only difference)\n - file names that don't tell you about the content of the file\n \n The main benefit of these things is that it will make it easier for you to find files, tell what’s in them and to interact with them programmatically.\n\n### Controlling randomness\n\nIf your code has an element of randomness in it (where the output depends on which random number was used to generate it), you'll need to make sure to set all the random number generators (RNGs) your code depends on.  \n\nIn Python, it's fairly complex to set every single random seed (especially if you're doing deep learning). Here's a selection of some of the most common ones. The variable \"my_seed\" here can be any number, as long as it's the same \n\n* \"Global\" random seed: random.seed(my_seed)\n    * This sets the seed within the random module & modules that depend on it.\n* Numpy & Keras random seed: np.random.seed(my_seed) \n    * Won't work if you're running code on multiple threads\n* Tensorflow: tf.set_random_seed(my_seed)\n* Anything that uses hash randomization (like Theano): PYTHONHASHSEED=0 \n    * Python 3 only\n    * This can be a security risk so don’t do it by default\n* cuDNN: ¯\\_(ツ)_/¯\n    * Some routines (like backward pass & backward pooling) do not guarantee reproducibility because they use atomic operations\n\nIn R, things are a little easier. There's a global variable `set.seed(my_seed)` that most packages rely on. The exceptions are packages that are wrappers around other languages, like Keras, which you can see a [ discussion of here](https://keras.rstudio.com/articles/faq.html#how-can-i-obtain-reproducible-results-using-keras-during-development).)\n\n## Exercise: \n____\n\nFor this exercise, we're going to be uploading code to Kaggle & getting it to run in the Kaggle notebook environment. Unless you wrote your code on Kaggle initially, this might be an exercise that building empathy with folks who are trying to get your code to work on their local computers!\n\nI expect that this part may take quite a bit of time (depending on where your project is currently) so start at the top and work as far down as you can. If you finish your project early (congrats!), you can either spend some time applying the best practices discussed above to your code or check out one of the projects linked in [the first section](#Step-0) and work on reproducing it as well.\n\n* **Zeroth (optional):** If you have a lot of utility functions that you use in your project, you may want to put them in a separate script and import them into your notebooks. You can see an example of how to do that [here](https://www.kaggle.com/rtatman/import-functions-from-kaggle-script?scriptVersionId=5121855).\n* **First:** Separate out all of your pre-processing code and use it to pre-process the data you uploaded earlier. (If it's not Python 3 or R, you may want to skip this step and upload your already pre-processed data instead.) When you've got it working, make sure your save your pre-processed data to your current working directory and commit your notebook. A versioned copy of your preprocessed data will be generated and saved when you commit your code.\n* **Second:** Now, add your preprocessing notebook as a data source to your analysis notebook. (Use the [+ Add Data]  button on the right hand side, click on the \"Kernel Output Files\" tab, filter by your work and then add your pre-processing kernel.) If you're planning on plotting analyses or models, make sure to save your models to the local working directory and commit your notebook. If you don't have any analysis and only summary figures, skip to the next step. \n* **Third- Nth:** Create a new notebook for each of your visualizations (with the appropriate kernel or dataset as input). Save your visualizations as outputs and run your kernel. \n\n> **Important note:** Your code is private by default unless you make it public or add a contributor, so if you want to share a link to your code you need to make sure you make it public first."},{"metadata":{"_uuid":"caf435c9d11d2931df0e3017c8b6168af0db5c0e"},"cell_type":"markdown","source":"# Environment\n\n____\n\nFor the most part, if you're working on Kaggle you don't need to worry about controlling for the environment; someone else forking your notebook will run it in the same environment you wrote it in. \n\nOf course, we want you to be able to move your work on and off of Kaggle if you need to, so in this section I'm going to cover how to get information on the environment used to run your code on Kaggle. If you're looking for other options for hosting your code & data, my co-authors and I discuss a number of them in [this paper](https://goo.gl/awupXz), which was published at the 2nd Reproducibility in Machine Learning Workshop at ICML this year (although I think Kaggle Kernels are pretty great 😁). \n\nTo download your code, you can use the cloud with a down arrow button by the \"Commit\" button, the \"Download Code\" button at the top of the \"Code\" tab of your compiled notebook or use [the API](https://www.kaggle.com/docs/api#interacting-with-kernels). You can also download, upload and run kernels using [the API](https://github.com/Kaggle/kaggle-api) if you're the type of person that likes working from the command like. \n\n### Getting environment information\n\nYou've got a number of options for how to get information on your environment on Kaggle. The simplest way is to check out our open source Docker [Python](https://github.com/Kaggle/docker-python) and [R](https://github.com/Kaggle/docker-rstats) containers. You can fork and run these locally. (Make sure you're looking at the version that corresponds with the kernel commit you want to reproduce!)\n\nIf you add additional custom packages to your kernel, a new Docker image is created for you and associated with your notebook. You can then use that Docker image to launch new notebooks later on.\n\nYou can also get the environment information programmatically from within a kernel. Here's an example of how to access and save information on the current session in R. It will be created and saved as a file every time you commit your notebook (so you have a record of each session you committed under). You can find the file under the \"Output\" tab of your compiled notebook. "},{"metadata":{"trusted":true,"_uuid":"325a8a007e3fb5aa1a61e12b83f08d95fb933a3b"},"cell_type":"code","source":"# check out session information\nsessionInfo()\n\n# get session info and write to file\nwriteLines(capture.output(sessionInfo()), \"sessionInfo.txt\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab1146fed2bd6d0d4f19402ed41abe772efac25a"},"cell_type":"markdown","source":"In Python, you can get this same information using the `sys_info()` function provided by IPython & `pip freeze`. \n\n    import IPython\n\n    # print system information (but not packages)\n    print(IPython.sys_info())\n\n    # get module information\n    !pip freeze > frozen-requirements.txt\n\n    # append system information to file\n    with open(\"frozen-requirements.txt\", \"a\") as file:\n        file.write(IPython.sys_info())"}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}