{"cells":[{"metadata":{"_uuid":"ebe7d5d24760e748cc10610139d8c04dd9673557"},"cell_type":"markdown","source":"# CNN Exercise - Deep Learning for Computer Vision\n__Introduction:__\n\nDeep neural networks have tremendous potential to learn complex non-linear functions, patterns, and representations. This includes real-world applications like image categorization and classification and the very popular concept of image artistic style transfer. Computer vision is all about the art and science of making machines understand high-level useful patterns and representations from images and videos so that it would be able to make intelligent decisions similar to what a human would do upon observing its surroundings. \n\nConvolutional neural networks or CNNs are extensively used for automated feature extraction in images.  In fact, CNNs are similar to the general deep neural networks, but with explicit assumption of input being a data set where which the location of a feature is relevant can be attempted via CNNs  like image, but not limited to then. Others examples are:\n- ***Time series***: your data is well ordered. A time series problem would make a 1–d convolution the right choice.\n- ***Weather***: Build a map of current weather conditions (location-based values, but not actual images). Add another dimension to it for the previous weather maps (in order) and you have a 4–d convolution problem to predict the weather.\n\nThis notebook explore convolutional neural networks through the task of image classification using publicly dataset  CIFAR-10. We will utilize our understanding of CNNs to then take on the task of style transfer and understand how neural networks can be used to understand high-level features. Through this notebook, we cover the following topics:\n- Image classification use CNNs from scratch\n- Transfer learning: image classification using pretrained models\n- Neural style transfer using CNNs\n\nFor an in-depth understanding of CNNs applied for visual recognition take look on the [Stanford course material](http://cs231n.github.io/convolutional-networks). Let us see a little brief overview of its key concepts:\n- ***A CNN is made up of Layers***: Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters. the main layers are:\n    - ***Convolutional Layer***: Is a set of slides or convolves learnable filters, also known as kernels or convolution matrix, to help capture spatial features. These cover the width, height and the full depth (color range) of the image. During the forward pass, we slide the filter across the width and the height of the image while computing the dot product between the filter attributes and the input at any position. The output is a two-dimensional activation map from each filter, which are then stacked to get the final output.\n    - ***Pooling Layer***: These are basically down-sampling layers used to reduce spatial size and number of parameters by apply functions such as max, average, L2-norm, and so on. These layers also help in controlling overfitting.  These layers are insert in between conv layers or in the end of a sequence of them.\n    - ***Fully Connected Layer***: This layer helps perform the tasks of classification. It is similar to fully connected layers in general neural networks. These have full connections to all neurons in the previous layer and can followed by a Dropout to help to reduce overfit.<p>\n- ***Parameter Sharing***: Conv layers use same set of weights across the filters thus reducing the overall number of parameters required.\n\nCNNs have gone through tremendous research and advancements have led to more complex and power architectures, like VGG-16, VGG-19, Inception V3, and many models that are more interesting.\n\nLet's start our studies:\n![image](http://cs231n.github.io/assets/cnn/convnet.jpeg)"},{"metadata":{"toc":true,"_uuid":"1864e11107cd510df9347e3e699fe83b4fa980b7"},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparing-environment-and-uploading-data\" data-toc-modified-id=\"Preparing-environment-and-uploading-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparing environment and uploading data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Packages\" data-toc-modified-id=\"Import-Packages-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import Packages</a></span></li></ul></li><li><span><a href=\"#Load-and-Prepare-Data\" data-toc-modified-id=\"Load-and-Prepare-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load and Prepare Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scaling-the-Data\" data-toc-modified-id=\"Scaling-the-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Scaling the Data</a></span></li><li><span><a href=\"#Prepare-the-target-variable\" data-toc-modified-id=\"Prepare-the-target-variable-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Prepare the target variable</a></span></li><li><span><a href=\"#Data-Augmentation\" data-toc-modified-id=\"Data-Augmentation-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Data Augmentation</a></span></li><li><span><a href=\"#Set-Global-Variables-and-Seed\" data-toc-modified-id=\"Set-Global-Variables-and-Seed-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Set Global Variables and Seed</a></span></li></ul></li><li><span><a href=\"#Image-Multiclassifier-using-CNNs-from-scratch\" data-toc-modified-id=\"Image-Multiclassifier-using-CNNs-from-scratch-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Image Multiclassifier using CNNs from scratch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-the-CNN-Model\" data-toc-modified-id=\"Create-the-CNN-Model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Create the CNN Model</a></span></li><li><span><a href=\"#Visualize-the-network-architecture\" data-toc-modified-id=\"Visualize-the-network-architecture-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Visualize the network architecture</a></span></li><li><span><a href=\"#Final-evaluation-of-the-model\" data-toc-modified-id=\"Final-evaluation-of-the-model-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Final evaluation of the model</a></span></li><li><span><a href=\"#Predict-class-of-image-in-practice\" data-toc-modified-id=\"Predict-class-of-image-in-practice-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Predict class of image in practice</a></span></li><li><span><a href=\"#Model-Interpretation\" data-toc-modified-id=\"Model-Interpretation-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Model Interpretation</a></span></li></ul></li><li><span><a href=\"#CNN-with-Pre-Trained-Models\" data-toc-modified-id=\"CNN-with-Pre-Trained-Models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>CNN with Pre-Trained Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-The-Model-by-a-Pre-Trained-Model\" data-toc-modified-id=\"Create-The-Model-by-a-Pre-Trained-Model-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Create The Model by a Pre-Trained Model</a></span></li><li><span><a href=\"#Final-evaluation-of-the-model\" data-toc-modified-id=\"Final-evaluation-of-the-model-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Final evaluation of the model</a></span></li><li><span><a href=\"#Predict-class-of-image-in-CNN-Pre-Trained-Classifier\" data-toc-modified-id=\"Predict-class-of-image-in-CNN-Pre-Trained-Classifier-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Predict class of image in CNN Pre-Trained Classifier</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"},{"metadata":{"_uuid":"4833fa19c5e5b547b96d8a86b924ba648d020c49"},"cell_type":"markdown","source":"## Preparing environment and uploading data\nYou can download the this python notebook and data from my [github repository](https://github.com/mgmarques/Studies-on-Kaggle). The data can download on Kaggle [here](https://www.kaggle.com/sgazer/cifar10batchespy/downloads/cifar10batchespy.zip/1).\n\n### Import Packages"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport pylab \nfrom PIL import Image\nfrom IPython.display import SVG\nimport matplotlib.pyplot as plt\nparams = {'legend.fontsize': 'x-large',\n          'figure.figsize': (15, 5),\n         'axes.labelsize': 'x-large',\n         'axes.titlesize':'x-large',\n         'xtick.labelsize':'x-large',\n         'ytick.labelsize':'x-large'}\nplt.rcParams.update(params)\n\n%matplotlib inline\n'''\nimport seaborn as sns\nsns.set(style=\"ticks\", color_codes=True, font_scale=1.5)\nfrom matplotlib import pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\nfrom matplotlib.colors import ListedColormap\n'''\n\nimport math\nimport timeit\nfrom six.moves import cPickle as pickle\nimport platform\n#from subprocess import check_output\nimport glob\n\nimport tensorflow as tf\nimport keras\nfrom keras.constraints import maxnorm\n#from keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom keras.utils.np_utils import to_categorical   \nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import applications\nfrom tqdm import tqdm_notebook\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bc0ec1fa96493db3a25bde39114322123effbe0"},"cell_type":"markdown","source":"Set the GPU for use and check if it successful activate."},{"metadata":{"_uuid":"caca6ea392173ad90a102b303c1c71b480165108","trusted":false},"cell_type":"code","source":"use_gpu = torch.cuda.is_available()\nuse_gpu","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35c821a3994c9f061bdc1b19ee4cc2c98978e0b0"},"cell_type":"markdown","source":"## Load and Prepare Data\nThe CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images divided into five training batches and one test batch, each with 10000 images. . \n\nThe dataset was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton and is available at [cs.totonto.edu](https://www.cs.toronto.edu/~kriz/cifar.html) as well as through the datasets module in keras through the keras.datasets module.\n\nSimilar to any Machine Learning algorithm, neural networks also require the input data to be certain shape, size, and type. Therefore, before we reach the modeling step, the first thing is to preprocess the data itself. The following snippet gets the datasets from the different batches files. First, each file need to be unpickled, then the independent variables and dependent variable are separate, next the dependent variables are separated in their respective colors channels and reshape according the size of the image, then the data are converted to a numpy array and transpose to reorganize the data from 3,32,32 to 32,32,3. Finally, the data is append to can used as a unique dataset."},{"metadata":{"_uuid":"b9075a06f0ff2dd261c45169a363d72ee10f9679","trusted":false},"cell_type":"code","source":"def unpickle(fname):\n    with open(fname, \"rb\") as f:\n        result = pickle.load(f, encoding='bytes')\n    return result\n\ndef getData():\n    labels_training = []\n    dataImgSet_training = []\n    labels_test = []\n    dataImgSet_test = []\n\n    # use \"data_batch_*\" for just the training set\n    for fname in glob.glob(\"../input/cifar-10-batches-py/*data_batch*\"):\n        print(\"Getting data from:\", fname)\n        data = unpickle(fname)\n\n        for i in range(10000):\n            img_flat = data[b\"data\"][i]\n            #fname = data[b\"filenames\"][i]\n            labels_training.append(data[b\"labels\"][i])\n\n            # consecutive 1024 entries store color channels of 32x32 image \n            img_R = img_flat[0:1024].reshape((32, 32))\n            img_G = img_flat[1024:2048].reshape((32, 32))\n            img_B = img_flat[2048:3072].reshape((32, 32))\n            \n            imgFormat = np.array([img_R, img_G, img_B])\n            imgFormat = np.transpose(imgFormat, (1, 2, 0))  #Change the shape 3,32,32 to 32,32,3 \n            dataImgSet_training.append(imgFormat)\n            \n    # use \"test_batch_*\" for just the test set\n    for fname in glob.glob(\"../input/cifar-10-batches-py/*test_batch*\"):\n        print(\"Getting data from:\", fname)\n        data = unpickle(fname)\n\n        for i in range(10000):\n            img_flat = data[b\"data\"][i]\n            #fname = data[b\"filenames\"][i]\n            labels_test.append(data[b\"labels\"][i])\n\n            # consecutive 1024 entries store color channels of 32x32 image \n            img_R = img_flat[0:1024].reshape((32, 32))\n            img_G = img_flat[1024:2048].reshape((32, 32))\n            img_B = img_flat[2048:3072].reshape((32, 32))\n            \n            imgFormat = np.array([img_R, img_G, img_B])\n            imgFormat = np.transpose(imgFormat, (1, 2, 0))  #Change the shape 3,32,32 to 32,32,3 \n            dataImgSet_test.append(imgFormat)\n    \n    \n    dataImgSet_training = np.array(dataImgSet_training)\n    labels_training = np.array(labels_training)\n    dataImgSet_test = np.array(dataImgSet_test)\n    labels_test = np.array(labels_test)\n    \n    return dataImgSet_training, labels_training, dataImgSet_test, labels_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6967b92fea8c0e61371cb45b7fe16d9de3bc4bf"},"cell_type":"markdown","source":"Check the input directory."},{"metadata":{"_uuid":"a00832562e91b3a32e206927b14acdf162b835d7","trusted":false},"cell_type":"code","source":"! ls ../input\n! ls ../input/cifar-10-batches-py","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2ecbaa5948b84a4319be0f7883be9bf02b44977"},"cell_type":"markdown","source":"Run load data process"},{"metadata":{"_uuid":"2e5b59238d7d6a037e58b2a55e5e353a06ad0d31","trusted":false},"cell_type":"code","source":"X_train, y_train, X_test, y_test = getData()\n\nlabelNamesBytes = unpickle(\"../input/cifar-10-batches-py/batches.meta\")\nlabelNames = []\nfor name in labelNamesBytes[b'label_names']:\n    labelNames.append(name.decode('ascii'))\n\nlabelNames = np.array(labelNames)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"973fab5c15950857b51aa19764edfc90ba452e80"},"cell_type":"markdown","source":"Let's take a look of few image samples."},{"metadata":{"_uuid":"e4c4d818ad08ec121f9e0acd49833926ccff56e1","trusted":false},"cell_type":"code","source":"fig = plt.figure(figsize=(6,6))\nfor i in range(0, 9):\n    ax = fig.add_subplot(330 + 1 + i)\n    plt.imshow(Image.fromarray(X_test[i]))\n    ax.set_title(labelNames[y_test[i]])\n    ax.axis('off')\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12eba08359217703f63148fa336ca698803ae8a9"},"cell_type":"markdown","source":"### Scaling the Data \nWhen dealing with numeric features, we have specific attributes, which may be completely unbounded in nature, like view counts of a video or web page hits. Using the raw values as input features might make models biased toward features having high magnitude values. It is still recommended to normalize and scale down the features, in our case, the images has a maximum of 255 pixels, then it is sufficient divide the dataset by 255.0 to normalize the data."},{"metadata":{"_uuid":"365bcc1a28ee4953fd9d8fbc71c70842f6d918da","trusted":false},"cell_type":"code","source":"X_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train = X_train / 255.0\nX_test = X_test / 255.0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc47b9e2d387a3720ccf8ac3fca6d9df1498556e"},"cell_type":"markdown","source":"### Prepare the target variable\nRemember there are 10 classes to work with and hence we are dealing with a multi-class classification problem, then we need performs one hot encoding of the labels."},{"metadata":{"_uuid":"4756cd9c346507062692fe2ccc56aeb9e72338ea","trusted":false},"cell_type":"code","source":"y_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\nnum_classes = y_test.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea14084ff7f676d73b09b378b8562970329b0ae5"},"cell_type":"markdown","source":"\n### Data Augmentation\nKeras provides the [ImageDataGenerator](https://keras.io/preprocessing/image/) class that defines the configuration for image data preparation and augmentation. This includes capabilities such as:\n\n- Sample-wise standardization. \n- Feature-wise standardization.\n- ZCA whitening.\n- Random rotation, shifts, shear and flips.\n- Dimension reordering.\n- Save augmented images to disk.\n\nRather than performing the operations on your entire image dataset in memory, the API is designed to generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches). This reduces your memory overhead, but adds some additional time cost during model training.\n\nAfter you have created and configured your ImageDataGenerator, you must fit it on your data. This will calculate any statistics required to actually perform the transforms to your image data.\n\nIt is also possible to standardize pixel values across the entire dataset. This is called feature standardization and mirrors the type of standardization often performed for each column in a tabular dataset. You can perform feature standardization by setting the ***featurewise_center*** and ***featurewise_std_normalization*** parameters as ***True***. Since these is the default and we already  made our standarization manualy divide the pixels by 256.0 we set this parameters to0 ***False***.\n\nA whitening transform of an image is a linear algebra operation that reduces the redundancy in the matrix of pixel images. Less redundancy in the image is intended to better highlight the structures and features in the image to the learning algorithm. To do this, ZCA shows better results than PCA and results in transformed images that keeps all of the original dimensions and unlike PCA, resulting transformed images still look like their originals.\n\nSometimes images in your sample data may have varying and different rotations in the scene, then add a randomly rotating might be of help when learning from photographs or where the objects may have different orientations. See that we train your model to better handle rotations of images by artificially and 15 degrees randomly rotating images from our dataset during training.\n\nYou can make a same consideration respect of the position, since objects in your images may not be centered in the frame. They may be off-center in a variety of different ways. You can train your deep learning network to expect and currently handle off-center objects by artificially creating shifted versions of your training data. In our model we do that by width_shift_range and height_shift_range set as 0.1.\n\nAnother augmentation to your image data that can improve performance on large and complex problems is to create random flips of images in your training data. See that I prefer only make a horizontal flip. "},{"metadata":{"_uuid":"713c02d049f934f963b08770758ec46e8177f171","trusted":false},"cell_type":"code","source":"datagen = ImageDataGenerator(\n    featurewise_center=True,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=False\n    )\ndatagen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95d786ab33c16390f99d3bdffd14c673d188409b"},"cell_type":"markdown","source":"### Set Global Variables and Seed"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"num_classes = 10\ninput_shape = (32, 32, 3)\nkernel = (3, 3)\n\n# fix random seed for reproducibility \nseed = 101\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e179f6248b26f3e6e3d2c44ac269a51a0aacb044"},"cell_type":"markdown","source":"Now that we have training and test datasets normalized and the labels converted, the next step is to build the CNN model. \n\n##  Image Multiclassifier using CNNs from scratch\nOur task here is to build a  image multiclassifier using CNNs from scratch, that can identify the correct class label of a given image. \n\n### Create the CNN Model\nSince we have two dimensional images (the third dimension is the channel information), we will be using Conv2D layers. The initial few conv layers of the model kind of work toward feature extraction while the last couple of layers (fully connected) help in classifying the data."},{"metadata":{"_uuid":"475d3334038504a9865269a6fad508448b9d4126","trusted":false},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(64, kernel_size=kernel, activation='relu', input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(128, kernel_size=kernel, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n#model.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d517dd36d2c4c802113be8eff050f61442665f1b"},"cell_type":"markdown","source":"### Visualize the network architecture\nWe can actually visualize this network architecture using the following code snippet to understand the layers that have been used in this network, in a better way. "},{"metadata":{"_uuid":"33c1ba854cf01b907daa177774698d103a162892","trusted":false},"cell_type":"code","source":"SVG(model_to_dot(model, show_shapes=True, show_layer_names=True, rankdir='TB').create(prog='dot', format='svg'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f89f7acaed09542af4af7927506457bfb69eee5"},"cell_type":"markdown","source":"The next step involves compiling. We use categorical_crossentropy as our loss function since we are dealing with multiple classes. Besides this, we use the Adadelta optimizer and then train the classifier on the training data. "},{"metadata":{"_uuid":"c5c039473377c3a5a42221a9720f6c93619726b6","trusted":false},"cell_type":"code","source":"#training\nbatch_size = 50\nepochs = 75\nlrate = 0.1\nepsilon=1e-08\ndecay=1e-4\n#optimizer = keras.optimizers.rmsprop(lr=lrate,decay=1e-4)\noptimizer = keras.optimizers.Adadelta(lr=lrate ) #, epsilon=epsilon, decay=decay)\n#optimizer = keras.optimizers.Adam(lr=lrate, beta_1=0.9, beta_2=0.999, epsilon=epsilon, decay=decay)\n\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75d5e085688baf99d71d4dae27e1fc6c40be4795"},"cell_type":"markdown","source":"Instead of calling the fit() function on our model, we must call the fit_generator() function and pass in the data generator and the desired length of an epoch as well as the total number of epochs on which to train. We will fit the model by 70 epochs and save the results in a history variable to can be plot and see the evolution by each epoch."},{"metadata":{"_uuid":"9230a048062597b143737a32f719f042bfa525ba","scrolled":false,"trusted":false},"cell_type":"code","source":"history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n                    steps_per_epoch=X_train.shape[0] // batch_size, epochs=epochs, verbose=1,\n                    validation_data=(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60694532116c0aafbe803f5e571faf4641c3c0aa"},"cell_type":"markdown","source":"From the preceding output, you can see that this takes close to a one minute on a GPU, the performance decreases by 10 in CPU, anywhere between 200-600 seconds. Any way, you notice that it is too expensive process, and you need observe their history of gain to decide if continue or abort to change some parameters. \n\nLet's see the graph with the results through the epochs interactions."},{"metadata":{"_uuid":"19fcff1205376f7c568ed4b621b5b16eb90fb431","trusted":false},"cell_type":"code","source":"def plot_results(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epoch = range(epochs)\n\n    fig = plt.figure(figsize=(20,6))\n    ax1 = fig.add_subplot(121)\n    plt.plot(epoch, acc, 'b', label='Training acc')\n    plt.plot(epoch, val_acc, 'r', label='Validation acc')\n    ax1.set_title('Training and validation accuracy')\n    ax1.legend()\n\n    ax2 = fig.add_subplot(122) \n    plt.plot(epoch, loss, 'b', label='Training loss')\n    plt.plot(epoch, val_loss, 'r', label='Validation loss')\n    ax2.set_title('Training and validation loss')\n    ax2.legend()\n\n    plt.show()\n\nplot_results(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"752366f4f044e54bfb223525439c577669c7a111"},"cell_type":"markdown","source":"### Final evaluation of the model\nLet's see the performance of model against the test dataset."},{"metadata":{"_uuid":"2d5486c463fe88febfb53043efe669079899383d","trusted":false},"cell_type":"code","source":"scores = model.evaluate(X_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint(\"Test Accuracy: %.2f%%\" % (scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"baf26646712c475dbdc2401d63335438e0052e17","trusted":false},"cell_type":"code","source":"#Saving the model\nmodel.save('cifar10_1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6afbd1771f59bda32fb676825fcab98a74bbd3a8"},"cell_type":"markdown","source":"### Predict class of image in practice\nWe select a one record from test dataset, it shows an image looks like a automobile and the model correctly identifies the same as depicted in this snippet."},{"metadata":{"_uuid":"b4cefc895eb737dab94318da7a0a6de1bda7baf0","trusted":false},"cell_type":"code","source":"# How CNN Classifies an Image?\nimg_idx = 122\nplt.imshow(X_test[img_idx],aspect='auto')\nprint('Actual label:', labelNames[np.argmax(y_test[img_idx])])\n# Preper image to predict\ntest_image =np.expand_dims(X_test[img_idx], axis=0)\nprint('Input image shape:',test_image.shape)\nprint('Predict Label:',labelNames[model.predict_classes(test_image,batch_size=1)[0]])\nprint('\\nPredict Probability:\\n', model.predict_proba(test_image,batch_size=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b464d540ac8976885ee97689e75e28a1a455955"},"cell_type":"markdown","source":"### Model Interpretation\nAs we do in a regular classification or regression tasks, it is interesting try to evaluate how our model do the work. In the regular tasks, we use some methods like feature importance, coefficients, p-values, and so on. Thus, it would be interesting to see how the image data is manipulated by the conv-net we just created, but how? Luckily, keras provides hooks to extract information at intermediate steps in the model. They depict how various regions of the image activate the conv layers and how the corresponding feature representations and patterns are extracted.\n\nLet's see it in our model, use the test record predict before to extract and view the activation maps of the image based on what representations are learned and extracted by the conv layers using the get_activations(...) and display_activations(...) functions in the notebook. "},{"metadata":{"_uuid":"ca14a4fa48313a44a55aaa9174493a0388827d3c","trusted":false},"cell_type":"code","source":"# Utility Methods to understand CNN\n# https://github.com/fchollet/keras/issues/431\ndef get_activations(model, model_inputs, print_shape_only=True, layer_name=None):\n    import keras.backend as K\n    print('----- activations -----')\n    activations = []\n    inp = model.input\n\n    model_multi_inputs_cond = True\n    if not isinstance(inp, list):\n        # only one input! let's wrap it in a list.\n        inp = [inp]\n        model_multi_inputs_cond = False\n\n    outputs = [layer.output for layer in model.layers if\n               layer.name == layer_name or layer_name is None]  # all layer outputs\n\n    funcs = [K.function(inp + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n\n    if model_multi_inputs_cond:\n        list_inputs = []\n        list_inputs.extend(model_inputs)\n        list_inputs.append(1.)\n    else:\n        list_inputs = [model_inputs, 1.]\n\n    # Learning phase. 1 = Test mode (no dropout or batch normalization)\n    # layer_outputs = [func([model_inputs, 1.])[0] for func in funcs]\n    layer_outputs = [func(list_inputs)[0] for func in funcs]\n    for layer_activations in layer_outputs:\n        activations.append(layer_activations)\n        if print_shape_only:\n            print(layer_activations.shape)\n        else:\n            print(layer_activations)\n    return activations","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"828dc34217e5c4d313d72e42746a954aca73a468","trusted":false},"cell_type":"code","source":"# https://github.com/philipperemy/keras-visualize-activations/blob/master/read_activations.py\ndef display_activations(activation_maps):\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    batch_size = activation_maps[0].shape[0]\n    assert batch_size == 1, 'One image at a time to visualize.'\n    for i, activation_map in enumerate(activation_maps):\n        print('Displaying activation map {}'.format(i))\n        shape = activation_map.shape\n        if len(shape) == 4:\n            activations = np.hstack(np.transpose(activation_map[0], (2, 0, 1)))\n        elif len(shape) == 2:\n            # try to make it square as much as possible. we can skip some activations.\n            activations = activation_map[0]\n            num_activations = len(activations)\n            if num_activations > 1024:  # too hard to display it on the screen.\n                square_param = int(np.floor(np.sqrt(num_activations)))\n                activations = activations[0: square_param * square_param]\n                activations = np.reshape(activations, (square_param, square_param))\n            else:\n                activations = np.expand_dims(activations, axis=0)\n        else:\n            raise Exception('len(shape) = 3 has not been implemented.')\n        #plt.imshow(activations, interpolation='None', cmap='binary')\n        fig, ax = plt.subplots(figsize=(18, 12))\n        ax.imshow(activations, interpolation='None', cmap='binary')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c35e2a0f7cadd8149bb622f2e9ae8d93e32508e9","trusted":false},"cell_type":"code","source":"activations = get_activations(model, test_image)\ndisplay_activations(activations)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"994b656f6087688dcb0782dff534746d99525392"},"cell_type":"markdown","source":"As you can see, in this case, it is possible to obtain a good results with a simple CNN, but it need some time to find the most adequate CNN architecture and the process is too much expensive.  So, let us go to see how we can get better results by make improvements on a pre-trained more complex models.\n\n## CNN with Pre-Trained Models\n\nSince obtaining large labeled datasets and training highly complex and deep neural networks is a time consuming task  In practice, we utilize a concept, what is formally termed as transfer learning. This concept of transfer learning helps us leverage existing models for our tasks. The core idea is to leverage the learning, which the model learned from being trained over a large dataset and then transfer this learning by re-using the same model to extract feature representations from new images.  There are many  models have deep and complex architectures that have been fine-tuned and trained over diverse, large datasets. Hence, these models have been proven to have amazing performance on complex object recognition tasks.\n\nThere are several strategies of performing transfer learning, the mains are:\n- ***Pre-trained model as feature extractor***: Since the main objective of convolutions and pooling layers are extract features from the data-set, we ca use the pre-trained model is used to extract features for our dataset. We build a fully connected classifier on top of these features. In this case we only need to train the fully connected classifier, which does not take much time.\n- ***Fine-tuning pre-trained models***: It is possible to fine-tune an existing pre-trained model by fixing some of the layers and allowing others to learn/update weights apart from the fully connected layers. Usually it is observed that initial layers capture generic features while the deeper ones become more specific in terms of feature extraction. Thus, depending upon the requirements, we fix certain layers and fine-tune the rest.\n\nAs a rule of thumb, when we have a small training set and our problem is similar to the task for which the pre-trained models were trained, we can use transfer learning. If we have enough data, we can try and tweak the convolutional layers so that they learn more robust features relevant to our problem.\n\nNext we will utilize a pre-trained conv-network as a fine-tuning pre-trained models and build fully connected layer based classifier on top of it and train the model to help us build a classifier on CIFAR10 dataset. \n\nFor do this, we will use the VGG-19 model from the Visual Geometry Group of the Oxford University is one state-of-the-art convolutional neural network. This has been shown to perform extremely well on various benchmarks and competitions. VGG19 is a 19-layer conv-net trained on ImageNet dataset. ImageNet is visual database of hand-annotated images amounting to 10 million spanning across 9,000+ categories.\n\nSince we would be using VGG-19 for fine-tuning, we do not need the top (or fully connected) layers of this model. keras makes this as simple as setting the include_top parameter to False. The following snippet loads the VGG-19 model architecture consisting of the conv layers and leaves out the fully connected layers"},{"metadata":{"_uuid":"8a51bba44db1d4aabcbdca1c0991b5aa28fd9c42","trusted":false},"cell_type":"code","source":"vgg_model = applications.VGG19(include_top=False, weights='imagenet', input_shape=input_shape)\ndisplay(vgg_model.summary())\nbottleneck_path = r'../working/bottleneck_features_train_vgg19.npy'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90804a0936035e95c0f56ed97292e442fe14f560"},"cell_type":"markdown","source":"In this case, we only need to train the fully connected classifier and the last level of the VGG model, and then we need set to False all others trainable layers of VGG. Note, if you want make some tuning, you can choice some of VGG layer to be training too."},{"metadata":{"_uuid":"5d3a3cc829587d1ddb7d0420b4441bd11a0438c6","trusted":false},"cell_type":"code","source":"# Set to false the layers except the last set of conv laer and their pooling\nfor layer in vgg_model.layers[:-5]:\n    layer.trainable = False\n\n# Check the trainable status of the individual layers\nfor layer in vgg_model.layers:\n    print(layer, layer.trainable)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d8b2f190729a70a789cf3bf9c841f2115734b10"},"cell_type":"markdown","source":"### Create The Model by a Pre-Trained Model \nLet' create our model, include the VGG in the starting with and add the new fully connected layers and the classification."},{"metadata":{"_uuid":"d8eab237e585a08ff5867401ff22e5be058e3193","trusted":false},"cell_type":"code","source":"# fix random seed for reproducibility\nseed = 101\nnp.random.seed(seed)\n\n# Create the model\nclf_model = Sequential()\n \n# Add the vgg convolutional base model\n#clf_model.add(Flatten(input_shape=bottleneck_features_train.shape[1:]))\nclf_model.add(vgg_model)\n\n# Add new layers\nclf_model.add(Flatten())\nclf_model.add(Dense(1024, activation='relu'))\nclf_model.add(Dropout(0.5))\nclf_model.add(Dense(num_classes, activation='softmax'))\n\nSVG(model_to_dot(clf_model, show_shapes=True, show_layer_names=True, rankdir='TB').create(prog='dot', format='svg'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"783d8e1c65ac714a6cfcdc3c2d2db8cadf242233"},"cell_type":"markdown","source":"We will fit the model by 75 epochs and save the results in a history variable to can be plot and see the evolution by each epoch."},{"metadata":{"_uuid":"8bf8f61e29c0469fb70b6e5368f072fb5a139717","trusted":false},"cell_type":"code","source":"#training\nbatch_size = 50\nepochs = 75\nlrate = 0.1\nepsilon=1e-08\ndecay=1e-4\n#opt_rms = optimizers.rmsprop(lr=lrate,decay=1e-4)\noptimizer = keras.optimizers.Adadelta(lr=lrate ) # decay=decay) #, epsilon=epsilon, \n#optimizer = keras.optimizers.Adam(lr=lrate, beta_1=0.9, beta_2=0.999, epsilon=epsilon, decay=decay)\n\nclf_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\nhistory = clf_model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n                    steps_per_epoch=X_train.shape[0] // batch_size, epochs=epochs, verbose=1,\n                    validation_data=(X_test,y_test))\n#history = clf_model.fit(bottleneck_features_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a47ac51239ab3da5f14e68fd6a46df0c4b0a1b6"},"cell_type":"markdown","source":"From the preceding output, you can see that this takes close to 45 seconds on a GPU, an improvement of only 15 seconds, but to training a more complex model.\n\nLet's see the graph with the results through each epoch interactions."},{"metadata":{"_uuid":"9c4f48b88bcce63bba6ba1bfb31c115768030595","trusted":false},"cell_type":"code","source":"plot_results(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbee17acab5a4581215c7a0fe0bf68bcc3ffaff8"},"cell_type":"markdown","source":"### Final evaluation of the model\nLet's see the performance of model against the test dataset."},{"metadata":{"_uuid":"eaf43481345d8c32648878f8f4abec4331a7f0eb","trusted":false},"cell_type":"code","source":"scores = clf_model.evaluate(X_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint(\"Test Accuracy: %.2f%%\" % (scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8e6541a75d849862101f8ba39f29ea294c8af32"},"cell_type":"markdown","source":"### Predict class of image in CNN Pre-Trained Classifier\nWe select a one record from test dataset, it shows an image looks like a horse and the model correctly identifies the same as depicted in this snippet."},{"metadata":{"_uuid":"db792399ddf235e776c36d578f1693f4aea737c7","trusted":false},"cell_type":"code","source":"img_idx = 177\nplt.imshow(X_test[img_idx],aspect='auto')\nprint('Actual label:', labelNames[np.argmax(y_test[img_idx])])\n# Preper image to predict\ntest_image =np.expand_dims(X_test[img_idx], axis=0)\nprint('Input image shape:',test_image.shape)\nprint('Predict Label:',labelNames[clf_model.predict_classes(test_image,batch_size=1)[0]])\nprint('\\nPredict Probability:\\n', clf_model.predict_proba(test_image,batch_size=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c407ad9ac84bdeeb3a5ef97307609e52a0cddda"},"cell_type":"markdown","source":"Our model achieved good accuracy in the training data, but it shows some overfit, since it lose too much accuracy in the validation and test. Therefore, we need work on tuning parameter, change some VGG layer to learnable, change the fully connected layer. and/or increase the number of epochs.\n\nThe task of fine-tuning a network is to tweak the parameters of an already trained network so that it adapts to the new task at hand. In general, the initial layers learn very general features and as we go higher up the network, the layers tend to learn patterns more specific to the task it is being trained on. Thus, for fine-tuning, we want to keep the initial layers intact and retrain the later layers for our task.\n\n## Conclusion\n\nTherefore, we saw the power, advantages, and disadvantages of transfer learning. The main advantage is instead of spending time reinventing the wheel, with a few lines of code, we were able to leverage state of the art neural network for our classification task. In the other hand, you have more control on the scratch approach, and in the case of the transfer you continue need work on tuning."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"248.352px"},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}