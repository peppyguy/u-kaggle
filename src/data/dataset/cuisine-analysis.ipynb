{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined bty the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom __future__ import print_function\n\nimport os\nimport json\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport time, random, math\nimport re  #regular expression\nimport operator\nfrom collections import defaultdict, Counter, OrderedDict\nimport nltk #natural language processing\nimport six\nimport scipy.sparse as sp\nfrom operator import itemgetter\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#matplotlib.rc('figure', figsize = (14, 7))\nmatplotlib.rc('font', size = 14)\n# Remove grid lines\n# Set backgound color to white\nmatplotlib.rc('axes', facecolor = 'white')\n\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\nfrom sklearn.ensemble import RandomForestClassifier as RFC, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom sklearn.metrics import accuracy_score, log_loss\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d7050ee8d57c2b7c9bd22c3d21cb197ba963242"},"cell_type":"code","source":"def timer(func):\n    def wrapper(*args, **kwargs):\n        ts = time.time()\n        results = func(*args, **kwargs)\n        te = time.time()\n        print(\"Time to execute {} = {} seconds\".format(func.__name__, te - ts))\n        return results\n              \n    return wrapper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53720e7dc334ff84a7d0715dfaa4e84062484a9d"},"cell_type":"code","source":"#default configurations\nkwargs = dict(histtype='stepfilled', alpha=1.0, bins=15)\n\ndef createRandomColorCode():\n    r = np.random.uniform(0, 1)\n    g = np.random.uniform(0, 1)\n    b = np.random.uniform(0, 1)\n    return tuple((r, g, b))\n\ndef histogram(*args, **kwargs):\n    ax = args[0]\n    data = args[1]\n    title = args[2]\n    x_label = args[3] if len(args) > 3 else \"\"\n    y_label = args[4] if len(args) > 4 else \"\"\n    ax.hist(data, **kwargs)\n    ax.set_title(title)\n    \ndef barplot(ax, data, title, ticklabels, horizontal=False):\n    \"\"\"\n    Plots a horizontal or vertical barplot\n    Parameters\n    -------------\n    ax - the figure axes\n    \"\"\"\n    N = len(data)\n    ind = np.arange(N)\n    width = 0.75\n    colors = ['#33FFFF', '#79E5DB', '#A2A6DF', '#F2B8F0', '#DDEA53']\n    if horizontal:\n        rects = ax.barh(ind, data, width, color=colors)\n        ax.set_yticks(ind)\n        ax.set_yticklabels(ticklabels)\n        ax.set_title(title)\n        for i, v in enumerate(rects):\n            ax.text(data[i], i + 0.25, str(data[i]), weight='bold', color='b')\n    else:\n        rects = ax.bar(ind, data, width, color=colors)\n        ax.set_xticks(ind)\n        ax.set_xticklabels(ticklabels)\n        for tick in ax.get_xticklabels():\n            tick.set_rotation(90)\n            \n        for i, rect in enumerate(rects):\n            xloc = rect.get_x() + 0.02\n            yloc = rect.get_height()\n            ax.text(xloc, yloc, str(data[i]), weight='bold', color='b')\n            \ndef plotBarFromDicts(results, metrics, colors):\n    fig = plt.figure(figsize=(10, 3))\n    N = len(metrics)\n\n    for i, item in enumerate(metrics):\n        ax = fig.add_subplot(1, N, i + 1)\n        data = [item[metrics[i]] for item in results]\n        barplot(ax, data, metrics[i].capitalize(), [item['label'] for item in results])\n    \n    fig.tight_layout()\n    plt.show()\n    plt.gcf().clear()\n    \ndef plotNGramDistribution(cuisines, metrics):\n    # Create the plot object\n    fig, axarr = plt.subplots(len(cuisines), 3, sharex=False, sharey=True, figsize=(18,60), squeeze=False)\n    for i, cuisine in enumerate(cuisines):\n        for j, metric in enumerate(metrics):\n            data = gb.get_group(cuisine)[metric]\n            title = '{}-{}'.format(cuisine, metric.split('_')[0])\n            histogram(axarr[i][j], data, title, **kwargs)\n            \n    plt.tight_layout()\n    plt.show()\n    plt.gcf().clear()\n    \n@timer\ndef fit_transform(data):\n    train_vectors = vectorizer.fit_transform(data)\n    return train_vectors\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a5018d08701b526ceec07cefd726629cd729653"},"cell_type":"code","source":"def formatNum(num):\n    return float(\"{0:.2f}\".format(num))\n\n@timer\ndef plot_most_used_ingredient():\n    top_ingredients = []\n    labels = []\n    indices = []\n    counts = gb.apply(len)\n    \n    for i, name in enumerate(list(gb.groups)):\n        word_freq = create_vocabulary(gb.get_group(name)['ingredients'])\n        word_freq = sorted(word_freq.items(), key=lambda x: x[1])\n        \n        _ = word_freq[-1][1]/counts.loc[name]\n        _ = math.ceil(_*100)\n        top_ingredients.append(_)\n        labels.append(name + ' (' + word_freq[-1][0] + ') ')\n        indices.append(name)\n        \n    return pd.DataFrame({'label': labels, 'values': top_ingredients}, index=indices)\n\ndef find_variants(df, pattern):\n    pat = re.compile(r'\\w+\\s+' + pattern, re.IGNORECASE)\n    matches = [pat.findall(\",\".join(row)) for row in list(df['ingredients'])]\n    return list(set(np.hstack(matches)))\n\n@timer\ndef getVariants(df, ingredients):\n    _ = {}\n    for ingredient in ingredients:\n        _[ingredient] = find_variants(df, ingredient)\n        print('Processing {} completed'.format(ingredient))\n    return _\n\n@timer\ndef getIngredientVariants(pattern):\n    results = pd.Series(name='variants')\n    for i, name in enumerate(list(gb.groups)):\n        frame = gb.get_group(name)\n        results.loc[name] = len(find_variants(frame, pattern))\n    return results\n\ndef get_percentage_ingredient_in_cuisine(column):\n    data = train.groupby('cuisine').agg({\n        column: sum\n    })\n    data['total'] = train.groupby('cuisine').apply(len).values\n    data['%'] = data.apply(lambda x: float(\"{0:.2f}\".format(x['has_cheese'] / x['total'])), axis=1)\n    return data\n\ndef preprocess(doc):\n    doc = doc.lower()\n    doc = re.sub(r'yoghurt', 'yogurt', doc)\n    doc = re.sub(r\"\\bchees\\b\", \"cheese\", doc)\n    doc = re.sub(r\"ic\\s*cream\", \"ice cream\", doc)\n    doc = re.sub(r\"5 spice[s]?\", \"panch phoron\", doc)\n    doc = re.sub(r\"low sodium salt\", \"kosher salt\", doc)\n    return doc\n\ndef get_copy(df):\n    temp = df.copy()\n    temp['joined_ingredients'] = temp['ingredients'].apply(lambda row: \", \".join(row))\n    return temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84a15706a1463c069cb4a1edc1124e40c3222708"},"cell_type":"code","source":"class AyataVectorizer(object):\n    def __init__(self, max_df=1.0, min_df=1, max_features=None, ngram_range=(1, 1), lowercase=True, token_pattern=r\"(?u)\\b\\w[\\w\\s]+\\w\\b\", stop_words=None):\n        self.stop_words = stop_words\n        self.vocabulary = None\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.lowercase = lowercase\n        self.token_pattern = re.compile(token_pattern)\n        self.ngram_range = ngram_range\n        \n    def build_preprocessor(self):\n        return lambda doc: doc.lower()\n    \n    def build_tokenizer(self):\n        return lambda x: self.token_pattern.findall(x)\n    \n    def build_analyzer(self):\n        preprocess = self.build_preprocessor()\n        tokenize = self.build_tokenizer()\n        return lambda doc: tokenize(preprocess(doc))\n    \n    def word_ngrams(self, tokens):\n        if self.stop_words is not None:\n            tokens = [w for w in tokens if w not in self.stop_words]\n        \n        min_n, max_n = self.ngram_range\n        if max_n != 1:\n            if min_n == 1:\n                # copy all unigrams when max_n > 1 and min_n == 1\n                original_tokens = tokens[:]\n                min_n += 1\n            else:\n                # dont need unigrams\n                tokens = []\n                \n            n_original_tokens = len(original_tokens)\n            tokens_append = tokens.append\n            space_join = \" \".join\n\n            for n in range(min_n, min(max_n + 1, n_original_tokens + 1)):\n                for i in range(n_original_tokens - n + 1):\n                    tokens_append(space_join(original_tokens[i: i + n]))\n        return tokens\n        \n    def create_term_frequency_matrix(self, docs):\n        j_indices = []\n        values = []\n        # initialize to zero\n        indptr = [0]\n        \n        analyze = self.build_analyzer()\n        \n        vocab = defaultdict()\n        vocab.default_factory = vocab.__len__\n        for i, doc in enumerate(docs):\n            feature_counter = {}\n            for token in self.word_ngrams(analyze(doc)):\n                feature_idx = vocab[token]\n                if feature_idx not in feature_counter:\n                    feature_counter[feature_idx] = 1\n                else:\n                    feature_counter[feature_idx] += 1\n            \n            j_indices.extend(feature_counter.keys())\n            values.extend(feature_counter.values())\n            indptr.append(len(j_indices))\n        \n        j_indices = np.asarray(j_indices, dtype=np.int32)\n        indptr = np.asarray(indptr, dtype=np.int32)\n        values = np.asarray(values, dtype=np.int32)\n        \n        X = sp.csr_matrix((values, j_indices, indptr), shape=(len(indptr) - 1, len(vocab)), dtype=int)\n        X.sort_indices()\n        return dict(vocab), X\n    \n    def sort_features(self, X, vocabulary):\n        sorted_vocab = sorted(vocabulary.items(), key=lambda x: x[0]) #sorted(six.iteritems(vocabulary))\n        map_index = np.empty(len(sorted_vocab), dtype=np.int32)\n        for new_indx, (item, old_index) in enumerate(sorted_vocab):\n            vocabulary[item] = new_indx\n            map_index[old_index] = new_indx\n            \n        X.indices = map_index.take(X.indices)\n        return X\n    \n    def document_frequency(self, X):\n        return np.bincount(X.indices)\n    \n    def limit_features(self, X, vocabulary, max_df, min_df):\n        dfs = self.document_frequency(X)\n        tfs = np.asarray(X.sum(axis=0)).ravel()\n        mask = np.ones(len(dfs), dtype=bool)\n        mask &= dfs <= max_df\n        mask &= dfs >= min_df\n        \n        #feature selection\n        mask_inds = (-tfs[mask]).argsort()\n        if self.max_features is None:\n            mask_inds = mask_inds[:]\n        else:\n            mask_inds = mask_inds[:self.max_features]\n        \n        #map old to new indices\n        new_mask = np.zeros(len(dfs), dtype=bool)\n        new_mask[np.where(mask)[0][mask_inds]] = True\n        mask = new_mask\n        \n        new_indices = np.cumsum(mask) - 1\n        removed_terms = set()\n        for term, old_index in list(six.iteritems(vocabulary)):\n            if mask[old_index]:\n                vocabulary[term] = new_indices[old_index]\n            else:\n                del vocabulary[term]\n                removed_terms.add(term)\n                \n        kept_indices = np.where(mask)[0]\n        return X[:, kept_indices], removed_terms\n        \n    def fit(self, docs):\n        vocabulary, X = self.create_term_frequency_matrix(docs)\n        X = self.sort_features(X, vocabulary)\n        n_doc = X.shape[0]\n        min_doc_count = self.min_df\n        max_doc_count = self.max_df * n_doc\n        X, stop_words = self.limit_features(X, vocabulary, max_doc_count, min_doc_count)\n        self.stop_words = stop_words\n        self.vocabulary = vocabulary\n        return X\n    \n    def get_feature_names(self):\n        return [t for t, i in sorted(six.iteritems(self.vocabulary),key=itemgetter(1))]\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c92c9d278e39601774a0b041bd971f746d1f29e4"},"cell_type":"code","source":"NEGAT_STR = r'\\bno[\\s-][a-zA-Z-\\s]+'\nNEGAT_PATTERN = re.compile(NEGAT_STR, re.IGNORECASE)\n\nHYPHEN_STR = r'^[a-zA-Z-\\s]*[-][a-zA-Z-\\s]+\\b'\nHYPHEN_PAT = re.compile(HYPHEN_STR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c0c3a6bf10d8cb4a85d254650e1d0d7c3254639"},"cell_type":"code","source":"with open(\"../input/train.json\", 'r') as file:\n    data = json.load(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dec848264c003e4beef664e6e0af552afa5f67b"},"cell_type":"code","source":"df = pd.DataFrame(data)\ntrain = df.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9e905536cf5f2cbadaec32bad0ae86b70a05093"},"cell_type":"code","source":"gb = train.groupby('cuisine')\nprint('There are {} unique groups'.format(len(gb.groups)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7f6976875369af3eb6bb76be3b5746983b1808c"},"cell_type":"code","source":"train['joined_ingredients'] = train['ingredients'].apply(lambda row: \", \".join(row))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39be725e3b1f35d0a4fcfa0cd3a66e082f2838ec"},"cell_type":"code","source":"train['joined_ingredients'] = train['joined_ingredients'].apply(lambda x: preprocess(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"779bcf5d52d377dcd4c6b704d52f08143bc36bdb"},"cell_type":"code","source":"train['joined_ingredients'].str.extract(r'([\\w\\s]*!)').dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43fad82604a92e500d3671a3769ed0d65bf2069d"},"cell_type":"code","source":"def replaceNumber(doc):\n    \n    doc = re.sub(r'\\d{1,}', 'NUM', doc)\n    return doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f6ed609b2768bdc2e78ea22af0b9a4ebf42a825"},"cell_type":"code","source":"train['joined_ingredients'] = train.apply(lambda x: replaceNumber(x['joined_ingredients']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1e66e4c1f3eee4ee68c4804d2c5e6f20c3ea985"},"cell_type":"code","source":"train['joined_ingredients'].str.extract(r'(NUM)').dropna().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51d8304277c934b19a4066ea5963922000510b98"},"cell_type":"code","source":"train['joined_ingredients'] = train['joined_ingredients'].apply(lambda x: preprocess(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13147ff49a00e809f5d10cfe4acd5171354d2fd9"},"cell_type":"markdown","source":"### Hyphen separated ingredients ###"},{"metadata":{"trusted":true,"_uuid":"20575d3ac7481831249e553af98535f15661f3b3"},"cell_type":"code","source":"train['hyphen_ingrd'] = train.apply(lambda x: HYPHEN_PAT.findall(x['joined_ingredients']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be240e9ecc1670046647fdefed4a87e5431381ad"},"cell_type":"code","source":"hyphen_separated_arr = list(np.hstack(train['hyphen_ingrd']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18c16aa2d7b1d4e7a4956f7da7d58ec1983510b9"},"cell_type":"code","source":"_ = defaultdict()\n_.default_factory = _.__len__\n\nfor index, item in enumerate(hyphen_separated_arr):\n    _[item] += 1\n    \n_ = dict(_)\n_ = sorted(_.items(), key=lambda x: x[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebdce7984fefb3f8d7dd109ede6054b5165bf1e3"},"cell_type":"code","source":"d = np.asarray([i[1] for i in _])\nmask = np.ones(len(d), dtype=bool)\nmask_names = np.asarray([i[0] for i in _])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7a2cd78d6c1fc4107732b2f90aa40adfe137674"},"cell_type":"code","source":"#fig, ax = plt.subplots(figsize=(11, 30))\n#barplot(ax, d[mask], \"\", mask_names.take(np.where(mask)[0]), horizontal=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efce0ca789df890dc9b429b5221f99905558f9fd"},"cell_type":"code","source":"# train['negations'] = train['joined_ingredients'].apply(lambda x: NEGAT_PATTERN.findall(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44732db55abdfd27b9d813611d00bcd6c1b019eb"},"cell_type":"code","source":"corpus = train['joined_ingredients']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4d431dff9aac0a1892bab0e133c11b87984161e"},"cell_type":"code","source":"vectorizer = CountVectorizer(min_df=2)\ndata_train = fit_transform(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf602c856a319b280d19ed7eea58e9a7b8bc4b59"},"cell_type":"code","source":"my_vectorizer = AyataVectorizer(max_df=0.9)\nX = my_vectorizer.fit(corpus)\ntrain_vectorizer = pd.DataFrame(X.toarray(), columns=my_vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9219a4bbc5f2c550abe732b0491d18c4be4194cb"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data_train, train.cuisine, test_size=0.33, random_state=42)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cc333d286ac2304c6123a343956752940921e3f"},"cell_type":"code","source":"clf = RFC(random_state=42)\nclf = clf.fit(X_train, y_train)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac53f27795728fbc7522f226190a7afaae8c012f"},"cell_type":"code","source":"predictions = clf.predict(X_test)\npredict_proba = clf.predict_proba(X_test)\npredictions_df = pd.DataFrame({'actuals': y_test, 'predicteds': predictions})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c55c6b5b929beb0098081b043d45ed5afa9ef1da"},"cell_type":"code","source":"print('Accuracy: ', accuracy_score(predictions_df['actuals'], predictions_df['predicteds']))\nprint('Log loss: ', log_loss(predictions_df['actuals'], predict_proba))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5828b1a8039d54aaf9c1e154322e1fecc1addc21"},"cell_type":"markdown","source":"### TEST ###"},{"metadata":{"trusted":true,"_uuid":"d16e0009f7de6a3454cd95e16f536ce70d37cb9c"},"cell_type":"code","source":"# with open(\"../input/test.json\", 'r') as file:\n#     test = json.load(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bff0fa15670852e227af103ebff26ad79cc07615"},"cell_type":"code","source":"# test_data = pd.DataFrame(test)\n# test_data[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77eb33bd042a912c507f496e87763c1902a12850"},"cell_type":"code","source":"# test_data['joined_ingredients'] = test_data['ingredients'].apply(lambda x: \",\".join(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49f75ea8b908d694744a34ca9b0aa754cfc00122"},"cell_type":"code","source":"# feature_vectors = vectorizer.transform(test_data['joined_ingredients'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"154a25e8f787af678edbe770956bad31c25ee985"},"cell_type":"code","source":"# vect_df = pd.DataFrame(feature_vectors.toarray(), columns=[vectorizer.get_feature_names()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f78bbec5ee0116f29ee3c1b7ec9cc0b9fa3ffa50"},"cell_type":"code","source":"# bestModel = clf\n# ids = test_data['id']\n# predictions = bestModel.predict(vect_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2ab8d3afbfff39ba69aa05a41e6004d967f65d9"},"cell_type":"code","source":"# submission = pd.DataFrame({\"cuisine\": predictions, \"id\": ids})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"788edc18d018f4f5fee3fb799ddc2721e6a71699"},"cell_type":"code","source":"# submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecd2b00011e8fa3f87071c56ed72d62c2eaea19a"},"cell_type":"code","source":"# submission['cuisine'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20b1522c8bb5aebaad190829c4f7c896ae5e1235"},"cell_type":"code","source":"def createVocab(docs):\n    j_indices = []\n    values = []\n    indptr = [0]\n    \n    vocab = defaultdict()\n    vocab.default_factory = vocab.__len__\n    \n    for idx, items in enumerate(docs):\n        feature_counter = {}\n        for n in items:\n            feature_idx = vocab[n]\n            if feature_idx not in feature_counter:\n                feature_counter[feature_idx] = 1\n            else:\n                feature_counter[feature_idx] += 1\n            \n        j_indices.extend(feature_counter.keys())\n        values.extend(feature_counter.values())\n        indptr.append(len(j_indices))\n    \n    X = sp.csr_matrix((np.asarray(values, dtype=np.int32), np.asarray(j_indices, dtype=np.int32), indptr), shape=(len(indptr) - 1, len(vocab)), dtype=int)\n    X.sort_indices()\n    return X, dict(vocab)\n\ndef sort_features(X, vocabulary):\n    sorted_features = sorted(six.iteritems(vocabulary)) #sorted(occurrences.items(), key=lambda x: x[0])\n    map_index = np.empty(len(sorted_features), dtype=np.int32)\n    sorted_vocab = defaultdict()\n    \n    for new_val, (term, old_val) in enumerate(sorted_features):\n        vocabulary[term] = new_val\n        map_index[old_val] = new_val\n        \n    X.indices = map_index.take(X.indices, mode='clip')\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f0e4740c40cf7a0a7679134735473a01407aea7"},"cell_type":"code","source":"corpus1 = [\n    'This is the first document.',\n    'This document is the second document',\n    'And this is the third one.',\n    'Is this the first document?',\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81ebbf40cfcd05c527c1889220ac7dc13452277e"},"cell_type":"code","source":"corpus1 = [doc.split() for doc in corpus1]\ncorpus1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"720cea22218886ffcff0b6cec4e8f38fb2ffd178"},"cell_type":"code","source":"X_test, vocab_test = createVocab(corpus1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e0b887da694c9e379f60ac098ac1846fae90dc1"},"cell_type":"code","source":"X_test.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4bfdf29e27273545225ba89498ef672de32ad3f"},"cell_type":"code","source":"print(vocab_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50d38ddaab65be25ad83c7c5b4f4b03a327b2a4a"},"cell_type":"code","source":"X_test.indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a9aa7036e08934df7a4d064ef2eb7fb55b37356"},"cell_type":"code","source":"sorted_features = sorted(six.iteritems(vocab_test))\nmap_index = np.empty(len(sorted_features), dtype=np.int32)\nprint(sorted_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"259333c2993c63a213afa75d5eb89209cc9e9935"},"cell_type":"code","source":"for new_val, (term, old_val) in enumerate(sorted_features):\n    vocab_test[term] = new_val\n    map_index[old_val] = new_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52f00c845babc2dbeae92c182e735138c6e11acf"},"cell_type":"code","source":"print(vocab_test)\nprint()\nprint(map_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6a2501b875491aff23026d5342768fc805ed825"},"cell_type":"code","source":"X_test.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3e45aa066f5a7916e6808698ac1780c9488f6af"},"cell_type":"code","source":"X_test.indices = map_index.take(X_test.indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"292dd2a377adecb72f8c4269fd7a5b23e4030a5b"},"cell_type":"code","source":"X_test.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ee26e18d871077b40e45b57311b7f2414a2a0b2"},"cell_type":"code","source":"X_test.indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f899bb04bd5614ca725f63b3a3cd188b0cc55930"},"cell_type":"code","source":"sorted(X_test.indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43606601c3efa7f3b3c1487ce0f4fd4b8e50808a"},"cell_type":"code","source":"# Document frequency of each term index\ndfs = np.bincount(X_test.indices) # on sorted array as in vocabulary\ndfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0957b31caae9704f807df63ddbc9810cd41ae5f8"},"cell_type":"code","source":"print(sorted(vocab_test.items(), key=lambda x: x[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a342ec23ed2055cb0df99b8056ddbccddfd162d"},"cell_type":"code","source":"tfs = np.asarray(X_test.sum(axis=0)).ravel()\ntfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da14e774d8835badba8e7ef8030f3f797dc121f1"},"cell_type":"code","source":"# placeholder for old indices\nmask = np.ones(len(dfs), dtype=bool)\nmask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e4909ed03377cf6b4d47842b720dd5daaee6d35"},"cell_type":"code","source":"dfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d53ef0266234e3b4930cd27296c300dc68e133c4"},"cell_type":"code","source":"#Find terms occurring in at most n documents\nmask &= dfs <= 2\nprint(dfs)\nprint()\nprint(mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5773f806953edcce6cead864476d4abab72ec3f"},"cell_type":"code","source":"tfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ed3861893331be5cbed29d87c627547b71ac981"},"cell_type":"code","source":"-tfs[mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b8235521fb6a9d8682a90f97779c2f4cbc38af4"},"cell_type":"code","source":"mask_inds = (-tfs[mask]).argsort()\nmask_inds = mask_inds[:]\nmask_inds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50c09139983c9cfe35e2c20b4d4388b34d1b0ffa"},"cell_type":"code","source":"new_mask = np.zeros(len(dfs), dtype=bool)\nnew_mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10da482bcad5b151c7eb2b6cd2807d88e5934189"},"cell_type":"code","source":"mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d127d09bc9f5d44149547078e3724f98bcfb3c0"},"cell_type":"code","source":"np.where(mask)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b0aaeee4c0b8e966da6203bbf489428b1869f8b"},"cell_type":"code","source":"np.where(mask)[0][mask_inds]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"692d6da04bb56da562d12ae11caf90d51074a3f7"},"cell_type":"code","source":"# Finds index of kept features\nnew_mask[np.where(mask)[0][mask_inds]] = True\nnew_mask ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27ee419fe4608330a67427f524bef9ad12be258c"},"cell_type":"code","source":"mask = new_mask\nmask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07bfdc97e7455a2957cfa31fa208bd2cc0babc00"},"cell_type":"code","source":"np.cumsum(mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4e7eb5c1d0c0b6ca499601545ea294aaef4a239"},"cell_type":"code","source":"new_indices = np.cumsum(mask) - 1 # maps old indices to new\nnew_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24723a8fad18937667f2eff093903b1712eccde2"},"cell_type":"code","source":"print(vocab_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9210db5f69d0806009a9010b9346b66f7936084"},"cell_type":"code","source":"removed_terms = set()\nfor term, old_index in list(six.iteritems(vocab_test)):\n    if mask[old_index]:\n        vocab_test[term] = new_indices[old_index]\n    else:\n        del vocab_test[term]\n        removed_terms.add(term)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dad09a829804a1b95031e10ddffddc22419e672"},"cell_type":"code","source":"print(vocab_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ca71d6a59035e2047d79e352475f1444fddf798"},"cell_type":"code","source":"print(removed_terms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5f517d2100c7735651b383e6038a70f4503b371"},"cell_type":"code","source":"kept_indices = np.where(mask)[0]\nkept_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58357de5d27f58910a313b757ffe8e96233c995e"},"cell_type":"code","source":"X = X_test[:, kept_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e48fc924dabcdae755415e4b298e8efd7e8b9e8f"},"cell_type":"code","source":"print(sorted(six.iteritems(vocab_test), key=itemgetter(1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c4fa8a892e5cf35fb294674cfce488558f4af0d"},"cell_type":"code","source":"df = np.bincount(X.indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"762c5637071cfa6bf347c79a5c6422f8875845cd"},"cell_type":"code","source":"n_samples, n_features = X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72c403fc49793bd167af6f09505a5e4f6b6ef34e"},"cell_type":"code","source":"idf = np.log(n_samples / df) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b0a0c0f34595cb8a465e3b4ea90533d19778f73"},"cell_type":"code","source":"X.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3de93309a8b1e7d5cf3f2d30e2dcbb67076b0156"},"cell_type":"code","source":"sorted(X.indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b380fefcd42ff94b2b20362c51166aed91716a83"},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0175321a533c08ee54942d3ce7351399977584b"},"cell_type":"code","source":"idf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b155e065eaa01334bd559e98dfb34bca168a924"},"cell_type":"code","source":"_idf_diag = sp.diags(idf, offsets=0,shape=(n_features, n_features), format='csr', dtype=np.float64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ddc1c4d21716d5906b8d27c17dd22fedbbfbeb4"},"cell_type":"code","source":"pd.DataFrame(X.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f350422b2708f1ecc451a809e41a1bc3255dae0"},"cell_type":"code","source":"pd.DataFrame(_idf_diag.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"913b0ee1c45debf595457db6492cbddd289d281a"},"cell_type":"code","source":"pd.DataFrame(X * _idf_diag.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7bd7e9c99639965112debbf4887257184c69e53"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}