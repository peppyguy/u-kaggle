{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Import all the required libraries\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom collections import Counter\n\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report\n\n# Tutorial about Python regular expressions: https://pymotw.com/2/re/\nimport re\nimport string\nimport nltk.corpus\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5c4322ac4b04ddde4b5b012617102105a2eefa1"},"cell_type":"code","source":"# using the SQLite Table to read data. (KAGGLE)\nimport sqlite3\nshow_tables = \"select tbl_name from sqlite_master where type = 'table'\" \nconn = sqlite3.connect('../input/database.sqlite') \npd.read_sql(show_tables,conn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d5ba79f6f921131d8845bcdf3986e1d4d24f576"},"cell_type":"code","source":"#filtering only positive and negative reviews i.e. not taking into consideration those reviews with Score=3\nfiltered_data = pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score != 3\"\"\", conn) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13e129aad77809100a8d95ca091858336ff74e97"},"cell_type":"code","source":"filtered_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aae069f885634d85fa1c7267d2bdd30cfc5862f8"},"cell_type":"code","source":"# Give reviews with Score>3 a positive rating, and reviews with a score<3 a negative rating.\ndef partition(x):\n    if x < 3:\n        return 'Negative'\n    return 'Positive'\n\n#changing reviews with score less than 3 to be positive and vice-versa\nactualScore = filtered_data['Score']\n\npositiveNegative = actualScore.map(partition) \n\nfiltered_data['Polarity'] = positiveNegative\n\nfiltered_data['Class_Label']= filtered_data['Polarity'].apply(lambda x : 1 if x == 'Positive' else 0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db49a2dcc0f76ad9d9d10374957ec7b3f5f649ff"},"cell_type":"code","source":"filtered_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2eff907e5a9d5aa2747e4bfd74cc7ad4532d69e9"},"cell_type":"code","source":"# Data Cleaning: Deduplication, clearing records whereHelpfulnessNumerator is greater than HelpfulnessDenominator \nsorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\nfinal=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n\ndisplay= pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3 AND Id=44737 OR Id=64422\nORDER BY ProductID\n\"\"\", conn)\nfinal=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n\n#How many positive and negative reviews are present in our dataset?\nfinal['Class_Label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bc90b795cff9eede3d97c0c92ec7f94de2757c8"},"cell_type":"code","source":"## Text Preprocessing: Stemming, stop-word removal and Lemmatization.\n\n# find sentences containing HTML tags (DATASET FOR BRUTEFORCE)\nimport re\ni=0;\nfor sent in final['Text'].values:\n    if (len(re.findall('<.*?>', sent))):\n        print(i)\n        print(sent)\n        break;\n    i += 1;\n\n# find sentences containing HTML tags (DATASET FOR KD_TREE)\nimport re\ni=0;\nfor sent in final['Text'].values:\n    if (len(re.findall('<.*?>', sent))):\n        print(i)\n        print(sent)\n        break;\n    i += 1;\n\n# Remove Stop-Words\n\nstop = set(stopwords.words('english')) #set of stopwords\nsno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n\ndef cleanhtml(sentence): #function to clean the word of any html-tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\ndef cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n    return  cleaned\nprint(stop)\nprint('************************************')\nprint(sno.stem('tasty'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"157284a96ce8632d819b0a943b017aae99a480fe"},"cell_type":"code","source":"#pre-processing: agegate all positive and negative words\ni=0\nstr1=' '\nfinal_string=[]\nall_positive_words=[] # store words from +ve reviews here\nall_negative_words=[] # store words from -ve reviews here.\ns=''\nfor sent in final['Text'].values:\n    filtered_sentence=[]\n    #print(sent);\n    sent=cleanhtml(sent) # remove HTMl tags\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n                if(cleaned_words.lower() not in stop):\n                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if (final['Polarity'].values)[i] == 'Positive': \n                        all_positive_words.append(s) #list of all words used to describe positive reviews\n                    if(final['Polarity'].values)[i] == 'Negative':\n                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n                else:\n                    continue\n            else:\n                continue \n    #print(filtered_sentence)\n    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n    #print(\"***********************************************************************\")\n    \n    final_string.append(str1)\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86d3525b5da3bc6a075f4240ba8567c3ea7fcfbf"},"cell_type":"code","source":"final['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review \nfinal['CleanedText']=final['CleanedText'].str.decode(\"utf-8\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddae6f066f98e6cbeb8b38bb6a373cbc44a40710"},"cell_type":"code","source":"#  GET THE TRAINING AND TEST DATA-SET \n\n#  data preprocessing\n\n# define column names\nnames = ['Time', 'Text','CleanedText', 'Polarity']\n\n\n# create design matrix X and target vector y\nX_NB =  final[names]\ny_NB = final['Class_Label']\n\nX_train_NB, X_test_NB, y_train_NB, y_test_NB = model_selection.train_test_split(X_NB, y_NB, test_size=0.2, random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2397c1d703654bcaf06c0bbaf4d0b93fecd1f5b"},"cell_type":"markdown","source":"# ASSIGNMENT- PART 1:  NB  CLASSIFIER  ON BOW  VECTOR"},{"metadata":{"_uuid":"45b215c33cb25c2f81f487b4bd2b68dbb72b7785"},"cell_type":"markdown","source":"# STEP 1) Computing the Bag of Words (BoW)"},{"metadata":{"trusted":true,"_uuid":"00d3f69d5e4862123af19bc08dabb7f340d77a56"},"cell_type":"code","source":"# Get the BoW matrix\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ncount_vect = CountVectorizer() \n\nbow_NB = count_vect.fit(X_train_NB['CleanedText'].values)\n\nbow_train_NB = bow_NB.transform(X_train_NB['CleanedText'].values)\n\nbow_test_NB = bow_NB.transform(X_test_NB['CleanedText'].values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1273099af646354906feed16a148481e7a91881"},"cell_type":"code","source":"# Colum Standardization of the Bag of Words vector\n\nfrom sklearn.preprocessing import StandardScaler\nscalar = StandardScaler(with_mean=False)\nscalar.fit(bow_train_NB)\nbow_train_NB_vectors = scalar.transform(bow_train_NB)\nbow_test_NB_vectors = scalar.transform(bow_test_NB)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c3940c02ad6b6791c849b1c2db4d35f734c34eb"},"cell_type":"code","source":"bow_train_NB_vectors.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88527b32143a0cb087978ab5a0410f56b00083d3"},"cell_type":"code","source":"y_train_NB.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af7be6dace2c0d615701eb07c78b89b0c72dfb00"},"cell_type":"markdown","source":"# Step 2) Naive Bayes Classifier for BOW"},{"metadata":{"trusted":true,"_uuid":"75014ce0a2546e7a7370d448e94cb14b797b0663"},"cell_type":"code","source":"test=1e-5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"144a667da8895eecfdc0f65b08552d24460c196c","scrolled":true},"cell_type":"code","source":"# 10FOLD CV  to get the best Alpha (Hyper-Parameter)  for BOW model\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#parameters = {\"alpha\":  np.array([1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9,0,1])}\n\nparameters = {\"alpha\":  np.array( [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10] )}\n\nn_folds = 10\n\ncv_timeSeries = TimeSeriesSplit(n_splits=n_folds)\n    \nmodel = MultinomialNB()\n\nmy_cv = TimeSeriesSplit(n_splits=n_folds).split(bow_train_NB_vectors)\n    \ngsearch_cv = GridSearchCV(estimator=model, param_grid=parameters, cv=my_cv)\n\ngsearch_cv.fit(bow_train_NB_vectors, y_train_NB)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2490f2730930f98d2c1ece726154cdf20abaef03"},"cell_type":"code","source":"# Display the details for the  Hyper-parametrized BOW model\n\nNB_OPTIMAL_classifier_for_BOW = gsearch_cv.best_estimator_\nprint(\"Best estimator for {} model : \".format(\"BOW\"), NB_OPTIMAL_classifier_for_BOW)\n\nNB_OPTIMAL_score_for_BOW = gsearch_cv.best_score_\nprint(\"Best Score for {} model : \".format(\"BOW\"), NB_OPTIMAL_score_for_BOW)\n\nOPTIMAL_MODEL_for_BOW= gsearch_cv.best_params_\nfor alpha in OPTIMAL_MODEL_for_BOW:\n    print(\"Optimal Alpha for {} model : \".format(\"BOW\"),'{:f}'.format(OPTIMAL_MODEL_for_BÃ“W[alpha]))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df70a4725a9f4675202d3cfdc5e3631c3e5590cb"},"cell_type":"code","source":"# Display Performance of the  Hyper-parametrized BOW model on TEST data\n\nnb_classifier = NB_OPTIMAL_classifier_for_BOW\n\ny_pred = nb_classifier.predict(bow_test_NB_vectors)\n    \n#Evaluate the model accuracy on TEST data\n\ntest_accuracy = accuracy_score(y_test_NB, y_pred, normalize=True) * 100\npoints = accuracy_score(y_test_NB, y_pred, normalize=False)\n\n# Display the classification report\nprint(classification_report(y_test_NB, y_pred,digits=4))\n\n#Display the model accuracy on TEST data\nprint('\\nThe number of accurate predictions out of {} data points on TEST data is {}'.format(bow_test_NB_vectors.shape[0], points))\nprint('Accuracy of the {} model on TEST data is {} %'.format(\"BOW\", '{:f}'.format(np.round(test_accuracy,2))))\n     \n# Display the confusion matrix\nimport scikitplot.metrics as sciplot\nsciplot.plot_confusion_matrix(y_test_NB, y_pred)\n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c40bc050a43714e54ac987f69c5d40059b9f908b"},"cell_type":"code","source":"\n    # '''Get top 50 features displayed from both the negative and the positive review classes.'''\n    # Reference URL: https://stackoverflow.com/questions/50526898/how-to-get-feature-importance-in-naive-bayes#50530697\n    \n    neg_class_prob_sorted = (-NB_OPTIMAL_classifier_for_BOW.feature_log_prob_[0, :]).argsort()               #Note : Putting a - sign indicates the indexes will be sorted in descending order.\n    pos_class_prob_sorted = (-NB_OPTIMAL_classifier_for_BOW.feature_log_prob_[1, :]).argsort()\n    \n    neg_class_features = np.take(bow_NB.get_feature_names(), neg_class_prob_sorted[:50])\n    pos_class_features = np.take(bow_NB.get_feature_names(), pos_class_prob_sorted[:50])\n    \n    print(\"The top 50 most frequent words from the positive class are :\\n\")\n    print(pos_class_features)\n    \n    print(\"\\nThe top 50 most frequent words from the negative class are :\\n\")\n    print(neg_class_features)\n    \n    del(neg_class_prob_sorted, pos_class_prob_sorted, neg_class_features, pos_class_features)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8426f773de917effba808c58b2f4328772b4de52"},"cell_type":"markdown","source":"# ASSIGNMENT- PART 2:  Naive-Bayes on TFIDF vector"},{"metadata":{"trusted":true,"_uuid":"8f599c3ce117ada43d1e47408366714301a2ca74"},"cell_type":"code","source":"# getting the base TFIDF vector\n\ntf_idf_vect_NB = TfidfVectorizer(ngram_range=(1,1))\n\ntfidf_NB = tf_idf_vect_NB.fit(X_train_NB['CleanedText'].values)\n\ntfidf_train_NB = tfidf_NB.transform(X_train_NB['CleanedText'].values)\n\ntfidf_test_NB  = tfidf_NB.transform(X_test_NB['CleanedText'].values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14ffedbc83d1185589468a551e012ca3aa1905c5"},"cell_type":"code","source":"#Colum Standardization of the TFIDF vector \n\nfrom sklearn.preprocessing import StandardScaler\nscalar = StandardScaler(with_mean=False)\nscalar.fit(tfidf_train_NB)\nTFIDF_train_NB_vectors = scalar.transform(tfidf_train_NB)\nTFIDF_test_NB_vectors = scalar.transform(tfidf_test_NB)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7e3c081cf8c2422797a0857495db3ba3e66ac06"},"cell_type":"code","source":"# 10-Fold Cross Validation to find the best Alpha for TFIDF model \n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.naive_bayes  import MultinomialNB\n\nparameters = {\"alpha\":  np.array( [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10] )}\n\nn_folds = 10\n\ncv_timeSeries = TimeSeriesSplit(n_splits=n_folds)\n    \nmodel = MultinomialNB()\n\nmy_cv = TimeSeriesSplit(n_splits=n_folds).split(TFIDF_train_NB_vectors)\n    \ngsearch_cv_TFIDF = GridSearchCV(estimator=model, param_grid=parameters, cv=my_cv, scoring='f1')\n    \ngsearch_cv_TFIDF.fit(TFIDF_train_NB_vectors, y_train_NB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dabc832ed4a843c96b4c039325face1c31be3b3"},"cell_type":"code","source":"# Display  the details of the hyper-parametrized NB classifer (TFIDF)\n\nNB_OPTIMAL_classifier_for_TFIDF = gsearch_cv_TFIDF.best_estimator_\nprint(\"Best estimator for {} model : \".format(\"TFIDF\"), NB_OPTIMAL_classifier_for_TFIDF)\n\nNB_OPTIMAL_score_for_TFIDF = gsearch_cv_TFIDF.best_score_\nprint(\"Best Score for {} model : \".format(\"TFIDF\"), NB_OPTIMAL_score_for_TFIDF)\n\nOPTIMAL_MODEL_for_TFIDF= gsearch_cv_TFIDF.best_params_\nfor alpha in OPTIMAL_MODEL_for_TFIDF:\n    print(\"Optimal Alpha for {} model : \".format(\"TFIDF\"), '{:f}'.format(OPTIMAL_MODEL_for_TFIDF[alpha]))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"331f19450fc9f954abd90be30e9a5846dd60faf9"},"cell_type":"code","source":"\n# Display  the performance of  TFIDF model on TEST data\n\n#Predict the labels for the test set\ny_pred_TFIDF = NB_OPTIMAL_classifier_for_TFIDF.predict(TFIDF_test_NB_vectors)\n    \n#Evaluate the accuracy of the model on TEST data\ntest_accuracy_TFIDF = accuracy_score(y_test_NB, y_pred_TFIDF, normalize=True) * 100\npoints_TFIDF = accuracy_score(y_test_NB, y_pred_TFIDF, normalize=False)\n\n#Display the classification_report\nprint(classification_report(y_test_NB, y_pred_TFIDF,digits=4))\n\n#Display the  accuracy of the model on TEST data\nprint('\\nThe number of accurate predictions out of {} data points on unseen data is {}'.format(TFIDF_test_NB_vectors.shape[0], points_TFIDF))\nprint('Accuracy of the {} model on unseen data is {} %'.format(\"TFIDF\", np.round(test_accuracy_TFIDF,2)))\n\n#Display the  confusion matrix\nimport scikitplot.metrics as sciplot\nsciplot.plot_confusion_matrix(y_test_NB, y_pred_TFIDF)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd006ba1561a1b9380a45cb7a4620d656d28491a"},"cell_type":"code","source":"\n # '''Get top 50 features displayed from both the negative and the positive review classes for the TF-IDF \n    # Reference URL: https://stackoverflow.com/questions/50526898/how-to-get-feature-importance-in-naive-bayes#50530697\n    \n    neg_class_prob_sorted_TFIDF = (-NB_OPTIMAL_classifier_for_TFIDF.feature_log_prob_[0, :]).argsort()              \n    pos_class_prob_sorted_TFIDF = (-NB_OPTIMAL_classifier_for_TFIDF.feature_log_prob_[1, :]).argsort()\n    \n    neg_class_features_TFIDF = np.take(tfidf_NB.get_feature_names(), neg_class_prob_sorted_TFIDF[:50])\n    pos_class_features_TFIDF = np.take(tfidf_NB.get_feature_names(), pos_class_prob_sorted_TFIDF[:50])\n    \n    print(\"The top 50 most frequent words from the positive class are :\\n\")\n    print(pos_class_features_TFIDF)\n    \n    print(\"\\nThe top 50 most frequent words from the negative class are :\\n\")\n    print(neg_class_features_TFIDF)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81353bb8a931f8cbe2854dafe69452ce3fe74673"},"cell_type":"code","source":"# Clearing the memory space for faster processing\ndel(neg_class_prob_sorted_TFIDF, pos_class_prob_sorted_TFIDF, neg_class_features_TFIDF, pos_class_features_TFIDF)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66a6ab933f65f4644de99bf0a8ab05e8faec309c"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"e137600b9a90a7172589ee56fa109ee3534c9b3f"},"cell_type":"markdown","source":"# ASSIGNMENT- PART 3:  Naive-Bayes on BI-GRAMS vector"},{"metadata":{"_uuid":"25ed506551aef7fab136b61399d9a70d0f612718"},"cell_type":"markdown","source":"# computing the  BI-GRAMS matrix"},{"metadata":{"trusted":true,"_uuid":"ce034694691d4e75b50c048926293a06c5e776e2"},"cell_type":"code","source":"# BI Grams matrix\n\nTFIDF_vect_BIGRAMS = TfidfVectorizer(ngram_range=(1,2) )  # here we are taking BIGRAMS only \n\nBIGRAMS_NB = TFIDF_vect_BIGRAMS.fit(X_train_NB['CleanedText'].values)\n\nBIGRAMS_train_NB = BIGRAMS_NB.transform(X_train_NB['CleanedText'].values)\n\nBIGRAMS_test_NB = BIGRAMS_NB.transform(X_test_NB['CleanedText'].values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c18fd218ba93654f8339db9d5dd6db81a7d40aea"},"cell_type":"code","source":"# Colum Standardization of the Bigrams vector\n\nfrom sklearn.preprocessing import StandardScaler\nscalar = StandardScaler(with_mean=False)\nscalar.fit(BIGRAMS_train_NB)\nBIGRAMS_train_NB_vectors = scalar.transform(BIGRAMS_train_NB)\nBIGRAMS_test_NB_vectors = scalar.transform(BIGRAMS_test_NB)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c95bf80adee3c619c3f6e59055a6c6ab2b90a1b"},"cell_type":"markdown","source":" ##  Running the NB Classifier on BIGRAMS data"},{"metadata":{"trusted":true,"_uuid":"0c961243f69b6432cab70541c07ef457824af192"},"cell_type":"code","source":"# 10 fold CV to get the Optimal Alpha for BIGRAMS model \n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.naive_bayes  import MultinomialNB\n\nparameters = {\"alpha\": np.array([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10])}\n\nn_folds = 10\n\ncv_timeSeries = TimeSeriesSplit(n_splits=n_folds)\n    \nmodel = MultinomialNB()\n\nmy_cv = TimeSeriesSplit(n_splits=n_folds).split(BIGRAMS_train_NB_vectors)\n    \ngsearch_cv_BIGRAMS = GridSearchCV(estimator=model, param_grid=parameters, cv=my_cv, scoring='f1')\n    \ngsearch_cv_BIGRAMS.fit(BIGRAMS_train_NB_vectors, y_train_NB)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4e587dc9cd99981121c813b597bf352acb0889e"},"cell_type":"code","source":"# Display  the Hyper-parametrized BIGRAMS model details\n\nNB_OPTIMAL_classifier_for_BIGRAMS = gsearch_cv_BIGRAMS.best_estimator_\nprint(\"Best estimator for {} model : \".format(\"BIGRAMS\"), NB_OPTIMAL_classifier_for_BIGRAMS)\n\nNB_OPTIMAL_score_for_BIGRAMS = gsearch_cv_BIGRAMS.best_score_\nprint(\"Best Score for {} model : \".format(\"BIGRAMS\"), NB_OPTIMAL_score_for_BIGRAMS)\n\nOPTIMAL_MODEL_for_BIGRAMS= gsearch_cv_BIGRAMS.best_params_\nfor alpha in OPTIMAL_MODEL_for_BIGRAMS:\n    print(\"Optimal Alpha for {} model : \".format(\"BIGRAMS\"), '{:f}'.format(OPTIMAL_MODEL_for_BIGRAMS[alpha]))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fca73378ab38920fbe25afa9824e309523f2c98d"},"cell_type":"code","source":"# Display  the performance of  BIGRAMS model on TEST data\n\n#Predict the labels of TEST data\n\ny_pred = NB_OPTIMAL_classifier_for_BIGRAMS.predict(BIGRAMS_test_NB_vectors)\n    \n#Get the accuracy of the model on TEST data\ntest_accuracy_BIGRAMS = accuracy_score(y_test_NB, y_pred, normalize=True) * 100\npoints_BIGRAMS = accuracy_score(y_test_NB, y_pred, normalize=False)\n\n# Display the classification_report\nprint(classification_report(y_test_NB, y_pred,digits=4))\n\n#Display the model accuracy of the model on TEST data\nprint('\\nThe number of accurate predictions out of {} data points on unseen data is {}'.format(BIGRAMS_test_NB_vectors.shape[0], points_BIGRAMS))\nprint('Accuracy of the {} model on unseen data is {} %'.format(\"BIGRAMS\", np.round(test_accuracy_BIGRAMS,2)))\n\n#Display the confusion matrix\nimport scikitplot.metrics as sciplot\nsciplot.plot_confusion_matrix(y_test_NB, y_pred)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbd1d63fbd525f0282fcc2b38712d9a8a3905b55"},"cell_type":"code","source":"\n    # '''Get top 50 features displayed from both the negative and the positive review classes for the BIGRAMS \n    # Reference URL: https://stackoverflow.com/questions/50526898/how-to-get-feature-importance-in-naive-bayes#50530697\n    \n    neg_class_prob_sorted_BIGRAMS = (-NB_OPTIMAL_classifier_for_BIGRAMS.feature_log_prob_[0, :]).argsort()               #Note : Putting a - sign indicates the indexes will be sorted in descending order.\n    pos_class_prob_sorted_BIGRAMS = (-NB_OPTIMAL_classifier_for_BIGRAMS.feature_log_prob_[1, :]).argsort()\n    \n    neg_class_features_BIGRAMS = np.take(BIGRAMS_NB.get_feature_names(), neg_class_prob_sorted_BIGRAMS[:50])\n    pos_class_features_BIGRAMS = np.take(BIGRAMS_NB.get_feature_names(), pos_class_prob_sorted_BIGRAMS[:50])\n    \n    print(\"The top 50 most frequent words from the positive class are :\\n\")\n    print(pos_class_features_BIGRAMS)\n    \n    print(\"\\nThe top 50 most frequent words from the negative class are :\\n\")\n    print(neg_class_features_BIGRAMS)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d0b99fc950ba30bbeca9e7522710ee7a0cad0da"},"cell_type":"code","source":"# Clearing the memory space for faster processing\ndel(neg_class_prob_sorted_BIGRAMS, pos_class_prob_sorted_BIGRAMS, neg_class_features_BIGRAMS, pos_class_features_BIGRAMS)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cb68c071564131f1bb1763295f77b2981b7b8be"},"cell_type":"markdown","source":"1. # ASSIGNMENT- PART 4:  Naive-Bayes on TRI-GRAMS vector"},{"metadata":{"_uuid":"ccd30c947ce0d65666f7264b5e20d8554478144a"},"cell_type":"markdown","source":"# computing the  TRI-GRAMS matrix"},{"metadata":{"trusted":true,"_uuid":"6e913bfd0f9e3d1c6e7c57556e9246a447cf0185"},"cell_type":"code","source":"# TRI Grams matrix\n\nTFIDF_vect_TRIGRAMS = TfidfVectorizer(ngram_range=(2,3) )  # here we r taking only TRI-RAMS\n\nTFIDF_NB_TRIRAMS = TFIDF_vect_TRIGRAMS.fit(X_train_NB['CleanedText'].values)\n\nTRIGRAMS_train_NB = TFIDF_NB_TRIRAMS.transform(X_train_NB['CleanedText'].values)\n\nTRIGRAMS_test_NB = TFIDF_NB_TRIRAMS.transform(X_test_NB['CleanedText'].values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39396dac5afcc15b3bd8804bd33724c97564620a"},"cell_type":"code","source":"# #Colum Standardization of the TRigrams vector created using cleaned data\n\nfrom sklearn.preprocessing import StandardScaler\nscalar = StandardScaler(with_mean=False)\nscalar.fit(TRIGRAMS_train_NB)\nTRIGRAMS_train_NB_vectors = scalar.transform(TRIGRAMS_train_NB)\nTRIGRAMS_test_NB_vectors = scalar.transform(TRIGRAMS_test_NB)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c88ded14fe3ca8b863943a04086e34febd655dd9"},"cell_type":"markdown","source":" ##  Running the NB Classifier on TRIGRAMS data"},{"metadata":{"trusted":true,"_uuid":"cabf64a19a0cecd037b4e571b1a73ed667e669f8"},"cell_type":"code","source":"# Running 10 fold CV to get the Hyper-Parameter Alpha for TRIGRAMS model\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.naive_bayes  import MultinomialNB\n\nparameters = {\"alpha\": np.array([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10])}\n\nn_folds = 10\n\ncv_timeSeries = TimeSeriesSplit(n_splits=n_folds)\n    \nmodel = MultinomialNB()\n\nmy_cv = TimeSeriesSplit(n_splits=n_folds).split(TRIGRAMS_train_NB_vectors)\n    \ngsearch_cv_TRIGRAMS = GridSearchCV(estimator=model, param_grid=parameters, cv=my_cv, scoring='f1')\n    \ngsearch_cv_TRIGRAMS.fit(TRIGRAMS_train_NB_vectors, y_train_NB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d73341007128001d046fc256e05553e173daa252"},"cell_type":"code","source":"# Display the details of the Hyper-parametrized (alpha) TRIGRAMS model\n\nNB_OPTIMAL_classifier_for_TRIGRAMS = gsearch_cv_TRIGRAMS.best_estimator_\nprint(\"Best estimator for {} model : \".format(\"TRIGRAMS\"), NB_OPTIMAL_classifier_for_TRIGRAMS)\n\nNB_OPTIMAL_score_for_TRIGRAMS = gsearch_cv_TRIGRAMS.best_score_\nprint(\"Best Score for {} model : \".format(\"TRIGRAMS\"), NB_OPTIMAL_score_for_TRIGRAMS)\n\nOPTIMALMODEL= gsearch_cv_TRIGRAMS.best_params_\nfor alpha in OPTIMALMODEL:\n    print(\"Optimal Alpha for {} model : \".format(\"TRIGRAMS\"), '{:f}'.format(OPTIMALMODEL[alpha]))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f303e694edf190cd2a6794554b736f4871348155"},"cell_type":"code","source":"#OPTIMALMODEL_TRIGRAMS= gsearch_cv_TRIGRAMS.best_params_\n#for alpha in OPTIMALMODEL_TRIGRAMS:\n #   OPTIMALMODEL_TRIGRAMS_ALPHA= OPTIMALMODEL_TRIGRAMS[alpha]\n    \n#    print(\"Optimal Alpha for {} model : \".format(\"TRIGRAMS\"), '{:f}'.format(OPTIMALMODEL_TRIGRAMS_ALPHA))\n       \n#print(\"Optimal Alpha for {} model : \".format(\"TRIGRAMS\"), '{:f}'.format(OPTIMALMODEL_TRIGRAMS[alpha]))\n    \n#print(OPTIMALMODEL_TRIGRAMS_ALPHA)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e8e35d0152ec6f8b710e3fac43c3b01c6c19f37"},"cell_type":"code","source":"# Display  the performance of  TRIGRAMS model on TEST data\n\n#Predict the labels for the test set\n\ny_pred_TRIGRAMS = NB_OPTIMAL_classifier_for_TRIGRAMS.predict(TRIGRAMS_test_NB_vectors)\n    \n#Get the accuracy of the model on TEST data\ntest_accuracy_TRIGRAMS = accuracy_score(y_test_NB, y_pred_TRIGRAMS, normalize=True) * 100\npoints_TRIGRAMS = accuracy_score(y_test_NB, y_pred_TRIGRAMS, normalize=False)\n\n#Display the classification_report\nprint(classification_report(y_test_NB, y_pred_TRIGRAMS,digits=4))\n\n#Display the accuracy of the model on TEST data\nprint('\\nThe number of accurate predictions out of {} data points on unseen data is {}'.format(TRIGRAMS_test_NB_vectors.shape[0], points_TRIGRAMS))\nprint('Accuracy of the {} model on unseen data is {} %'.format(\"TRIGRAMS\", np.round(test_accuracy_TRIGRAMS,2)))\n\n#Display the confusion_matrix\nimport scikitplot.metrics as sciplot\nsciplot.plot_confusion_matrix(y_test_NB, y_pred_TRIGRAMS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a92315f1a04b109c2ce94719a7204a4efdd5a714"},"cell_type":"code","source":"\n    # '''Get top 50 features displayed from both the negative and the positive review classes for the TRI-GRAMS \n    # Reference URL: https://stackoverflow.com/questions/50526898/how-to-get-feature-importance-in-naive-bayes#50530697\n    \n    neg_class_prob_sorted_TRIGRAMS = (-NB_OPTIMAL_classifier_for_TRIGRAMS.feature_log_prob_[0, :]).argsort()               #Note : Putting a - sign indicates the indexes will be sorted in descending order.\n    pos_class_prob_sorted_TRIGRAMS = (-NB_OPTIMAL_classifier_for_TRIGRAMS.feature_log_prob_[1, :]).argsort()\n    \n    neg_class_features_TRIGRAMS = np.take(TFIDF_NB_TRIRAMS.get_feature_names(), neg_class_prob_sorted_TRIGRAMS[:50])\n    pos_class_features_TRIGRAMS = np.take(TFIDF_NB_TRIRAMS.get_feature_names(), pos_class_prob_sorted_TRIGRAMS[:50])\n    \n    print(\"The top 50 most frequent words from the positive class are :\\n\")\n    print(pos_class_features_TRIGRAMS)\n    \n    print(\"\\nThe top 50 most frequent words from the negative class are :\\n\")\n    print(neg_class_features_TRIGRAMS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db8cd8bdebecf36073cf3bf5a80de21cac27543f"},"cell_type":"code","source":"# Clearing the memory space for faster processing\ndel(neg_class_prob_sorted_TRIGRAMS, pos_class_prob_sorted_TRIGRAMS, neg_class_features_TRIGRAMS, pos_class_features_TRIGRAMS)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"336d4df9c426ec70bad9ff9f2e2132ab80330abd"},"cell_type":"code","source":"#from prettytable import prettyTable\n#tablenew = PrettyTable()\n# Summarize the Model details \n\n#tablenew.field_names = ([\"Model\", \"hyper parameter (Alpha)\", \"Best CV Score\", \"TEST ACCURACY\"])\n#tablenew.add_row([\"BOW-Model\", round(OPTIMAL_MODEL_for_BOW[alpha],5), NB_OPTIMAL_score_for_BOW, np.round(test_accuracy,2)])\n#tablenew.add_row([\"BIGRAMS-Model\", round(OPTIMAL_MODEL_for_BIGRAMS[alpha],5), NB_OPTIMAL_score_for_BIGRAMS, np.round(test_accuracy_BIGRAMS,2)])\n#tablenew.add_row([\"TRIGRAMS-Model\", round(OPTIMAL_MODEL_for_TRIGRAMS[alpha],5),NB_OPTIMAL_score_for_TRIGRAMS, np.round(test_accuracy_TRIGRAMS,2)])\n#tablenew.add_row([\"TFIDF-Model\", round(OPTIMAL_MODEL_for_TFIDF[alpha],5), NB_OPTIMAL_score_for_TFIDF, np.round(test_accuracy_TFIDF,2)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27b8f666886f84c41fcf06a6848781021864a0c5"},"cell_type":"markdown","source":"#  Summarizing the Results obtained from the various models:\n"},{"metadata":{"trusted":true,"_uuid":"2b1d657ee4308dda15b42ea176a01388aff485a2"},"cell_type":"code","source":"def print_table(table):\n    longest_cols = [\n        (max([len(str(row[i])) for row in table]) + 3)\n        for i in range(len(table[0]))\n    ]\n    row_format = \"\".join([\"{:>\" + str(longest_col) + \"}\" for longest_col in longest_cols])\n    for row in table:\n        print(row_format.format(*row))\n\ntable = [\n    [\"Model\", \"OPTIMAL_ALPHA\", \"BEST_CV_SCORE\", \"TEST_ACCURACY\"],\n    [\"BOW-Model\", round(OPTIMAL_MODEL_for_BOW[alpha],5), NB_OPTIMAL_score_for_BOW, round(test_accuracy,2)],\n    [\"TFIDF-Model\", round(OPTIMAL_MODEL_for_TFIDF[alpha],5), NB_OPTIMAL_score_for_TFIDF, round(test_accuracy_TFIDF,2)],\n    [\"BIGRAMS-Model\", round(OPTIMAL_MODEL_for_BIGRAMS[alpha],5), NB_OPTIMAL_score_for_BIGRAMS,round(test_accuracy_BIGRAMS,2)],\n    [\"TRIGRAMS-Model\",round(OPTIMALMODEL[alpha],5), NB_OPTIMAL_score_for_TRIGRAMS,round(test_accuracy_TRIGRAMS,2)]]\n \nprint_table(table)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}