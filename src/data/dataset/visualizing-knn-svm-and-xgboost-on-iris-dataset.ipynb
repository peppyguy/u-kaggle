{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "88e8ec6d-76e5-5f12-d9a6-2b40017a2b99"
      },
      "source": [
        "Here we use Python to visualize how certain machine learning algorithms classify certain data points in the Iris dataset. Let's begin by importing the Iris dataset and splitting it into features and labels. We will use only the petal length and width for this analysis.\n",
        "\n",
        "These visualizations and their code can be found in Sebastian Raschka's book, Python Machine Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6443d6db-6440-5d93-d37c-4fde4c428ca5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import data and modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "%pylab inline\n",
        "pylab.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# We'll use the petal length and width only for this analysis\n",
        "X = iris.data[:, [2, 3]]\n",
        "y = iris.target\n",
        "\n",
        "# Place the iris data into a pandas dataframe\n",
        "iris_df = pd.DataFrame(iris.data[:, [2, 3]], columns=iris.feature_names[2:])\n",
        "\n",
        "# View the first 5 rows of the data\n",
        "print(iris_df.head())\n",
        "\n",
        "# Print the unique labels of the dataset\n",
        "print('\\n' + 'The unique labels in this data are ' + str(np.unique(y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "eeaf0cb8-4c35-7422-1e08-345a25ed309a"
      },
      "source": [
        "Next, we'll split the data into training and test datasets.\n",
        "-----------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8393598e-1344-0e22-43fc-fdda6c75c08a"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n",
        "\n",
        "print('There are {} samples in the training set and {} samples in the test set'.format(\n",
        "X_train.shape[0], X_test.shape[0]))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "34a39b41-96cc-b560-c966-ddab1e67f65c"
      },
      "source": [
        "For many machine learning algorithms, it is important to scale the data. Let's do that now using sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5867b347-4875-1e21-4313-633873a56915"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "sc.fit(X_train)\n",
        "\n",
        "X_train_std = sc.transform(X_train)\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "print('After standardizing our features, the first 5 rows of our data now look like this:\\n')\n",
        "print(pd.DataFrame(X_train_std, columns=iris_df.columns).head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dc68363a-7def-d618-e190-0ddd26324bf2"
      },
      "source": [
        "If we plot the original data, we can see that one of the classes is linearly separable, but the other two are not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6f3d2990-5c6b-8ec3-4a38-63fe8ca3d241"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "markers = ('s', 'x', 'o')\n",
        "colors = ('red', 'blue', 'lightgreen')\n",
        "cmap = ListedColormap(colors[:len(np.unique(y_test))])\n",
        "for idx, cl in enumerate(np.unique(y)):\n",
        "    plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
        "               c=cmap(idx), marker=markers[idx], label=cl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "09b4dc98-b068-b70c-bf35-77e82b2adc8b"
      },
      "source": [
        "Let's try to use a Linear SVC to predict the the labels of our test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7e8e8442-ae96-b764-8ef6-67add2fa2ede"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\n",
        "svm.fit(X_train_std, y_train)\n",
        "\n",
        "print('The accuracy of the svm classifier on training data is {:.2f} out of 1'.format(svm.score(X_train_std, y_train)))\n",
        "\n",
        "print('The accuracy of the svm classifier on test data is {:.2f} out of 1'.format(svm.score(X_test_std, y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b76090ca-e371-6ac7-8b32-d5b404328849"
      },
      "source": [
        "It looks like our classifier performs pretty well. Let's visualize how the model classified the samples in our test data. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1b61e350-768e-a4e7-18df-6a31d617a974"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "\n",
        "def versiontuple(v):\n",
        "    return tuple(map(int, (v.split(\".\"))))\n",
        "\n",
        "\n",
        "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
        "\n",
        "    # setup marker generator and color map\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # plot the decision surface\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
        "                    alpha=0.8, c=cmap(idx),\n",
        "                    marker=markers[idx], label=cl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "78cfec64-2e2a-79d4-017d-e30725060f0b"
      },
      "outputs": [],
      "source": [
        "plot_decision_regions(X_test_std, y_test, svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "090509b2-66bd-0916-571c-4224d55ffa41"
      },
      "source": [
        "Now, let's test out a KNN classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "abd39cb6-86d8-32b6-6108-5cf69f5a4470"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
        "knn.fit(X_train_std, y_train)\n",
        "\n",
        "print('The accuracy of the knn classifier is {:.2f} out of 1 on training data'.format(knn.score(X_train_std, y_train)))\n",
        "print('The accuracy of the knn classifier is {:.2f} out of 1 on test data'.format(knn.score(X_test_std, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0e91c717-6928-e355-e15a-20b4e62ccec7"
      },
      "outputs": [],
      "source": [
        "plot_decision_regions(X_test_std, y_test, knn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "286d43b6-5fe4-da78-41e6-03e4e390d435"
      },
      "source": [
        "And just for fun, we'll plot an XGBoost classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "66ef8e17-46e6-5c33-131e-0444cf3acb56"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "xgb_clf = xgb_clf.fit(X_train_std, y_train)\n",
        "\n",
        "print('The accuracy of the xgb classifier is {:.2f} out of 1 on training data'.format(xgb_clf.score(X_train_std, y_train)))\n",
        "print('The accuracy of the xgb classifier is {:.2f} out of 1 on test data'.format(xgb_clf.score(X_test_std, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "40bf092b-3043-025f-b478-d14b760b58b8"
      },
      "outputs": [],
      "source": [
        "plot_decision_regions(X_test_std, y_test, xgb_clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "05bffe84-c21b-11b1-0a02-30100d339404"
      },
      "source": [
        "In all classifiers, the performance on the test data was better than the training data. At least with the parameters specified in this very simple approach, the KNN algorithm seems to have performed the best. However, this may not be the case depending on the dataset and more careful parameter tuning."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}