{"cells":[{"metadata":{"_uuid":"63fa02cc6f3a2593013a53dcd20bdd9c54533efb"},"cell_type":"markdown","source":"# 1. Introduction\n\nIn this kernel I am going to build two different CNN models: for Bee **subspecies** and Bee **health** classification. Then **visualize** kernels in Conv2D layers and see how they convolve images.\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"#####################################\n# Libraries\n#####################################\n# Common libs\nimport pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport random\nfrom pathlib import Path\n\n# Image processing\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\n#from skimage.transform import rescale, resize, downscale_local_mean\n\n# Charts\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# ML\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n#from sklearn.preprocessing import OneHotEncoder\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization,LeakyReLU\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nimport tensorflow\n\n#####################################\n# Settings\n#####################################\n\n# Set random seed to make results reproducable\nnp.random.seed(42)\ntensorflow.set_random_seed(42)\n\n# Global variables\nimg_folder='../input/bee_imgs/bee_imgs/'\nimg_width=100\nimg_height=100\nimg_channels=3","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"# 2. Read Bee data"},{"metadata":{"_uuid":"5d69ff10797c793abb7c5a05594f8d3463769995","scrolled":true,"trusted":true},"cell_type":"code","source":"bees=pd.read_csv('../input/bee_data.csv', \n                index_col=False,  \n                parse_dates={'datetime':[1,2]},\n                dtype={'subspecies':'category', 'health':'category','caste':'category'})\n#bees = bees.sample(300)\n\n# Will use this function later to load images of preprocessed bees\n# Don't load images just from the start to save memory for preprocessing steps\ndef read_img(file):\n    \"\"\"\n    Read and resize img, adjust channels. \n    Caution: This function is not independent, it uses global vars: img_folder, img_channels\n    @param file: file name without full path\n    \"\"\"\n    img = skimage.io.imread(img_folder + file)\n    img = skimage.transform.resize(img, (img_width, img_height), mode='reflect')\n    return img[:,:,:img_channels]\n\n# Cannot impute nans, drop them\nbees.dropna(inplace=True)\n\n# Some image files don't exist. Leave only bees with available images.\nimg_exists = bees['file'].apply(lambda f: os.path.exists(img_folder + f))\nbees = bees[img_exists]\n\nbees.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1419a5e71da60223177d900fd0e432e46bfa67a0"},"cell_type":"markdown","source":"# 3. Bee data EDA\n## 3.1 Distribution of bees by categories"},{"metadata":{"_uuid":"66c7d150a5af4da47f98ac1962e1ad571ab657e8","scrolled":false,"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,8))\n\nbees.subspecies.value_counts().plot(kind='bar',ax=ax[0, 0])\nax[0,0].set_ylabel('Count')\nax[0,0].set_title('Subspecies')\n\nbees.location.value_counts().plot(kind='bar', ax=ax[0, 1])\nax[0,1].set_title('Location')\nax[0,1].set_ylabel('Count')\n\nbees.caste.value_counts().plot(kind='bar', ax=ax[1, 0])\nax[1,0].set_title('Caste')\nax[1,0].set_ylabel('Count')\n\nbees.health.value_counts().plot(kind='bar', ax=ax[1,1])\nax[1,1].set_title('Health')\nax[1,1].set_ylabel('Count')\n\nf.subplots_adjust(hspace=0.7)\nf.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d750593ed655926232fabd8653ebadde62b19b31"},"cell_type":"markdown","source":"## 3.2 Look at Bees images"},{"metadata":{"_uuid":"4689e5fbb34e8257c76ddbf675bb2d2cf9ec9d58"},"cell_type":"markdown","source":"**Subspecies of Bee**"},{"metadata":{"_uuid":"cb4156649e8ebd2d06fd0b08d44ddbe6adeb6f9f","scrolled":true,"trusted":true},"cell_type":"code","source":"# Select first X subspecies titles \nsubspecies = bees['subspecies'].cat.categories\nf, ax = plt.subplots(nrows=1,ncols=subspecies.size, figsize=(12,3))\ni=0\n# Draw the first found bee of given subpecies\nfor s in subspecies:\n    if s == 'healthy': continue\n    file=img_folder + bees[bees['subspecies']==s].iloc[0]['file']\n    im=imageio.imread(file)\n    ax[i].imshow(im, resample=True)\n    ax[i].set_title(s, fontsize=8)\n    i+=1\n    \nplt.suptitle(\"Subspecies of Bee\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4aa22d2fa1d2031fe007634453c5be718c75495e"},"cell_type":"markdown","source":"**Healthy Bees**"},{"metadata":{"_uuid":"0949d8e3f7b45958689f615a7db2968e73e48192","trusted":true},"cell_type":"code","source":"ncols = 5\nhealthy = bees[bees['health'] == 'healthy'].sample(ncols)\n\nf, ax = plt.subplots(nrows=1,ncols=ncols, figsize=(12,3))\n# Read image of original size from disk, because bees['img'] contains resized numpy array\nfor i in range(0,5): \n    file = img_folder + healthy.iloc[i]['file']\n    ax[i].imshow(imageio.imread(file))\n\nplt.suptitle(\"Healthy Bees\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfac2443f700a6c1a067f2e3997b51b3a16e7e31"},"cell_type":"markdown","source":"**Sick Bees**"},{"metadata":{"_uuid":"2e3e93204162add0911088af55e5fb8bdbb96d48","trusted":true},"cell_type":"code","source":"health_cats = bees['health'].cat.categories\nf, ax = plt.subplots(1, health_cats.size-1, figsize=(12,4))\ni=0\nfor c in health_cats:\n    if c == 'healthy': continue\n    bee = bees[bees['health'] == c].sample(1).iloc[0]\n    ax[i].imshow(imageio.imread(img_folder + bee['file']))\n    ax[i].set_title(bee['health'], fontsize=8)\n    i += 1\n    \nplt.suptitle(\"Sick Bees\")    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79fb4788f90b3603c8b71472f44647b808bab25e"},"cell_type":"markdown","source":"# 4. Bee subspecies classification\nPreprocessing includes data balancing and augmentation.\nThen we'll be ready to train CNN.\n\n## 4.1. Data preprocessing for Bee subspecies\n### 4.1.1 Balancing samples by subspecies\nSplit all Bees to train, validation and test. Then balance train dataset.\nSplitting should be done before balancing to avoid putting the same upsampled Bee to both train and test.\n"},{"metadata":{"trusted":true,"_uuid":"a167115aa4fcc937097a1c5c8899b909f33fb2d5"},"cell_type":"code","source":"# The same split-balance idea will be used in 2 places: subspecies and health CNN.\n# Let's put this logic in function here to reuse.\ndef split_balance(bees, field_name):\n    \"\"\" \n    Split to train, test and validation. \n    Then balance train by given field name.\n    Draw plots before and after balancing\n    \n    @param bees: Total Bees dataset to balance and split\n    @param field_name: Field to balance by\n    @return:  balanced train bees, validation bees, test bees\n    \"\"\"\n    # Split to train and test before balancing\n    train_bees, test_bees = train_test_split(bees, random_state=24)\n\n    # Split train to train and validation datasets\n    # Validation for use during learning\n    train_bees, val_bees = train_test_split(train_bees, test_size=0.1, random_state=24)\n\n    #Balance by subspecies to train_bees_bal_ss dataset\n    # Number of samples in each category\n    ncat_bal = int(len(train_bees)/train_bees[field_name].cat.categories.size)\n    train_bees_bal = train_bees.groupby(field_name, as_index=False).apply(lambda g:  g.sample(ncat_bal, replace=True)).reset_index(drop=True)\n    return(train_bees_bal, val_bees, test_bees)\n    \ndef plot_balanced(train_bees, train_bees_bal, field_name):\n    \"\"\"\n    Draw distribution of field by categories before and after balancing\n    @param train_bees: before balancing\n    @param train_bees_bal: after balancing\n    @param field_name: balancing field\n    \"\"\"\n    # Plot before and after balancing\n    f, axs = plt.subplots(1,2, figsize=(8,4))\n\n    # Before\n    ax = train_bees[field_name].value_counts().plot(kind='bar', ax=axs[0])\n    ax.set_title('%s before balancing' % field_name)\n    ax.set_ylabel('Count')\n\n    # After\n    ax = train_bees_bal[field_name].value_counts().plot(kind='bar', ax=axs[1])\n    ax.set_title('%s after balancing' % field_name)\n    ax.set_ylabel('Count')\n\n    plt.tight_layout()\n    plt.show()\n\n# Split/balance and plot the result\ntrain_bees_bal, val_bees, test_bees = split_balance(bees, 'subspecies')\nplot_balanced(bees, train_bees_bal, 'subspecies')\n\n# Will use balanced dataset as main\ntrain_bees = train_bees_bal","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9650143e38ccda561450d08d241cb7ce51c89e8"},"cell_type":"markdown","source":"\n### 4.1.2 Prepare features\nWe prepared train_bees dataset but did not load images from files until this state. Now load them and use ImageDataGenerator to randomly shift/rotate/zoom. "},{"metadata":{"trusted":true,"_uuid":"dcda4508b7424518d4c11a0e4b7ad2a05f803601"},"cell_type":"code","source":"# The same way of loading images and one hot encoding will be used in 2 places: subspecies and health CNN.\n# Let's put this logic in function here to reuse.\ndef prepare2train(train_bees, val_bees, test_bees, field_name):\n    \"\"\"\n    Load images for features, drop other columns\n    One hot encode for label, drop other columns\n    @return: image generator, train images, validation images, test images, train labels, validation labels, test labels\n    \"\"\"\n    # Bees already splitted to train, validation and test\n    # Load and transform images to have equal width/height/channels. \n    # read_img function is defined in the beginning to use in both health and subspecies. \n    # Use np.stack to get NumPy array for CNN input\n\n    # Train data\n    train_X = np.stack(train_bees['file'].apply(read_img))\n    #train_y = to_categorical(train_bees[field_name].values)\n    train_y  = pd.get_dummies(train_bees[field_name], drop_first=False)\n\n    # Validation during training data to calc val_loss metric\n    val_X = np.stack(val_bees['file'].apply(read_img))\n    #val_y = to_categorical(val_bees[field_name].values)\n    val_y = pd.get_dummies(val_bees[field_name], drop_first=False)\n\n    # Test data\n    test_X = np.stack(test_bees['file'].apply(read_img))\n    #test_y = to_categorical(test_bees[field_name].values)\n    test_y = pd.get_dummies(test_bees[field_name], drop_first=False)\n\n    # Data augmentation - a little bit rotate, zoom and shift input images.\n    generator = ImageDataGenerator(\n            featurewise_center=False,  # set input mean to 0 over the dataset\n            samplewise_center=False,  # set each sample mean to 0\n            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n            samplewise_std_normalization=False,  # divide each input by its std\n            zca_whitening=False,  # apply ZCA whitening\n            rotation_range=180,  # randomly rotate images in the range (degrees, 0 to 180)\n            zoom_range = 0.1, # Randomly zoom image \n            width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n            height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n            horizontal_flip=True,  # randomly flip images\n            vertical_flip=True)\n    generator.fit(train_X)\n    return (generator, train_X, val_X, test_X, train_y, val_y, test_y)\n\n#train_bees['subspecies'].cat.values[:-100]\n# Call image preparation and one hot encoding\ngenerator, train_X, val_X, test_X, train_y, val_y, test_y = prepare2train(train_bees, val_bees, test_bees, 'subspecies')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f969b9f496c11dacac4583a319f3dcbd56d3a6ea"},"cell_type":"markdown","source":"## 4.2 Train Bee Subspecies CNN"},{"metadata":{"_uuid":"053d12032f6d113094617826f5f78bba8555c115","trusted":true},"cell_type":"code","source":"# We'll stop training if no improvement after some epochs\nearlystopper1 = EarlyStopping(monitor='loss', patience=10, verbose=1)\n\n# Save the best model during the traning\ncheckpointer1 = ModelCheckpoint('best_model1.h5'\n                                ,monitor='val_acc'\n                                ,verbose=1\n                                ,save_best_only=True\n                                ,save_weights_only=True)\n# Build CNN model\nmodel1=Sequential()\nmodel1.add(Conv2D(6, kernel_size=3, input_shape=(img_width, img_height,3), activation='relu', padding='same'))\nmodel1.add(MaxPool2D(2))\nmodel1.add(Conv2D(12, kernel_size=3, activation='relu', padding='same'))\nmodel1.add(Flatten())\nmodel1.add(Dense(train_y.columns.size, activation='softmax'))\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train\ntraining1 = model1.fit_generator(generator.flow(train_X,train_y, batch_size=60)\n                        ,epochs=20\n                        ,validation_data=[val_X, val_y]\n                        ,steps_per_epoch=50\n                        ,callbacks=[earlystopper1, checkpointer1])\n# Get the best saved weights\nmodel1.load_weights('best_model1.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac639c24adf884c806ce1ab985fe7db7e9aab26a"},"cell_type":"markdown","source":"## 4.3 Evaluate bee subspecies detection model"},{"metadata":{"_uuid":"b096e119c62a41dc227ad9f12131ca0c28dbed8b","trusted":true},"cell_type":"code","source":"# This is a function to use in Bee subspecies and health evaluation\ndef eval_model(training, model, test_X, test_y, field_name):\n    \"\"\"\n    Model evaluation: plots, classification report\n    @param training: model training history\n    @param model: trained model\n    @param test_X: features \n    @param test_y: labels\n    @param field_name: label name to display on plots\n    \"\"\"\n    ## Trained model analysis and evaluation\n    f, ax = plt.subplots(2,1, figsize=(5,5))\n    ax[0].plot(training.history['loss'], label=\"Loss\")\n    ax[0].plot(training.history['val_loss'], label=\"Validation loss\")\n    ax[0].set_title('%s: loss' % field_name)\n    ax[0].set_xlabel('Epoch')\n    ax[0].set_ylabel('Loss')\n    ax[0].legend()\n    \n    # Accuracy\n    ax[1].plot(training1.history['acc'], label=\"Accuracy\")\n    ax[1].plot(training1.history['val_acc'], label=\"Validation accuracy\")\n    ax[1].set_title('%s: accuracy' % field_name)\n    ax[1].set_xlabel('Epoch')\n    ax[1].set_ylabel('Accuracy')\n    ax[1].legend()\n    plt.tight_layout()\n    plt.show()\n\n    # Accuracy by subspecies\n    test_pred = model.predict(test_X)\n    \n    acc_by_subspecies = np.logical_and((test_pred > 0.5), test_y).sum()/test_y.sum()\n    acc_by_subspecies.plot(kind='bar', title='Accuracy by %s' % field_name)\n    plt.ylabel('Accuracy')\n    plt.show()\n\n    # Print metrics\n    print(\"Classification report\")\n    test_pred = np.argmax(test_pred, axis=1)\n    test_truth = np.argmax(test_y.values, axis=1)\n    print(metrics.classification_report(test_truth, test_pred, target_names=test_y.columns))\n\n    # Loss function and accuracy\n    test_res = model.evaluate(test_X, test_y.values, verbose=0)\n    print('Loss function: %s, accuracy:' % test_res[0], test_res[1])\n\n# Call evaluation function\neval_model(training1, model1, test_X, test_y, 'subspecies')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c24afb681b32e6ef0f7f82da7b5ceb48957c29c"},"cell_type":"markdown","source":"# 5. CNN model for  Bee health classification"},{"metadata":{"_uuid":"785153eaf5e17c01d6d509b9b66154a98c3b83e9"},"cell_type":"markdown","source":"## 5.1. Data preprocessing for Bee health classification\n### 5.1.1 Balance Bees by health\nThis balancing 1:1 repeats the one from Bee subspecies section. So just call the function we defined there for reuse."},{"metadata":{"trusted":true,"_uuid":"f8030f902454b74b47630fc89189b8607d286105"},"cell_type":"code","source":"# Split/balance and plot the result\ntrain_bees_bal, val_bees, test_bees = split_balance(bees, 'health')\nplot_balanced(train_bees, train_bees_bal, 'health')\n\n# Will use balanced dataset as main\ntrain_bees_bal = train_bees","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"094a5867505333fb96bedec2efba0ebd8071c5b1"},"cell_type":"markdown","source":"### 5.1.2 Features augmentation and labels one hot encoding for Health CNN\nThe same preparation as in Bee subspecies section, we can again call functions, defined there.\n"},{"metadata":{"trusted":true,"_uuid":"6df8822d7bf2a79f23e01900ed6d0cdee7acb088"},"cell_type":"code","source":"# Call image preparation and one hot encoding from Bee subspecies section\ngenerator, train_X, val_X, test_X, train_y, val_y, test_y = prepare2train(train_bees, val_bees, test_bees, 'health')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7620ba6877b6079964c584b9db82261c06f866fb"},"cell_type":"markdown","source":"## 5.2 Train Bee health CNN"},{"metadata":{"_uuid":"0419a7cc53de6386e9f7ac61f229ffd57160624f","scrolled":true,"trusted":true},"cell_type":"code","source":"# We'll stop training if no improvement after some epochs\nearlystopper2 = EarlyStopping(monitor='val_acc', patience=10, verbose=1)\n\n# Save the best model during the traning\ncheckpointer2 = ModelCheckpoint('best_model2.h5'\n                                ,monitor='val_acc'\n                                ,verbose=1\n                                ,save_best_only=True\n                                ,save_weights_only=True)\n# Build CNN model\nmodel2=Sequential()\nmodel2.add(Conv2D(5, kernel_size=3, input_shape=(img_width, img_height,3), activation='relu', padding='same'))\nmodel2.add(MaxPool2D(2))\nmodel2.add(Conv2D(10, kernel_size=3, activation='relu', padding='same'))\nmodel2.add(Flatten())\nmodel2.add(Dense(train_y.columns.size, activation='softmax'))\nmodel2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train\ntraining2 = model2.fit_generator(generator.flow(train_X,train_y, batch_size=60)\n                        ,epochs=20\n                        ,validation_data=[val_X, val_y]\n                        ,steps_per_epoch=50\n                        ,callbacks=[earlystopper2, checkpointer2])\n# Get the best saved weights\nmodel2.load_weights('best_model2.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40e9d809e0db73459ffcb6602e4952ddd36f54ef"},"cell_type":"markdown","source":"## 5.3 Evaluate Bee health classification model"},{"metadata":{"_uuid":"66d8a1413a6d48065a2066bb4601df1d0de0a344","scrolled":true,"trusted":true},"cell_type":"code","source":"# Call evaluation with charts, defined in Bee subspecies section\neval_model(training2, model2, test_X, test_y, 'health')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"577f53b0f300436684db9337f5f7b2e8c3239cce"},"cell_type":"markdown","source":"# 6. Visualization of Conv2D layers\n"},{"metadata":{"_uuid":"c75a17f623e78f677dfae5ada6d87f7cfd8d1d30"},"cell_type":"markdown","source":"Let's look how our models process images. Our models contains Conv2D layers with kernels inside. We are going to convolve a sample image through kernels and see how does it look before and after each kernel. For each kernel visualize: kernel itself, input image, output image. No idea how to interprete these results, let's do it for fun :)\n\nFunction for Conv2D layers visualization:"},{"metadata":{"_uuid":"0556df0097ce5c7585e8e27517ef8bd47f4d436f","trusted":true},"cell_type":"code","source":"# Common function for visualization of kernels\ndef visualize_layer_kernels(img, conv_layer, title):\n    \"\"\"\n    Displays how input sample image looks after convolution by each kernel\n    :param img: Sample image array\n    :param conv_layer: Layer of Conv2D type\n    :param title: Text to display on the top \n    \"\"\"\n    # Extract kernels from given layer\n    weights1 = conv_layer.get_weights()\n    kernels = weights1[0]\n    kernels_num = kernels.shape[3]\n    \n    # Each row contains 3 images: kernel, input image, output image\n    f, ax = plt.subplots(kernels_num, 3, figsize=(7, kernels_num*2))\n\n    for i in range(0, kernels_num):\n        # Get kernel from the layer and draw it\n        kernel=kernels[:,:,:3,i]\n        ax[i][0].imshow((kernel * 255).astype(np.uint8), vmin=0, vmax=255)\n        ax[i][0].set_title(\"Kernel %d\" % i, fontsize = 9)\n        \n        # Get and draw sample image from test data\n        ax[i][1].imshow((img * 255).astype(np.uint8), vmin=0, vmax=255)\n        ax[i][1].set_title(\"Before\", fontsize=8)\n        \n        # Filtered image - apply convolution\n        img_filt = scipy.ndimage.filters.convolve(img, kernel)\n        ax[i][2].imshow((img_filt * 255).astype(np.uint8), vmin=0, vmax=255)\n        ax[i][2].set_title(\"After\", fontsize=8)\n        \n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.93)\n    plt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cc6bad0c3f8da4e4868aa8a2f7d5cc8c830726c"},"cell_type":"markdown","source":"## 6.1 Visualize convolutions in Bee subspecies CNN\n"},{"metadata":{"_uuid":"ca71ec8bff6ef29197449d18f24bc08dd1112e30","trusted":true},"cell_type":"code","source":"# Take sample image to visualize convolution\nidx = random.randint(0,len(test_X)-1)\nimg = test_X[idx,:,:,:]\n# Take 1st convolutional layer and look at it's filters\nconv1 = model1.layers[0]\nimg = visualize_layer_kernels(img, conv1, \"Subspecies CNN. Layer 0\")\n\n# Take sample image to visualize convolutoin\nidx = random.randint(0,len(test_y)-1)\nimg = test_X[idx,:,:,:]\n# Take another convolutional layer and look at it's filters\nconv2 = model1.layers[2]\nres = visualize_layer_kernels(img, conv2, \"Subspecies CNN. Layer 2\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75ab17f9d3369800a2b4ab1d8c793a836f653255"},"cell_type":"markdown","source":"## 6.2 Visualize convolutions in Bee health CNN"},{"metadata":{"_uuid":"b5cb47b7cd4d89206ea329bf71a7de7e702f6f28","trusted":true},"cell_type":"code","source":"# Take sample image to visualize convolution\nidx = random.randint(0,len(test_X)-1)\nimg = test_X[idx,:,:,:]\n#img = img[:,:,:]\n# Take 1st convolutional layer and look at it's filters\nconv1 = model2.layers[0]\nvisualize_layer_kernels(img, conv1, \"Health CNN layer 0\")\n\n# Take sample image to visualize convolutoin\nidx = random.randint(0,len(test_X)-1)\nimg = test_X[idx,:,:,:]\n# Take another convolutional layer and look at it's filters\nconv2 = model2.layers[2]\nvisualize_layer_kernels(img, conv2, \"Health CNN layer 2\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c944484e8be57d3a0b7fe4db9b21099dd888ebe"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}