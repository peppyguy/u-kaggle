{"cells":[{"metadata":{"_uuid":"e6116d8cf6327915c5006fc96f926b88390d7a18"},"cell_type":"markdown","source":"## Introduction\nIn this kernel, we apply different neural network models to MNIST data set and compare their performance."},{"metadata":{"_uuid":"c43294b5e6e5c25ef3edc0388f7dee09af735eae"},"cell_type":"markdown","source":"## About MNIST Dataset\nThe dataset contains hand written images of digits [ 0 - 9 ].  We have to build algorithm to find what digit is given image."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6ab8dacf0edbf4299f7b076bfd88d5628606bc4"},"cell_type":"markdown","source":"Loading data set."},{"metadata":{"trusted":true,"_uuid":"932f9c8dbfaa7fb894096f2f8c7f2199ae7347e3"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\n\nlabels = train['label'].values\nimages = train.drop(labels = ['label'],axis = 1).values\ndel train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"820f7da682f4a9c23dd45b04e2d6299c827d1f4c"},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true,"_uuid":"7c7e6567b3a4122ffaeed0a81d1c56a9e35c2a2b"},"cell_type":"code","source":"images = images/255\nimages = images.reshape(-1,28,28,1)\nimages.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92cd2e1ddd61370d1acfabd8b261def4f41cf8a6"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_images,test_images,train_labels, test_labels = train_test_split(images,labels, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c9a47584c3e29d9cf69c1104a8c95f4d9bdc45b"},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## 1 . Single Layer Neural Network"},{"metadata":{"trusted":true,"_uuid":"b5d9514a9c9fc08d7ee96d7f770ae273291c9eac"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Flatten(input_shape = (28,28,1)))\nmodel.add(Dense(10,activation = 'softmax'))\nmodel.compile(optimizer=tf.train.AdamOptimizer(), \n             loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53b64534632e875c592fb94ad05208eb29e8ef19"},"cell_type":"code","source":"model.fit(train_images,train_labels,epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39e767cf4531e72bdecad76637eb5b3515923245"},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_images, test_labels)\n\nprint('Test accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ff24843a787c26da611f8a78b7e5513a5aebbf5"},"cell_type":"markdown","source":"Next, we look at accuracy by adding a layer."},{"metadata":{"_uuid":"6944f1d74c343ff8f322fe54eee09df2f00175fb"},"cell_type":"markdown","source":"## 2 . Two Layer Neural Network"},{"metadata":{"_uuid":"05b09c6cde0eaee0cc3ef4cd7e69a7d8ab48e2ef"},"cell_type":"markdown","source":"### 2.1  With 32 Neurons in hidden layer"},{"metadata":{"trusted":true,"_uuid":"11631d0fccd79b93ab15cab4a4c56258d0d32ea9"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Flatten(input_shape = (28,28,1)))\nmodel.add(Dense(32,activation = 'relu'))\nmodel.add(Dense(10,activation = 'softmax'))\nmodel.compile(optimizer=tf.train.AdamOptimizer(), \n             loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"508b66736949bf9b26ec74fcc60eabf4f1534366"},"cell_type":"code","source":"model.fit(train_images,train_labels,epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2028b2edc05e828a00af006054758ed58b194c32"},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_images, test_labels)\n\nprint('Test accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c84475b35f045d460151226010c466271e74e09"},"cell_type":"markdown","source":"It seems accuracy is increasing when a layer is added. Let's look at what happens when we increase the number of perceptrons. We do the same with 64, 128 perceptrons in hidden layer."},{"metadata":{"_uuid":"8c8235d61a44920a23a9cb0f6885aba56b82888a"},"cell_type":"markdown","source":"### 2.2 With 64 Neurons in hidden layer"},{"metadata":{"trusted":true,"_uuid":"2c8d9486c4ab7617e902e114d07770cd50d4e1d1"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Flatten(input_shape = (28,28,1)))\nmodel.add(Dense(64,activation = 'relu'))\nmodel.add(Dense(10,activation = 'softmax'))\nmodel.compile(optimizer=tf.train.AdamOptimizer(), \n             loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdd76b533f8c5e3d0b5a9826c3852e0b1e5c5d47"},"cell_type":"code","source":"model.fit(train_images,train_labels,epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef477034d701ade4bb70ae9bf98d85b025e629fc"},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_images, test_labels)\n\nprint('Test accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9eb7e588f41ff9299d14d2dee89159dcd72a3db"},"cell_type":"markdown","source":"### 2.3 With 128 Neurons in hidden layer"},{"metadata":{"trusted":true,"_uuid":"41f1a3e911714210c265918b2c9a452585b13716"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Flatten(input_shape = (28,28,1)))\nmodel.add(Dense(128,activation = 'relu'))\nmodel.add(Dense(10,activation = 'softmax'))\nmodel.compile(optimizer=tf.train.AdamOptimizer(), \n             loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84e83b297fddda3f44808125c201f4c3d2c34a46"},"cell_type":"code","source":"model.fit(train_images,train_labels,epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c732b8fe6b9ddfa60dedab88fd6fa97906c4c9c6"},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_images, test_labels)\n\nprint('Test accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"418d38d97ee4cc17880ce8821a02377856a43139"},"cell_type":"markdown","source":"From our observation with 32, 64 and 128 perceptrons in hidden layer, it seems higher the preceptrons in hidden layer, higher the accuracy."},{"metadata":{"_uuid":"2718bc2634d4d7c17418262c59aee265e0479228"},"cell_type":"markdown","source":"## 3. Multilayer Neural Network"},{"metadata":{"_uuid":"60bd008dedcf53cce83da8fac259dbd44262e57b"},"cell_type":"markdown","source":"### 3.1 Three Hidden layers"},{"metadata":{"trusted":true,"_uuid":"50a0bd4e090ba2caa1e8c1bae17d368c09ed716b"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Flatten(input_shape = (28,28,1)))\nmodel.add(Dense(32,activation = 'relu'))\nmodel.add(Dense(64,activation = 'relu'))\nmodel.add(Dense(128,activation = 'relu'))\nmodel.add(Dense(10,activation = 'softmax'))\nmodel.compile(optimizer=tf.train.AdamOptimizer(), \n             loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f893f9efaf2011fd3c3a8f7dedc023d6815f22c"},"cell_type":"code","source":"model.fit(train_images,train_labels,epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04e0c5872f19856ab6ef1a6bf848165e1de3d532"},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_images, test_labels)\n\nprint('Test accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1692b62b3cd7ad5205af195e2debc22d7ddb8155"},"cell_type":"markdown","source":"### 3.2 Four hidden layers"},{"metadata":{"trusted":true,"_uuid":"ad33664fe2e747e0d6da8454320682b416c41afd"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Flatten(input_shape = (28,28,1)))\nmodel.add(Dense(32,activation = 'relu'))\nmodel.add(Dense(64,activation = 'relu'))\nmodel.add(Dense(128,activation = 'relu'))\nmodel.add(Dense(256,activation = 'relu'))\nmodel.add(Dense(10,activation = 'softmax'))\nmodel.compile(optimizer=tf.train.AdamOptimizer(), \n             loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc332d240426038fe52b93407b81341e4ff86034"},"cell_type":"code","source":"model.fit(train_images,train_labels,epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b989ff2c19d50da39cdc9a98c6b6be797769bef"},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_images, test_labels)\n\nprint('Test accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efb64ac71a0c1bfc0ac4c7cbe9c58e8419525ad9"},"cell_type":"markdown","source":"Inreasing the number of layers to 3 and 4 gave less accuracy (0.9697 and 0.9709 respectively). For  two layer neural network, the accuracy was 0.972. Two layer neural network performed better. It may be due to overfitting in case of Multilayer perceptron."},{"metadata":{"trusted":true,"_uuid":"9a829bb2ac42d927731717f239b1a65de5ebb69e"},"cell_type":"code","source":"from keras.layers import Conv2D, MaxPool2D, Dropout","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed63a7bdb43b99a027e1c281d3d50332f5b3752d"},"cell_type":"markdown","source":"## 4. Convolution Neural Network\nWe use kernel size of 5 * 5."},{"metadata":{"_uuid":"3ff0ab683e017620a1ed63503d3f707631f3b2b4"},"cell_type":"markdown","source":"### 4.1 One convolution layer"},{"metadata":{"trusted":true,"_uuid":"de0d4300723a0937041661617a470b4bf33caa06"},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\nmodel.compile(optimizer=tf.train.AdamOptimizer(), \n             loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df639d93aae9993d40d0b56fd16e529aade95e12"},"cell_type":"code","source":"model.fit(train_images,train_labels,epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17368011cddce822a3eceecd255d5b263a2f627c"},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_images, test_labels)\n\nprint('Test accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f55f3a8f91b07c4dacd41a2b040ac5ae579c948"},"cell_type":"markdown","source":"### 4.2 More than on Convolution layers"},{"metadata":{"trusted":true,"_uuid":"3d833e97d7b67eebf5f36731d6070c5e458e0784"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32,kernel_size = (5,5),activation = 'relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters = 64,kernel_size = (3,3),activation = 'relu'))\nmodel.add(Conv2D(filters = 64,kernel_size = (3,3),activation = 'relu'))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\nmodel.compile(optimizer=tf.train.AdamOptimizer(), \n             loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7673302b18605c3ec073cfb03d69962858027b24"},"cell_type":"code","source":"model.fit(train_images,train_labels,epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4376fe86961f6993600459c6fb4c0d0557295467"},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_images, test_labels)\n\nprint('Test accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a26390401578fe8eaa67bb406ebe549f5e5aba04"},"cell_type":"markdown","source":"## Result\n\nConvolution neural network has highest accuracy. This may be due to CNN's ability to learn hidden futures in a better way.\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}