{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"[Hakan CEBECİ'nin](http://https://www.udemy.com/user/software-development/)\n\n* [Doğal Dil İşleme A-Z™: (NLP)](http://https://www.udemy.com/dogal-dil-isleme/) \n\nkursundan öğrendiklerimi denediğim ve derlediğim kernelimdir.\n"},{"metadata":{"trusted":true,"_uuid":"91089a96def02f009eae6f7a607cbca06bb4f873"},"cell_type":"code","source":"# NLTK-Tokenize\nfrom nltk.tokenize import sent_tokenize, word_tokenize\ntext = \"Alan Mathison Turing was an English computer scientist, mathematician, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general purpose computer. Turing is widely considered to be the father of theoretical computer science and artificial intelligence.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7ad9d0cd5583550451243b0b21cb52336fd75a4"},"cell_type":"code","source":"text.split()[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffd2e7f59b45a892996fc797d944083d74fec56c"},"cell_type":"code","source":"word_tokenize(text)[0:10] #kelıme tokenlestırme","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ae07f0404b5da99d6e4022e4c9d73b1eb2bb010"},"cell_type":"code","source":"sent_tokenize(text)[0:10] #cumle tokenlestırme","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"728279c5e90df95fb2c96077b38f7005139844c9"},"cell_type":"code","source":"# NLTK-StopWords\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ntext = 'Fazıl Say is a Turkish pianist and composer who was born in Ankara, described recently as \"not merely a pianist of genius; but undoubtedly he will be one of the great artists of the twenty-first century\".'\n\nstopwords = stopwords.words('english')\nprint(stopwords[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc3ea891ad8f31780306f0454a96879ee7e7a898"},"cell_type":"code","source":"words = word_tokenize(text)\nfiltered_words = []\nfor word in words:\n    if word not in stopwords:\n        filtered_words.append(word)\n        \nfiltered_words[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6444b7849b36d27b2efaf518b492badda11e8b79"},"cell_type":"code","source":"# NLTK-Stemmer\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n\nwords = ['drive', 'driving', 'driver', 'drives', 'drove', 'cats', 'children']\n#words = ['Boyunluk', 'Boynu', 'Boylar', 'Boyun', 'Boy']\n\nfor w in words:\n    print(ps.stem(w))\n    \n#kelimenin sonundaki ekler kesiliyor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c769865bc25657272d193c567eaaf33aa18e508"},"cell_type":"code","source":"# NLTK-Part of Speech Tagging\nimport nltk\n\ntext = 'Friedrich Wilhelm Nietzsche was a German philosopher, cultural critic, composer, poet, philologist, and a Latin and Greek scholar whose work has exerted a profound influence on Western philosophy and modern intellectual history. He began his career as a classical philologist before turning to philosophy. He became the youngest ever to hold the Chair of Classical Philology at the University of Basel in 1869 at the age of 24. Nietzsche resigned in 1879 due to health problems that plagued him most of his life; he completed much of his core writing in the following decade. In 1889 at age 44, he suffered a collapse and afterward, a complete loss of his mental faculties. He lived his remaining years in the care of his mother until her death in 1897 and then with his sister Elisabeth Förster-Nietzsche. Nietzsche died in 1900.'\ntokenized = nltk.word_tokenize(text)\ntokenized[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44ee917792c47d76fcc0e63354e4a4411f10b68d"},"cell_type":"code","source":"\"\"\"\nCC     coordinating conjunction\nCD     cardinal digit\nDT     determiner\nEX     existential there (like: \"there is\" ... think of it like \"there exists\")\nFW     foreign word\nIN     preposition/subordinating conjunction\nJJ     adjective 'big'\nJJR    adjective, comparative 'bigger'\nJJS    adjective, superlative 'biggest'\nLS     list marker 1)\nMD     modal could, will\nNN     noun, singular 'desk'\nNNS    noun plural 'desks'\nNNP    proper noun, singular 'Harrison'\nNNPS   proper noun, plural 'Americans'\nPDT    predeterminer 'all the kids'\nPOS    possessive ending parent's\nPRP    personal pronoun I, he, she\nPRP$   possessive pronoun my, his, hers\nRB     adverb very, silently,\nRBR    adverb, comparative better\nRBS    adverb, superlative best\nRP     particle give up\nTO     to go 'to' the store.\nUH     interjection errrrrrrrm\nVB     verb, base form take\nVBD    verb, past tense took\nVBG    verb, gerund/present participle taking\nVBN    verb, past participle taken\nVBP    verb, sing. present, non-3d take\nVBZ    verb, 3rd person sing. present takes\nWDT    wh-determiner which\nWP     wh-pronoun who, what\nWP$    possessive wh-pronoun whose\nWRB    wh-abverb where, when\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c3fa8346699e0083267ad7b00dc4656da31d671"},"cell_type":"code","source":"nltk.pos_tag(tokenized)[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d272ce9daf1cd2b8cd0d71f55a0b6d3e6628251e"},"cell_type":"code","source":"# NLTK-named entitiy recognition\n\nimport nltk\ntext = \"Steve Jobs was an American entrepreneur and business magnate. He was the chairman, chief executive officer (CEO), and a co-founder of Apple Inc., chairman and majority shareholder of Pixar, a member of The Walt Disney Company's board of directors following its acquisition of Pixar, and the founder, chairman, and CEO of NeXT. Jobs is widely recognized as a pioneer of the microcomputer revolution of the 1970s and 1980s, along with Apple co-founder Steve Wozniak. \"\ntokenized = nltk.word_tokenize(text)\nprint(tokenized[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b4025cc459717bee65101d8a54bbc0aa3db0ad0"},"cell_type":"code","source":"tagged = nltk.pos_tag(tokenized)\nprint(tagged[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ee5a0d5367f1b670541aced1f05984246f23bf0"},"cell_type":"code","source":"named_ent = nltk.ne_chunk(tagged)\nprint(named_ent[0:10])\n#named_ent.draw()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2dc5445cd47c898086d8cec166df1966fa20778"},"cell_type":"code","source":"\"\"\"\nNE Türü         \tÖrnek\nORGANIZATION    \tGeorgia-Pacific Corp., WHO\nPERSON          \tEddy Bonte, President Obama\nLOCATION        \tMurray River, Mount Everest\nDATE            \tJune, 2008-06-29\nTIME            \ttwo fifty a m, 1:30 p.m.\nMONEY           \t175 million Canadian Dollars, GBP 10.40\nPERCENT         \ttwenty pct, 18.75 %\nFACILITY        \tWashington Monument, Stonehenge\nGPE             \tSouth East Asia, Midlothian\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bdd6dbd9521a2349747d905b7fd08d2d8348c9f"},"cell_type":"code","source":"# NLTK-Lemmatizing\nfrom nltk.stem import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\nwords = ['drive', 'driving', 'driver', 'drives', 'drove', 'cats', 'children']\nfor w in words:\n    print(lem.lemmatize(w))\n#kelimenin sözlükteki köküne iniliyor (morfolojik)\n\nprint(lem.lemmatize('drove', 'v'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6f61d7355e8b29a6950e317bdd5808b5e300a86"},"cell_type":"code","source":"#Word2Vec\nimport numpy as np\n#numpy vektor ve matrısler uzerıne ıslem yapmamızı sağlar\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Word2Vec corpus üzerinde tüm kelimelerin üzerinden geç\n# her kelimenin etrafındaki kelimeleri tahmin et\n# iki kelime birbirne ne kadar sık bulunuyorsa vektöre yansıt\nf = open('../input/hurriyet/hurriyet.txt', 'r', encoding='utf8')\ntext = f.read()\nt_list = text.split('\\n')\n\ncorpus = []\n\nfor cumle in t_list:\n    corpus.append(cumle.split())\nprint(corpus[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"653f99653e684c30fa9fcb2e5c14ac53f144ccb8"},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/word2vec-algoritmalari/word2vec_algoritmalar.PNG\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f113393b3b917b884ed16d9750f08b53f1310d5"},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/sgram-cbow/skipgram_cbow.PNG\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9671530475321d91fdf06c9094db26b842e396db"},"cell_type":"code","source":"model = Word2Vec(corpus, size=100, window=5, min_count=5, sg=1) #genelde size 5-300 uzunlugunda vektor olustururuz #gozetımsız ogrenme yontemı\n#size 100 uzunlugunda vektor\n#window sol ve sagda bakılacak kelıme sayısı\n#sg 1 ise skip-gram kullanılacak, default olarak cbow kullanılıyor.\n#mın_count kelıme en az kac kere gecıyorsa al \nmodel.wv['ankara']\n# wv word vektorun kısaltılmısı\n#kelime vektörü, word vector, word embedding, embedding hep aynı şeyi ifade ediyor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3f8c8238e64c5066658bdff82813b5aa87c208f"},"cell_type":"code","source":"print(model.wv.most_similar('almanya'))\n#yazdıgınız kelıme kelıme haznesınde yoksa hata alırsınız.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6197e409f1b591ab2a4374c2a454512089a11e57"},"cell_type":"code","source":"#model.save('word2vec.model')\n#model = Word2Vec.load('word2vec.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8df87248700a65baad5f7161523bbb62125124c2"},"cell_type":"code","source":"def closestwords_tsneplot(model, word):\n    '''\n     bu sınıf verılen modelı ıle kelımeyı alır\n     kelımeye en yakın kelımelerın vektorlerını bır dızıye atarız\n     TSNE ıle bu vektorlerı grafıge donustururuz\n    '''\n    word_vectors = np.empty((0,100)) # en yakın olanları lısteyi hazırladık\n    word_labels = [word] #kelımeyı dızı halıne getırdık\n    \n    close_words = model.wv.most_similar(word) #yakın olan kelımeler bulundu\n    \n    word_vectors = np.append(word_vectors, np.array([model.wv[word]]), axis=0) #gelen kelımenın vektoru eklendı\n    \n    for w, _ in close_words: #w kelimenın kendısı dıgerı ıse yakınlık oranı\n        word_labels.append(w)\n        word_vectors = np.append(word_vectors, np.array([model.wv[w]]), axis=0) #yakın kelımelerın vektorlerı de eklendı\n        #boylece  gelen kelıme ve yakın kelımeler word_labels, bunların vektorlerı ise word_vectors akta\n        \n    tsne = TSNE(random_state=0) #kelımeleı grafıge doken kutuphane\n    Y = tsne.fit_transform(word_vectors)\n    \n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n    \n    plt.scatter(x_coords, y_coords)\n    \n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(5, -2), textcoords='offset points')\n        \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6912550c8be677064c411a9483ceb93f88be902b"},"cell_type":"code","source":"closestwords_tsneplot(model, 'almanya')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"775008d989d13a35011b831744643f78f64e9a0f"},"cell_type":"code","source":"#Glove\nfrom IPython.display import Image\nImage(\"../input/glove-calismasi/glove_calsma_bicimi.PNG\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5447b11f4af54e61818ba15d12e81a59e57877f7"},"cell_type":"code","source":"def read_data(file_name):\n    with open(file_name,'r') as f:\n        word_vocab = set() # not using list to avoid duplicate entry\n        word2vector = {}\n        for line in f:\n            line_ = line.strip() #Remove white space\n            words_Vec = line_.split()\n            word_vocab.add(words_Vec[0])\n            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n    print(\"Total Words in DataSet:\",len(word_vocab))\n    return word_vocab,word2vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c01dd1f91dfbe0fbcd282e0c640fb3defd888d3"},"cell_type":"code","source":"from gensim.scripts.glove2word2vec import glove2word2vec\nfrom gensim.models import KeyedVectors # global vectors for word representations (Glove 2014)\n'''\nglove_input =  read_data('../input/glove6b100dtxt/glove.6B.100d.txt')\nword2vec_output =  read_data('../input/glove-vec/glove.6B.100d.word2vec') #word2vec yazıyor ama glove kullanılacak word2vec yazmasının amacı gensım yuklemeyı kolaylastırma\nglove2word2vec(glove_input, word2vec_output)\n\n\nmodel = KeyedVectors.load_word2vec_format(word2vec_output, binary=False)\nmodel['istanbul']\nmodel.most_similar('ankara') #bu yontem word2vec vardı ama glove 6 mılyar kelıme ıcınde daha basarılı\nmodel.most_similar(positive=['woman', 'king'], negative=['man'], topn=2)\n#topn demek sadece n tane goster\nmodel.most_similar(positive=['berlin', 'turkey'], negative=['ankara'], topn=1)\nmodel.most_similar(positive=['teach', 'doctor'], negative=['treat'], topn=1)\n\n\n'''\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}