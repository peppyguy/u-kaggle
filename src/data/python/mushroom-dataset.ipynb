{"cells":[{"metadata":{"_uuid":"f7afdf454751f7ce35983af48a7523fea6f97965"},"cell_type":"markdown","source":"<font size=5>**Mushroom Classification**</font>\n\n\n\n**About this Dataset**\n\n\nThis data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one\n\n\n**Attribute Information**\n\n\n1. cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n2. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n3. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y\n4. bruises?: bruises=t,no=f\n5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s\n6. gill-attachment: attached=a,descending=d,free=f,notched=n\n7. gill-spacing: close=c,crowded=w,distant=d\n8. gill-size: broad=b,narrow=n\n9. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y\n10. stalk-shape: enlarging=e,tapering=t\n11. stalk-root: bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,rooted=r,missing=?\n12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n14. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y\n15. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y\n16. veil-type: partial=p,universal=u\n17. veil-color: brown=n,orange=o,white=w,yellow=y\n18. ring-number: none=n,one=o,two=t\n19. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z\n20. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y\n21. population: abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y\n22. habitat: grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d"},{"metadata":{"_uuid":"a5f401604ee014f581229700c681ff685c5605da"},"cell_type":"markdown","source":"**Import all the necessary Data processing and Machine Learning libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5abf0eff3997d978fe7fb58c4affe3597d6bc9fe"},"cell_type":"markdown","source":"Read the csv file into a Panda Dataframe"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/mushrooms.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddd16c1c6f98114584bf5d09447a786416a131a9"},"cell_type":"markdown","source":"Check the sample data and do some simple analysis on the dataset"},{"metadata":{"trusted":true,"_uuid":"9f445ffa3f64acf2174a3a8bcc8058f155ea5a65"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"310b494b18ec64388ece39f275cf8a41cfe765d1"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af4509d70168080f8b4384c5c62eb2d5c60c40a8"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6280c709e317b61afa5335eae0623e30505bb2ea"},"cell_type":"markdown","source":"Check the distribution of the target class.\nHere the data is almost evenly distributed"},{"metadata":{"trusted":true,"_uuid":"2082c5cf483590742f07848e89a96db272f1afc3"},"cell_type":"code","source":"sns.countplot(x='class',data=df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce93404353aca88ceedc3d6ddef203972d855001"},"cell_type":"markdown","source":"Since sckikit-learn accepts only numeric values.Add a new column Class_code\nand assign a numeric value to both the class"},{"metadata":{"trusted":true,"_uuid":"0ccb640d6cdc15ee4a96fa119bfd56b69c0fcbb8"},"cell_type":"code","source":"lb_class = LabelEncoder()\ndf[\"class_code\"] = lb_class.fit_transform(df[\"class\"])\ndf[[\"class\", \"class_code\"]].head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"497b618da4c66a1ea7e079f5f410014a89a4689f"},"cell_type":"markdown","source":"<font size=5>**Exploratory Data Analysis**</font>\n\nLet us do some Exploratory Data Analysis on this dataset\n\nThe below graph represents the percentage of  distribution of target class (poisonous or edible) by its individual attributes.\n\nEx: Consider the bruises (bruises=t,no=f) variable,\n\nIf a sample contains bruises (bruises=t), the proabablity that it is edible is 82%"},{"metadata":{"trusted":true,"_uuid":"62ebb0673ec262147676fbadddd376a0610d7a35"},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nfig,axs=plt.subplots(nrows=8,ncols=3,figsize=(30, 75))\n\ni=0\nj=0\nk=0\n\nfor col in df.columns:\n    \n    i=int(k/3)\n    j=k%3\n    \n    axe=sns.countplot(x=col, hue=\"class\", data=df,ax=axs[i][j]) # for Seaborn version 0.7 and more\n    \n    bars = axe.patches\n    half = int(len(bars)/2)\n    left_bars = bars[:half]\n    right_bars = bars[half:]\n\n    for left, right in zip(left_bars, right_bars):\n        height_l = np.nan_to_num(left.get_height())\n        height_r = np.nan_to_num(right.get_height())\n        total = height_l + height_r\n\n        axe.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n        axe.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")\n    \n    k=k+1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a14c4c3b979ef53d5b40c745852c393ed1f0dab"},"cell_type":"markdown","source":"By above Graph we have noticed some intresting observations.\n\n* If a sample contains bruises (bruises=t), the proabablity that it is edible is 82%.\n* If a sample does not have an odor(odor=n), the proabablity that it is edible is 97%.\n* If a sample has a crowded gill-spacing(gill-spacing=w),the proabablity that it is edible is 91%.\n* If a sample has a narrow gill-size(gill-size=n), ,the proabablity that it is poisonous is 89%.\n  \n\nVarious other attributes can be used to determine wheather a sample is edible or poisonous\n"},{"metadata":{"_uuid":"9f22ef020e9d2f2a33b3e2996e31d5ff4315c7e7"},"cell_type":"markdown","source":"Check-out the all the columns present in the dataframe"},{"metadata":{"trusted":true,"_uuid":"2c0052ecee780c92f6a023fe97f536c76c9c7b42"},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f62b3473a46f61be49e524d0c0bdea4de123b864"},"cell_type":"markdown","source":"Since these are categorical variables without any definite order(nominal variables),dummy variables has to be created for all the  categorical variables.\n\nThis can be accomplished using get_dummies function from pandas"},{"metadata":{"trusted":true,"_uuid":"98c473a0e2a886e3bcb48dcf0b2dc5625956ca76"},"cell_type":"code","source":"df=pd.get_dummies(data=df,columns=[ 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n       'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n       'stalk-surface-below-ring', 'stalk-color-above-ring',\n       'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n       'ring-type', 'spore-print-color', 'population', 'habitat'],drop_first=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e5d2991a4d556f5860c1fa123c7d505b8f6faad"},"cell_type":"markdown","source":"Check out the head of the dataframe, this has created 119 columns!"},{"metadata":{"trusted":true,"_uuid":"1652de735f0540f8ab4bc0b7f0ba32aea9c0903f"},"cell_type":"code","source":"print(len(df.columns))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de82fd9f04765f6f137f1129adb76a6010289e3b"},"cell_type":"code","source":"X=df.drop(['class','class_code'],axis=1)\ny=df['class_code']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbd08c2036f829cc577505725941c057c9f89695"},"cell_type":"markdown","source":"Since this has created 119 attributes, it is highly possible that some of the attributes are co-related.One of the way to eliminate this problem and reduce the dimensionality is to use **Principle Component Analysis (PCA)**.\n\n**PCA** is a technique for feature extraction — so it combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables!\n\nAlso please note that PCA does make independent variables less interpretable.\n\nLet's try to figure out how many Principal components we need so that it captures a good variance of the dataset."},{"metadata":{"trusted":true,"_uuid":"5b8b62634466a908da340d473c973ef75dfe8f62"},"cell_type":"code","source":"n_components=[1,10,20,30,40,50,75,100]\n\nfor comp in n_components:\n    pca_comp=PCA(n_components=comp)\n    pca_comp.fit_transform(X)\n    print(comp,sum(pca_comp.explained_variance_ratio_)*100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1c79e29fd372f237fc3284a02391608202f5021"},"cell_type":"markdown","source":"Above output shows that the 1st component captures around 16% of variance in the dataset and with only 10 components we capture around 65% of variance.\n\nWith 119 columns in the original dataset, we are able to explain 65% of variance with only 10 pricipal components !.\n\nWe observe that with 40 components we capture 95% of the variance and with 50 and 75 components we capture aroud 98% and 99% of the variance.\n\n**Note:** One more important aspects to consider while using PCA is that all the variables need to be scaled.However since we are dealing with only categorical variables, i am choosing to skip this particular step as it does not provide a considerable benifit.\n\nLet's go ahead and choose the number of components as 40."},{"metadata":{"trusted":true,"_uuid":"8d8e2d844665c8b2d5304dc69b701d1cdaf3b3aa"},"cell_type":"code","source":"pca = PCA(n_components=40)\npca_x=pd.DataFrame(pca.fit_transform(X))\n\nsum(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1616ae2cb06fe720e15d2878e8a992f1850227d6"},"cell_type":"markdown","source":"Check out the head of the newly created dataframe after applying PCA.You can observe that a dataframe is created with 40 principal componets."},{"metadata":{"trusted":true,"_uuid":"9a893d3d9ecafd6bce418055a050c228cf73d2c8"},"cell_type":"code","source":"pca_x.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e761fa7cbd17e58bf9b70b85245e9f2e07df9671"},"cell_type":"markdown","source":"Do a train-test split on the newly created dataframe with test_size as 0.3"},{"metadata":{"trusted":true,"_uuid":"23e567ed40c344f9b27f760dc5183595e75ec7b2"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(pca_x,y,test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"187a61f595cd88143cf6c6153fbe81e764088763"},"cell_type":"markdown","source":"<font size=4>**Machine Learning**</font>"},{"metadata":{"trusted":true,"_uuid":"bc25f68a71048b977264bb90e478c85651df8af8"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n\nLR_model= LogisticRegression()\n\nLR_model.fit(X_train,y_train)\n\nLR_y_pred = LR_model.predict(X_test) # This will give you positive class prediction probabilities  \n\naccuracy=accuracy_score(y_test, LR_y_pred)*100\nprint(\"Accuracy Score: \",\"{0:.2f}\".format(accuracy))\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, LR_y_pred)),annot=True,fmt=\"g\", cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"304a4bc76b91376a611aba0f2ed4da7507de1a81"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nRF_model=RandomForestClassifier()\n\nRF_model.fit(X_train,y_train)\nRF_y_pred = RF_model.predict(X_test) # This will give you positive class prediction probabilities  \n\naccuracy=accuracy_score(y_test, RF_y_pred)*100\nprint(\"Accuracy Score: \",\"{0:.2f}\".format(accuracy))\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, RF_y_pred)),annot=True,fmt=\"g\", cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"525305cf8b41dd738d2d564333319dc21058bb9c"},"cell_type":"code","source":"from sklearn import svm\n\nSVM_model=svm.LinearSVC()\n\nSVM_model.fit(X_train,y_train)\nSVM_y_pred = SVM_model.predict(X_test) # This will give you positive class prediction probabilities  \n\naccuracy=accuracy_score(y_test, SVM_y_pred)*100\n\nprint(\"Accuracy Score: \",\"{0:.2f}\".format(accuracy))\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, SVM_y_pred)),annot=True,fmt=\"g\", cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8142808d6f9a8a8a768cb17a1eabfe6b4eb67a3"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_model=KNeighborsClassifier()\nknn_model.fit(X_train,y_train)\n\nknn_y_pred = knn_model.predict(X_test)  \n\naccuracy=accuracy_score(y_test, knn_y_pred)*100\n\nprint(\"Accuracy Score: \",\"{0:.2f}\".format(accuracy))\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, knn_y_pred)),annot=True,fmt=\"g\", cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f04424d60809e585119a6b05779a53fc5b258e98"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}