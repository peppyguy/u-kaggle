{"cells":[{"metadata":{"_cell_guid":"53938549-dbb6-4078-bd7f-2f48585b6252","_uuid":"0e0bc1ff47fcff9d925c375d194811d24713bb06"},"cell_type":"markdown","source":"**Importation and preprocessing**\n------------------------------------------"},{"metadata":{"_uuid":"0d130c8f9f180ed42ac373596d1d37b6e9dfab06"},"cell_type":"markdown","source":"The demonetization of ₹500 and ₹1000 banknotes was a step taken by the Government of India on 8 November 2016, ceasing the usage of all ₹500 and ₹1000 banknotes of the Mahatma Gandhi Series as a form of legal tender in India from 9 November 2016.\nThe announcement was made by the Prime Minister of India Narendra Modi in an unscheduled live televised address to the nation at 20:15 Indian Standard Time (IST) the same day. In the announcement, Modi declared circulation of all ₹500 and ₹1000 banknotes of the Mahatma Gandhi Series as invalid and announced the issuance of new ₹500 and ₹2000 banknotes of the Mahatma Gandhi New Series in exchange for the old banknotes. This analysis is done on 6000 most recent tweets on #demonetization. There are 6000 rows(one for each tweet) and 14 columns.\n\nTo begin with the analysis we are going firstly to import all of packages necessaries and secndly to display our the text of your data."},{"metadata":{"_cell_guid":"c3a7095a-e559-87db-a928-c35327d699e4","_uuid":"c75ee9b77348aed3a83d08b171bb82db224fe0af","trusted":true},"cell_type":"code","source":"import bokeh\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nfrom wordcloud import WordCloud, STOPWORDS\nimport re\nimport seaborn as sns\nfrom IPython.display import display\npd.options.mode.chained_assignment = None\nimport matplotlib\nimport re\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import plot_importance\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk import tokenize\nmatplotlib.style.use('ggplot')\n\n#Getting data\ntweets = pd.read_csv('../input/demonetization-tweets.csv', encoding = \"ISO-8859-1\")\ndisplay(tweets.text.head(10))","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"d2d6b1ec4bc94d3ec41f10ad9130277251b7b499"},"cell_type":"markdown","source":"It is necesary to clean our text in order to extract the best information. To do that we are going to create a new feature: tweetos which will contain the name of the tweetos. It seems that we can extract the name of the tweetos easily because it is between \"RT @\" and \":\"."},{"metadata":{"_cell_guid":"e0a8def2-5f6f-d07d-d004-e3f99f1ba230","_uuid":"07efb9513c0bb1efdcb9a458582a1c01450f3998","trusted":true},"cell_type":"code","source":"# Preprocessing del RT @blablabla:\ntweets['tweetos'] = '' \n\n# add tweetos first part\nfor i in range(len(tweets['text'])):\n    try:\n        tweets['tweetos'][i] = tweets['text'].str.split(' ')[i][0]\n    except AttributeError:    \n        tweets['tweetos'][i] = 'other'\n\n# Preprocessing tweetos. select tweetos contains 'RT @'\nfor i in range(len(tweets['text'])):\n    if tweets['tweetos'].str.contains('@')[i]  == False:\n        tweets['tweetos'][i] = 'other'\n        \n# remove URLs, RTs, and twitter handles\nfor i in range(len(tweets['text'])):\n    tweets['text'][i] = \" \".join([word for word in tweets['text'][i].split()\n                                if 'http' not in word and '@' not in word and '<' not in word])   \n        \ndisplay(tweets.text.head(10))","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"e2a01ac8-e5f0-1e00-b789-a52ff0067f7a","_uuid":"0397c7ad13e0debfad05f6afb0cddfb66b5a6c33"},"cell_type":"markdown","source":"**WordCloud**\n---------------------"},{"metadata":{"_cell_guid":"1926c9f8-1916-c59b-bb95-ac96355991af","_uuid":"140f888757bfb49a892d1dc8a190ba7a4e1b07f0","collapsed":true,"trusted":true},"cell_type":"code","source":"def wordcloud_by_province(tweets):\n    stopwords = set(STOPWORDS)\n    stopwords.add(\"https\")\n    stopwords.add(\"00A0\")\n    stopwords.add(\"00BD\")\n    stopwords.add(\"00B8\")\n    stopwords.add(\"ed\")\n    stopwords.add(\"demonetization\")\n    stopwords.add(\"Demonetization co\")\n    #Narendra Modi is the Prime minister of India\n    stopwords.add(\"lakh\")\n    wordcloud = WordCloud(background_color=\"white\",stopwords=stopwords,random_state = 2016).generate(\" \".join([i for i in tweets['text_new'].str.upper()]))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.title(\"Demonetization\")\n\nwordcloud_by_province(tweets)  ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a6ae675c-c21f-ac86-437a-757996e1e059","_uuid":"0e78139660adf7058a0366fe9447846b6246d10b"},"cell_type":"markdown","source":"**\"Since terrorists\"? \"Narenda Modi\"? Ok We must continue to investigate. I think that we must investigate separetly the tweets with the words \"terrorists\" and \"narendramodi\"** "},{"metadata":{"_cell_guid":"3f1fae3e-d902-e569-8e82-3c6bb8e7c0ce","_uuid":"cc155e209aacf5a0b60d08a1f11c4820cf0aa5ac"},"cell_type":"markdown","source":"**Part II: Timeseries plotting**\n--------------------------------"},{"metadata":{"_cell_guid":"3273f889-217a-e789-7ca7-bc6ecfbfc57c","_uuid":"1c55b1783c3cb6448773167e0313a9a44d0eefd8","collapsed":true,"trusted":true},"cell_type":"code","source":"print(tweets['retweetCount'].describe())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cd69f23a-555e-1493-29fd-32f3f92eeede","_uuid":"3bd33608d94e8fd43269f08dd0ba8a88e9b2c358","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets['nb_words'] = 0\nfor i in range(len(tweets['text'])):\n    tweets['nb_words'][i] = len(tweets['text'][i].split(' '))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b71ed0cf-555f-32af-979b-14620286dc52","_uuid":"8e5c7e1311389e6a272a401229e7c3b98184e29e","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets['hour'] = pd.DatetimeIndex(tweets['created']).hour\ntweets['date'] = pd.DatetimeIndex(tweets['created']).date\ntweets['minute'] = pd.DatetimeIndex(tweets['created']).minute","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a8e13028-95d7-29bd-12a9-a07321db73ef","_uuid":"2f36e9a5fce6adec414e443aabca470c567e3e9d","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets_hour = tweets.groupby(['hour'])['retweetCount'].sum()\ntweets_minute = tweets.groupby(['minute'])['retweetCount'].sum()\ntweets['text_len'] = tweets['text'].str.len()\ntweets_avgtxt_hour = tweets.groupby(['hour'])['text_len'].mean()\ntweets_avgwrd_hour = tweets.groupby(['hour'])['nb_words'].mean()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2679ff08-774c-d5f5-2a0b-751c680f48f3","_uuid":"868e7047149c75c467781e2aa49fa23e6350143a","collapsed":true,"trusted":true},"cell_type":"code","source":"import seaborn as sns\ntweets_hour.transpose().plot(kind='line',figsize=(6.5, 4))\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.title('The number of retweet by hour', bbox={'facecolor':'0.8', 'pad':0})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fdf84c1b-0ef3-8655-7ba4-b26c75027d36","_uuid":"bb93350ba6e96d7412e2de930c07fddb21540f7e","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets_minute.transpose().plot(kind='line',figsize=(6.5, 4))\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.title('The number of retweet by minute', bbox={'facecolor':'0.8', 'pad':0})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"335fc7d3-112e-f65e-a1c3-c7b9fc88e9b1","_uuid":"ca141e6cf799ae91d999b913e67f8064e9fda72d","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets_avgtxt_hour.transpose().plot(kind='line',figsize=(6.5, 4))\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.title('The Average of lenght by hour', bbox={'facecolor':'0.8', 'pad':0})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"baea8e62-2977-8121-f1bf-451c122d449e","_uuid":"26f520a17eb145417f4592aa5f8f8d54ac7132b2","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets_avgwrd_hour.transpose().plot(kind='line',figsize=(6.5, 4))\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.title('The Average number of words by hour', bbox={'facecolor':'0.8', 'pad':0})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"688b1920-4641-ca90-a581-1f3ba72ec270","_uuid":"dfef621deb5ccfce9fbda83cd95802b87b06ad15","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_stop_words(s, n):\n\t'''\n\t:s : pd.Series; each element as a list of words from tokenization\n\t:n : int; n most frequent words are judged as stop words \n\n\t:return : list; a list of stop words\n\t'''\n\tfrom collections import Counter\n\tl = get_corpus(s)\n\tl = [x for x in Counter(l).most_common(n)]\n\treturn l\n\ndef get_corpus(s):\n\t'''\n\t:s : pd.Series; each element as a list of words from tokenization\n\n\t:return : list; corpus from s\n\t'''\n\tl = []\n\ts.map(lambda x: l.extend(x))\n\treturn l\n\n#freqwords = get_stop_words(tweets['text'],n=60)\n\n#freq = [s[1] for s in freqwords]\n\n#plt.title('frequency of top 60 most frequent words', bbox={'facecolor':'0.8', 'pad':0})\n#plt.plot(freq)\n#plt.xlim([-1,60])\n#plt.ylim([0,1.1*max(freq)])\n#plt.ylabel('frequency')\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"da0d5e37-ad0f-94c6-5724-be9d773fe95b","_uuid":"c95a7f29aca5c426a67b197b873403b941c704c2"},"cell_type":"markdown","source":"**Part III: Source of tweets**\n-----------------------"},{"metadata":{"_cell_guid":"5f00c1c9-98a0-7173-8840-0408dd4ff33a","_uuid":"61589a93ff1c3997fd7168b25dd81d237a0d3d2e","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets['statusSource_new'] = ''\n\nfor i in range(len(tweets['statusSource'])):\n    m = re.search('(?<=>)(.*)', tweets['statusSource'][i])\n    try:\n        tweets['statusSource_new'][i]=m.group(0)\n    except AttributeError:\n        tweets['statusSource_new'][i]=tweets['statusSource'][i]\n        \n#print(tweets['statusSource_new'].head())   \n\ntweets['statusSource_new'] = tweets['statusSource_new'].str.replace('</a>', ' ', case=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4fdb3e18-7cef-fbad-2172-be234ca69490","_uuid":"8f608b4bc3c1b1885a3d2ab81fde4c772a622dc6","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets['statusSource_new'] = tweets['statusSource_new'].str.replace('</a>', ' ', case=False)\n#print(tweets[['statusSource_new','retweetCount']])\n\ntweets_by_type= tweets.groupby(['statusSource_new'])['retweetCount'].sum()\n#print(tweets_by_type)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f320bbcd-a3e6-79cf-5df7-ff1118a79069","_uuid":"d9d166c0523d78a9bf436f16c6f33d95fa339c6e","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets_by_type.transpose().plot(kind='bar',figsize=(10, 5))\n#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.title('Number of retweetcount by Source', bbox={'facecolor':'0.8', 'pad':0})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a1ed0e56-1f2f-4392-7d7b-d84025711ece","_uuid":"b6a45264d94665da0fff24d3b4bf33108aaff247"},"cell_type":"markdown","source":"**Top 3 of Source: 1 - Twitter For Android 2 - Twitter Web Client and finally 3 - Twitter for Iphone !**"},{"metadata":{"_cell_guid":"98670679-5f13-0a2d-7f7d-be00d7f0a3a0","_uuid":"bdcdbcff12fe43446c82fac3eda462e3d722224f","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets['statusSource_new2'] = ''\n\nfor i in range(len(tweets['statusSource_new'])):\n    if tweets['statusSource_new'][i] not in ['Twitter for Android ','Twitter Web Client ','Twitter for iPhone ']:\n        tweets['statusSource_new2'][i] = 'Others'\n    else:\n        tweets['statusSource_new2'][i] = tweets['statusSource_new'][i] \n#print(tweets['statusSource_new2'])       \n\ntweets_by_type2 = tweets.groupby(['statusSource_new2'])['retweetCount'].sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"913b1d3a-c4e7-4a60-04a4-da9dd4abe630","_uuid":"0b5889018b0d63e612238894a5388f4b7f1eefdd","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets_by_type2.rename(\"\",inplace=True)\nexplode = (0, 0, 0, 1.0)\ntweets_by_type2.transpose().plot(kind='pie',figsize=(6.5, 4),autopct='%1.1f%%',shadow=True,explode=explode)\nplt.legend(bbox_to_anchor=(1, 1), loc=6, borderaxespad=0.)\nplt.title('Number of retweetcount by Source bis', bbox={'facecolor':'0.8', 'pad':5})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b3fbc407-16f3-b936-4ac1-9691075b5448","_uuid":"c22193a282b4ca8e8575821d43e2dfdce73d3cc5"},"cell_type":"markdown","source":"**Part IV: Clustering with Kmeans**\n--------------------------"},{"metadata":{"_cell_guid":"c0650978-d249-3b91-2440-307e93106353","_uuid":"752833d8515ac78c07fb13e3407532052de5a1ff","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n####\nfrom nltk.stem import WordNetLemmatizer\n#tweets['text_sep'] = [''.join(z).strip() for z in tweets['text_new']]\ntweets['text_lem'] = [''.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in tweets['text_new']]       \n####\nvectorizer = TfidfVectorizer(max_df=0.5,max_features=10000,min_df=10,stop_words='english',use_idf=True)\nX = vectorizer.fit_transform(tweets['text_lem'].str.upper())\nprint(X.shape)\n#print(tweets['text_sep'])\n#print(tweets['text_new'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0d2fca04-933f-bc4c-0bb7-e0a3d04e4410","_uuid":"31405ae7ad3537061edf878c53eb4e9fc6472097","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=5,init='k-means++',max_iter=200,n_init=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e3fc2e97-68f7-12e4-8b55-d07f5cd8db1c","_uuid":"51242edc06d9070ddf0320cb7516a5b66daff233","collapsed":true,"trusted":true},"cell_type":"code","source":"km.fit(X)\nterms = vectorizer.get_feature_names()\norder_centroids = km.cluster_centers_.argsort()[:,::-1]\nfor i in range(5):\n    print(\"cluster %d:\" %i, end='')\n    for ind in order_centroids[i,:10]:\n        print(' %s' % terms[ind], end='')\n    print()    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"93bc2e3b-3d78-e1e6-4c47-8bab20860e8e","_uuid":"2c4eca8118bebfadb034125b59fd0fe7477a8fe3"},"cell_type":"markdown","source":"**It is possible to to improve that. We must delete the words \"https\"092d\" \"00a0\" etc...**"},{"metadata":{"_cell_guid":"bd48f605-1149-34a6-006f-ed6a9f896de3","_uuid":"c7759d5fbcb4531e15be517ed7e67b3e89b2b6d5"},"cell_type":"markdown","source":"**Part V: Correlation between numerical features**\n--------------------------------------------------"},{"metadata":{"_cell_guid":"4541f0ef-b870-7e68-7d95-15503e850b28","_uuid":"a6f04f480213a97f56c0ffc6c7aca4d40d80f083","collapsed":true,"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ntweets['favorited'] = le.fit_transform(tweets['favorited'])\ntweets['replyToSN'] = tweets['replyToSN'].fillna(-999)\ntweets['truncated'] = le.fit_transform(tweets['truncated'])\ntweets['replyToSID'] = tweets['replyToSID'].fillna(-999)\ntweets['id'] = le.fit_transform(tweets['id'])\ntweets['replyToUID'] = tweets['replyToUID'].fillna(-999)\ntweets['statusSource_new'] = le.fit_transform(tweets['statusSource_new'])\ntweets['isRetweet'] = le.fit_transform(tweets['isRetweet'])\ntweets['retweeted'] = le.fit_transform(tweets['retweeted'])\ntweets['screenName'] = le.fit_transform(tweets['screenName'])\ntweets['tweetos'] = le.fit_transform(tweets['tweetos'])\n\ntweets_num = tweets[tweets.select_dtypes(exclude=['object']).columns.values]\ntweets_num.drop('Unnamed: 0',inplace=True,axis=1)\ntweets_num.drop('retweeted',inplace=True,axis=1)\ntweets_num.drop('favorited',inplace=True,axis=1)\nprint(tweets.select_dtypes(exclude=['object']).columns.values)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2d630f42-70cc-3578-0a77-c7aaad127761","_uuid":"bb84c1202d87c5eed69ab64a12fa3ae1fa8ea6c7","collapsed":true,"trusted":true},"cell_type":"code","source":"#from string import letters\nimport seaborn as sns\n\nsns.set(style=\"white\")\n# Compute the correlation matrix\ncorr = tweets_num.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 4))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(920, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.1,\n            square=True, xticklabels=True, yticklabels=True,\n            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\nplt.title('Correlation between numerical features', bbox={'facecolor':'0.8', 'pad':0})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"56629406-c97b-f13d-def1-dd1efe5b5316","_uuid":"810d623b18b0c15a9879494dd7cab2ceedd4bc7c"},"cell_type":"markdown","source":"**Low correlation between numerical features (between 0.30 and -O.30)**"},{"metadata":{"_cell_guid":"0ee5ac8e-11bf-3278-8fc5-34784343d92c","_uuid":"a98498f54db0971008d42b99969aac0ea1983686"},"cell_type":"markdown","source":"**Part VI: Sentiment Analysis**\n-------------------------------"},{"metadata":{"_cell_guid":"39aa4adb-e54e-1fbf-fac2-dd66ca97dc7e","_uuid":"64c4cfac9cf0b5b55a48e8a83b790565b12c2263"},"cell_type":"markdown","source":"**This part is inspired by Priya Ananthram's scripts. You can look her work here:** https://www.kaggle.com/priyaananthram/d/arathee2/demonetization-in-india-twitter-data/sentiment-analysis-of-tweets"},{"metadata":{"_cell_guid":"7ca9093b-b64f-2452-64e9-ae138551dd5e","_uuid":"07fdfec714314cd1bbb5f0929bf501dc8d89f55a","collapsed":true,"trusted":true},"cell_type":"code","source":"sid = SentimentIntensityAnalyzer()\n\ntweets['sentiment_compound_polarity']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['compound'])\ntweets['sentiment_neutral']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neu'])\ntweets['sentiment_negative']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neg'])\ntweets['sentiment_pos']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['pos'])\ntweets['sentiment_type']=''\ntweets.loc[tweets.sentiment_compound_polarity>0,'sentiment_type']='POSITIVE'\ntweets.loc[tweets.sentiment_compound_polarity==0,'sentiment_type']='NEUTRAL'\ntweets.loc[tweets.sentiment_compound_polarity<0,'sentiment_type']='NEGATIVE'\ntweets.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"334df886-8bf5-d2a2-bdfc-a8e1aea7fb0c","_uuid":"f856a2c7155fbae1576b2475e740f02c2d38c372","collapsed":true,"trusted":true},"cell_type":"code","source":"matplotlib.style.use('ggplot')\n\ntweets_sentiment = tweets.groupby(['sentiment_type'])['sentiment_neutral'].count()\ntweets_sentiment.rename(\"\",inplace=True)\nexplode = (0, 0, 1.0)\nplt.subplot(221)\ntweets_sentiment.transpose().plot(kind='barh',figsize=(10, 6))\nplt.title('Sentiment Analysis 1', bbox={'facecolor':'0.8', 'pad':0})\nplt.subplot(222)\ntweets_sentiment.plot(kind='pie',figsize=(10, 6),autopct='%1.1f%%',shadow=True,explode=explode)\nplt.legend(bbox_to_anchor=(1, 1), loc=3, borderaxespad=0.)\nplt.title('Sentiment Analysis 2', bbox={'facecolor':'0.8', 'pad':0})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7231847e-17be-5d68-6e49-c04d37696418","_uuid":"84d8eba6abdb854ee966483a375a0da53719a55c","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets['count'] = 1\ntweets_filtered = tweets[['hour', 'sentiment_type', 'count']]\npivot_tweets = tweets_filtered.pivot_table(tweets_filtered, index=[\"sentiment_type\", \"hour\"], aggfunc=np.sum)\nprint(pivot_tweets.head())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9f6c362b-0bfd-3e49-6eb1-6635fdd36fa9","_uuid":"427cc71bf91ec3d2261126a7aacf4f18ae3e9d41","collapsed":true,"trusted":true},"cell_type":"code","source":"sentiment_type = pivot_tweets.index.get_level_values(0).unique()\n#f, ax = plt.subplots(2, 1, figsize=(8, 10))\nplt.setp(ax, xticks=list(range(0,24)))\n\nfor sentiment_type in sentiment_type:\n    split = pivot_tweets.xs(sentiment_type)\n    split[\"count\"].plot( legend=True, label='' + str(sentiment_type))\nplt.title('Evolution of sentiments by hour', bbox={'facecolor':'0.8', 'pad':0})    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aeef5c85-4c83-715d-b540-4b506e9d3834","_uuid":"b2f1d78527a934d11dedc67400582588f85ad71c"},"cell_type":"markdown","source":"**Part Bonus: Modelisation of retweetCount just with numerical features?**\n---------------------------------------------------------"},{"metadata":{"_cell_guid":"68b727a8-755f-f40a-1c0d-ffed2bf90298","_uuid":"202ee24fad3f2a0cea5a21a46fc12efc2052c858","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets_num_mod = tweets[tweets.select_dtypes(exclude=['object']).columns.values]\ntarget = tweets_num_mod['retweetCount']\ntweets_num_mod.drop('retweetCount',inplace=True,axis=1)\ntweets_num_mod.drop('Unnamed: 0',inplace=True,axis=1)\n\n#Just simple  and single model\nmodel_xg = XGBRegressor()\nmodel_rf = RandomForestRegressor()\nmodel_et = ExtraTreesRegressor()\nmodel_gb = GradientBoostingRegressor()\nmodel_dt = DecisionTreeRegressor()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"07db6b21-c3ec-29ad-655a-6c5a6ccba767","_uuid":"fc64d4e334c788c6840ffb0827ccf7b9e48363f3","collapsed":true,"trusted":true},"cell_type":"code","source":"scores_xg = cross_val_score(model_xg, tweets_num_mod, target, cv=5,scoring='r2')\nscores_rf = cross_val_score(model_rf, tweets_num_mod, target, cv=5,scoring='r2')\nscores_dt = cross_val_score(model_dt, tweets_num_mod, target, cv=5,scoring='r2')\nscores_et = cross_val_score(model_et, tweets_num_mod, target, cv=5,scoring='r2')\nscores_gb = cross_val_score(model_gb, tweets_num_mod, target, cv=5,scoring='r2')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"365759a1-ca0d-1abd-fd6d-878db3607dd3","_uuid":"ddb4a4f6922ba2719847d80332df0a8d3051f031","collapsed":true,"trusted":true},"cell_type":"code","source":"print(\"Mean of scores for XG:\", sum(scores_xg) / float(len(scores_xg)))\nprint(\"Mean of scores for RF:\", sum(scores_rf) / float(len(scores_rf)))\nprint(\"Mean of scores for DT:\", sum(scores_dt) / float(len(scores_dt)))\nprint(\"Mean of scores for ET:\", sum(scores_et) / float(len(scores_et)))\nprint(\"Mean of scores for gb:\", sum(scores_gb) / float(len(scores_et)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"edc08d34-7fb6-b5e4-c8d0-49a5329a069f","_uuid":"aaab8383227217c19666814185b1b8fb575189a0","collapsed":true,"trusted":true},"cell_type":"code","source":"model_xg.fit(tweets_num_mod,target)\n# plot feature importance for xgboost\nplot_importance(model_xg)\nplt.title('Feature importance', bbox={'facecolor':'0.8', 'pad':0})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4f648926-7f7f-b344-5ed9-f9b1e228b355","_uuid":"f06d327d8aa5f325ba9eba3ed7a6887a13e99413"},"cell_type":"markdown","source":"**We can see that with ExtraTreesRegressor() we have 0.8XX with the metric R2. Very good score. It's possible to improve that with more work on the feature text...**"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}