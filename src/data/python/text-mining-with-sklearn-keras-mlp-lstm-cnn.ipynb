{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "556e7862-c59b-a1e7-c8d2-91bcd7dc5b8c"
      },
      "source": [
        "Aim: Predict Rating from Review using basic and deep models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4d6a2d9e-f548-ceaf-9226-97d0ef00023c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import six\n",
        "from abc import ABCMeta\n",
        "from scipy import sparse\n",
        "from scipy.sparse import issparse\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils import check_X_y, check_array\n",
        "from sklearn.utils.extmath import safe_sparse_dot\n",
        "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from collections import defaultdict\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras import backend as K\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3764b93f-8c69-a9c2-d40a-70f164032661"
      },
      "outputs": [],
      "source": [
        "#https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "daf69378-7811-49c3-1b1c-b28aad12ad91"
      },
      "source": [
        "## Preprocessing Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2688fc8f-ca7b-9bd7-bc6e-8ea18ebdb957"
      },
      "source": [
        "Here is the process :\n",
        "* Remove the non Letters\n",
        "* Convert everything to lower case\n",
        "* Remove stop words\n",
        "* Stem the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "68c37e32-789d-1ecf-d88f-679cd79fe461"
      },
      "outputs": [],
      "source": [
        "def review_to_wordlist( review, remove_stopwords=True):\n",
        "    # Function to convert a document to a sequence of words,\n",
        "    # optionally removing stop words.  Returns a list of words.\n",
        "    #\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "\n",
        "    #\n",
        "    # 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
        "    #\n",
        "    # 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "    #\n",
        "    # 4. Optionally remove stop words (True by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "\n",
        "    b=[]\n",
        "    stemmer = english_stemmer #PorterStemmer()\n",
        "    for word in words:\n",
        "        b.append(stemmer.stem(word))\n",
        "\n",
        "    # 5. Return a list of words\n",
        "    return(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a43527ec-5612-afeb-ffdd-3b823b676e72"
      },
      "source": [
        "## Import Datas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "51e77b0c-7cd7-6ba0-53e7-c126d6ea2d6f"
      },
      "source": [
        "We import only 20000 lines of our total data in order to run the notebook faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a741e1fc-0191-c6f4-f476-52d494d17698"
      },
      "outputs": [],
      "source": [
        "data_file = '../input/Amazon_Unlocked_Mobile.csv'\n",
        "\n",
        "n = 413000  \n",
        "s = 20000 \n",
        "skip = sorted(random.sample(range(1,n),n-s))\n",
        "\n",
        "\n",
        "data = pd.read_csv( data_file, delimiter = \",\", skiprows = skip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5984c90f-8e97-8279-9cef-ee7735c75025"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cd77f712-e27a-a5c3-2892-bd8b1c2a3106"
      },
      "outputs": [],
      "source": [
        "data = data[data['Reviews'].isnull()==False]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a4b9503f-8820-40f2-31e3-a016d155bc2b"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(data, test_size = 0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d263bb83-4cf3-4f86-d30d-38902e140f30"
      },
      "source": [
        "## Labels Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "075d9e7c-236b-e7bf-d374-971404c9bd18"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data['Rating'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ba1cbbaf-2581-18c9-71bd-244c82d504b3"
      },
      "source": [
        "Much More 5 than others ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "de5cb5a7-d4cd-9854-d479-4d1bb5ab7e71"
      },
      "source": [
        "### Apply Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d7df42d-fc16-a8d0-2ac9-36ef7eca2a97"
      },
      "outputs": [],
      "source": [
        "clean_train_reviews = []\n",
        "for review in train['Reviews']:\n",
        "    clean_train_reviews.append( \" \".join(review_to_wordlist(review)))\n",
        "    \n",
        "clean_test_reviews = []\n",
        "for review in test['Reviews']:\n",
        "    clean_test_reviews.append( \" \".join(review_to_wordlist(review)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "615da5fd-a050-83f4-41c7-179d2e6d9af6"
      },
      "source": [
        "## TFidf transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8b8b704d-3214-9422-13fd-275128b1155f"
      },
      "source": [
        "### TFidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b6ed9c5d-ebd9-0a56-aeeb-3cab6ec2568a"
      },
      "source": [
        "We will use tfidf transformation with ngrams between 1 and 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ec98d8dc-f48e-4c28-15d1-d37a7e84a1cc"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer( min_df=2, max_df=0.95, max_features = 200000, ngram_range = ( 1, 4 ),\n",
        "                              sublinear_tf = True )\n",
        "\n",
        "vectorizer = vectorizer.fit(clean_train_reviews)\n",
        "train_features = vectorizer.transform(clean_train_reviews)\n",
        "\n",
        "test_features = vectorizer.transform(clean_test_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b4cebdfe-17c7-ffb7-969b-8048424a885a"
      },
      "source": [
        "### Select Best Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a68058f9-c22e-70b1-8936-70229e52a513"
      },
      "source": [
        "We only select the best features for our prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0db6ebf3-7d61-17a0-a6d1-562c50166506"
      },
      "outputs": [],
      "source": [
        "fselect = SelectKBest(chi2 , k=10000)\n",
        "train_features = fselect.fit_transform(train_features, train[\"Rating\"])\n",
        "test_features = fselect.transform(test_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d968b546-51c6-41c7-0680-3043d1ba6660"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cc3fb1cc-b41f-45fc-8855-a976bc41f8cd"
      },
      "source": [
        "### NBayes, RF, GBM, SGDClassifier ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f0f8fc18-ac27-ca7a-259a-a2eb26c0bc78"
      },
      "outputs": [],
      "source": [
        "model1 = MultinomialNB(alpha=0.001)\n",
        "model1.fit( train_features, train[\"Rating\"] )\n",
        "\n",
        "model2 = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
        "model2.fit( train_features, train[\"Rating\"] )\n",
        "\n",
        "model3 = RandomForestClassifier()\n",
        "model3.fit( train_features, train[\"Rating\"] )\n",
        "\n",
        "model4 = GradientBoostingClassifier()\n",
        "model4.fit( train_features, train[\"Rating\"] )\n",
        "\n",
        "pred_1 = model1.predict( test_features.toarray() )\n",
        "pred_2 = model2.predict( test_features.toarray() )\n",
        "pred_3 = model3.predict( test_features.toarray() )\n",
        "pred_4 = model4.predict( test_features.toarray() )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "72c6581b-7ac8-7d6c-f1b4-e3415c7aad03"
      },
      "source": [
        "### NBSVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7d5aab86-1504-c7c4-eff7-5f26065e7135"
      },
      "source": [
        "taken from https://github.com/lrei/nbsvm/blob/master/nbsvm2.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ea4e8d4f-74b8-87b2-e044-03de8a99de4d"
      },
      "outputs": [],
      "source": [
        "class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
        "\n",
        "    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n",
        "        self.alpha = alpha\n",
        "        self.max_iter = max_iter\n",
        "        self.C = C\n",
        "        self.svm_ = [] # fuggly\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X, y = check_X_y(X, y, 'csr')\n",
        "        _, n_features = X.shape\n",
        "\n",
        "        labelbin = LabelBinarizer()\n",
        "        Y = labelbin.fit_transform(y)\n",
        "        self.classes_ = labelbin.classes_\n",
        "        if Y.shape[1] == 1:\n",
        "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
        "\n",
        "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
        "        # so we don't have to cast X to floating point\n",
        "        Y = Y.astype(np.float64)\n",
        "\n",
        "        # Count raw events from data\n",
        "        n_effective_classes = Y.shape[1]\n",
        "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
        "        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n",
        "                                 dtype=np.float64)\n",
        "        self._compute_ratios(X, Y)\n",
        "\n",
        "        # flugglyness\n",
        "        for i in range(n_effective_classes):\n",
        "            X_i = X.multiply(self.ratios_[i])\n",
        "            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n",
        "            Y_i = Y[:,i]\n",
        "            svm.fit(X_i, Y_i)\n",
        "            self.svm_.append(svm) \n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_effective_classes = self.class_count_.shape[0]\n",
        "        n_examples = X.shape[0]\n",
        "\n",
        "        D = np.zeros((n_effective_classes, n_examples))\n",
        "\n",
        "        for i in range(n_effective_classes):\n",
        "            X_i = X.multiply(self.ratios_[i])\n",
        "            D[i] = self.svm_[i].decision_function(X_i)\n",
        "        \n",
        "        return self.classes_[np.argmax(D, axis=0)]\n",
        "        \n",
        "    def _compute_ratios(self, X, Y):\n",
        "        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n",
        "        if np.any((X.data if issparse(X) else X) < 0):\n",
        "            raise ValueError(\"Input X must be non-negative\")\n",
        "\n",
        "        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n",
        "        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n",
        "        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n",
        "        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n",
        "        check_array(self.ratios_)\n",
        "        self.ratios_ = sparse.csr_matrix(self.ratios_)\n",
        "\n",
        "        #p_c /= np.linalg.norm(p_c, ord=1)\n",
        "        #ratios[c] = np.log(p_c / (1 - p_c))\n",
        "\n",
        "\n",
        "def f1_class(pred, truth, class_val):\n",
        "    n = len(truth)\n",
        "\n",
        "    truth_class = 0\n",
        "    pred_class = 0\n",
        "    tp = 0\n",
        "\n",
        "    for ii in range(0, n):\n",
        "        if truth[ii] == class_val:\n",
        "            truth_class += 1\n",
        "            if truth[ii] == pred[ii]:\n",
        "                tp += 1\n",
        "                pred_class += 1\n",
        "                continue;\n",
        "        if pred[ii] == class_val:\n",
        "            pred_class += 1\n",
        "\n",
        "    precision = tp / float(pred_class)\n",
        "    recall = tp / float(truth_class)\n",
        "\n",
        "    return (2.0 * precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "def semeval_senti_f1(pred, truth, pos=2, neg=0): \n",
        "\n",
        "    f1_pos = f1_class(pred, truth, pos)\n",
        "    f1_neg = f1_class(pred, truth, neg)\n",
        "\n",
        "    return (f1_pos + f1_neg) / 2.0;\n",
        "\n",
        "\n",
        "def main(train_file, test_file, ngram=(1, 3)):\n",
        "    print('loading...')\n",
        "    train = pd.read_csv(train_file, delimiter='\\t', encoding='utf-8', header=0,\n",
        "                        names=['text', 'label'])\n",
        "\n",
        "    # to shuffle:\n",
        "    #train.iloc[np.random.permutation(len(df))]\n",
        "\n",
        "    test = pd.read_csv(test_file, delimiter='\\t', encoding='utf-8', header=0,\n",
        "                        names=['text', 'label'])\n",
        "\n",
        "    print('vectorizing...')\n",
        "    vect = CountVectorizer()\n",
        "    classifier = NBSVM()\n",
        "\n",
        "    # create pipeline\n",
        "    clf = Pipeline([('vect', vect), ('nbsvm', classifier)])\n",
        "    params = {\n",
        "        'vect__token_pattern': r\"\\S+\",\n",
        "        'vect__ngram_range': ngram, \n",
        "        'vect__binary': True\n",
        "    }\n",
        "    clf.set_params(**params)\n",
        "\n",
        "    #X_train = vect.fit_transform(train['text'])\n",
        "    #X_test = vect.transform(test['text'])\n",
        "\n",
        "    print('fitting...')\n",
        "    clf.fit(train['text'], train['label'])\n",
        "\n",
        "    print('classifying...')\n",
        "    pred = clf.predict(test['text'])\n",
        "   \n",
        "    print('testing...')\n",
        "    acc = accuracy_score(test['label'], pred)\n",
        "    f1 = semeval_senti_f1(pred, test['label'])\n",
        "    print('NBSVM: acc=%f, f1=%f' % (acc, f1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cd69a67b-3abb-6417-21d1-59ff3fc44344"
      },
      "outputs": [],
      "source": [
        "model5 = NBSVM(C=0.01)\n",
        "model5.fit( train_features, train[\"Rating\"] )\n",
        "\n",
        "pred_5 = model5.predict( test_features )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2f27a88e-53e3-4c82-319c-4bfe7272b995"
      },
      "source": [
        "## Visualize Errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "621a7de7-eb20-6331-80a5-e7a8c252b595"
      },
      "source": [
        "### Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0bf3a12a-93c8-dfba-e459-4174d05a5098"
      },
      "outputs": [],
      "source": [
        "print(classification_report(test['Rating'], pred_2, target_names=['1','2','3','4','5']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7bc7ae4e-c670-df97-1d1c-484c64efbbfa"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bb00617b-cd4f-79c7-f670-b973e46978d8"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(test['Rating'], pred_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7cffefa2-15b6-3073-8bf5-af83ca01273f"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(cnf_matrix, classes=['1','2','3','4','5'],\n",
        "                      title='Confusion matrix, without normalization')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e6a4d510-e8c2-305b-8905-8956701abaa3"
      },
      "outputs": [],
      "source": [
        "print('prediction 1 accuracy: ', accuracy_score(test['Rating'], pred_1))\n",
        "print('prediction 2 accuracy: ', accuracy_score(test['Rating'], pred_2))\n",
        "print('prediction 3 accuracy: ', accuracy_score(test['Rating'], pred_3))\n",
        "print('prediction 4 accuracy: ', accuracy_score(test['Rating'], pred_4))\n",
        "print('prediction 5 accuracy: ', accuracy_score(test['Rating'], pred_5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f0704e43-9ab0-25a4-0a8e-68d518d7887e"
      },
      "source": [
        "## Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "688a6e71-22c2-1f0e-18a1-35da16e90636"
      },
      "source": [
        "### MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6af800f5-ded6-6772-47cb-2f421477feb6"
      },
      "source": [
        "A Multilayer Perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function. (source Wikipedia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6bc1bb7a-1ee9-bf95-c248-86b3f81ac314"
      },
      "source": [
        "They can be usefull but are not the best deep models to use with unstructured data as text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d429144e-39ed-7c73-6a15-d6940aba8075"
      },
      "source": [
        "We will apply the MLP on the Tfidf Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "99ab74c0-2e18-6326-8533-c24a0d932968"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "nb_classes = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "65baef9c-1531-33bf-be3d-0dc1984b26ed"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer( min_df=2, max_df=0.95, max_features = 1000, ngram_range = ( 1, 3 ),\n",
        "                              sublinear_tf = True )\n",
        "\n",
        "vectorizer = vectorizer.fit(clean_train_reviews)\n",
        "train_features = vectorizer.transform(clean_train_reviews)\n",
        "\n",
        "test_features = vectorizer.transform(clean_test_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "18670f42-9924-c06a-af8a-ee09ce2d8950"
      },
      "outputs": [],
      "source": [
        "X_train = train_features.toarray()\n",
        "X_test = test_features.toarray()\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "y_train = np.array(train['Rating']-1)\n",
        "y_test = np.array(test['Rating']-1)\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "\n",
        "# pre-processing: divide by max and substract mean\n",
        "scale = np.max(X_train)\n",
        "X_train /= scale\n",
        "X_test /= scale\n",
        "\n",
        "mean = np.mean(X_train)\n",
        "X_train -= mean\n",
        "X_test -= mean\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "# Here's a Deep Dumb MLP (DDMLP)\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_dim=input_dim))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# we'll use categorical xent for the loss, and RMSprop as the optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "print(\"Training...\")\n",
        "model.fit(X_train, Y_train, nb_epoch=5, batch_size=16, validation_split=0.1, show_accuracy=True)\n",
        "\n",
        "print(\"Generating test predictions...\")\n",
        "preds = model.predict_classes(X_test, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "20a7e1d9-4b7a-f03c-82a6-5974504ba15d"
      },
      "outputs": [],
      "source": [
        "print('prediction 6 accuracy: ', accuracy_score(test['Rating'], preds+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6711f545-9784-2e77-4a7a-58af16519096"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dbb07555-21c1-aa73-822e-04b7ad761b94"
      },
      "source": [
        "The forward pass of a RNN is the same as the one of a MLP except that outputs from hidden layers are also used as inputs from the same layer. That means that the input from the hidden layer is both the outputs from the hidden layer one step back in time and the external input. So we have the equation:\n",
        "\n",
        "\n",
        "\\begin{equation} \n",
        "a_{h,t} = \\sum_{i}{w_{i,h}*x_{i,t}} + \\sum_{h'}{w_{h',h}*b_{h',t-1}} \n",
        "\\end{equation} \n",
        "\n",
        "\n",
        "\\begin{equation} \n",
        "b_{h,t} = \\phi_{h}(a_{h,t}) \n",
        "\\end{equation} \n",
        "\n",
        "\n",
        "where: \n",
        "* $x_{t,i} =$ value of input i at time t \n",
        "* $a_{t,j} =$ network input to unit j at time t \n",
        "* $b_{t,j}=$ output of activation of unit j at time t\n",
        "* $w_{i,h} =$  weights of the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b00f06a3-eae9-b00c-df17-6c84b7990105"
      },
      "source": [
        "Long Short Term Memory networks \u2013 usually just called LSTM \u2013 are a special kind of RNN, capable of learning long-term dependencies. (The idea behind those is to counter the vanishing problem of some basic RNN.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a45e658b-9a42-9ac9-20ca-d568988e2df4"
      },
      "source": [
        "Thus, LSTM can be very usefull in text mining problems since it involves dependencies in the sentences which can be caught in the \"memory\" of the LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "09ec7a73-b091-92e3-a24f-77a28c18dd31"
      },
      "outputs": [],
      "source": [
        "max_features = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "nb_classes = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f28a2622-6859-41e9-f98d-aec9098962e2"
      },
      "outputs": [],
      "source": [
        "# vectorize the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer(nb_words=max_features)\n",
        "tokenizer.fit_on_texts(train['Reviews'])\n",
        "sequences_train = tokenizer.texts_to_sequences(train['Reviews'])\n",
        "sequences_test = tokenizer.texts_to_sequences(test['Reviews'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c9405324-f068-a1b2-fc63-dd98a4d3a82f"
      },
      "outputs": [],
      "source": [
        "print('Pad sequences (samples x time)')\n",
        "X_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5e908847-9bcb-cf94-9805-0c25bb3f5b08"
      },
      "source": [
        "The embedding layer in text mining is really important. Indeed, it is a way to map our text input into a space (a dictionary of dimension here 128). The layer is trained through iterations (epochs) to have a better weights for the dictionary that allow to minimize the global error of the network.\n",
        "\n",
        "Skip-gram, CBOW, and GloVe (or any other word2vec variant) are pre-trained word embeddings which can be set as the weight of an embedding layer. If the weight of this layer (generally the first layer of the network) is not initialized by these pre-trained vectors, the model/network itself would assign random weights and will learn the embeddings (i.e. weights) on the fly.\n",
        "\n",
        "Explanation summarized from :\n",
        "https://github.com/fchollet/keras/issues/3110"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "36d3d256-4fa8-5664-a716-61e7af263220"
      },
      "outputs": [],
      "source": [
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128, dropout=0.2))\n",
        "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1,\n",
        "          validation_data=(X_test, Y_test))\n",
        "score, acc = model.evaluate(X_test, Y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "\n",
        "print(\"Generating test predictions...\")\n",
        "preds = model.predict_classes(X_test, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a5d0507c-d01c-11e3-70b0-ae5a790bea26"
      },
      "outputs": [],
      "source": [
        "print('prediction 7 accuracy: ', accuracy_score(test['Rating'], preds+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5468eeeb-e44b-925e-c70b-0f29e9d70f6b"
      },
      "source": [
        "add epochs and reviews to be more accurate with LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aa310c6a-414d-3781-90a5-6cb83e4da82a"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f26ea171-e228-5eb3-2f74-21754a10eead"
      },
      "source": [
        "Explanation from: https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "33881dc1-063c-97c1-7e60-8bc57c45167d"
      },
      "source": [
        "\n",
        "There are four main operations in the ConvNet :\n",
        "\n",
        "Convolution\n",
        "Non Linearity (ReLU)\n",
        "Pooling or Sub Sampling\n",
        "Classification (Fully Connected Layer)\n",
        "\n",
        "Convolution consists in applying different filters (here we chose to use 250 filters) on our input (here text). The main goal of the convolution step is to extract features from the input.\n",
        "\n",
        "The different filters allow to apply different convolutions (mathematical convolutions) on our input. As a result we obtain N (N: number of filters) convolved input after the application of our convolved layer.\n",
        "\n",
        "In order to add non linearity , we use Relu after the convolution layer. \n",
        "\n",
        "After we have pooling. Spatial Pooling reduces the dimensionality of each feature map but retains the most important information. Spatial Pooling can be of different types: Max, Average, Sum etc.\n",
        "\n",
        "The last step is the same as the MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b4f60df0-909e-6bc4-e2c1-210902655155"
      },
      "outputs": [],
      "source": [
        "nb_filter = 250\n",
        "filter_length = 3\n",
        "hidden_dims = 250\n",
        "nb_epoch = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b8f524ba-611f-67b9-5973-e1d3d7b3ee08"
      },
      "outputs": [],
      "source": [
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128, dropout=0.2))\n",
        "# we add a Convolution1D, which will learn nb_filter\n",
        "# word group filters of size filter_length:\n",
        "model.add(Convolution1D(nb_filter=nb_filter,\n",
        "                        filter_length=filter_length,\n",
        "                        border_mode='valid',\n",
        "                        activation='relu',\n",
        "                        subsample_length=1))\n",
        "\n",
        "def max_1d(X):\n",
        "    return K.max(X, axis=1)\n",
        "\n",
        "model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n",
        "model.add(Dense(hidden_dims)) \n",
        "model.add(Dropout(0.2)) \n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "109858be-7b08-d3b6-4930-d4db0f124d7d"
      },
      "outputs": [],
      "source": [
        "print('Train...')\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1,\n",
        "          validation_data=(X_test, Y_test))\n",
        "score, acc = model.evaluate(X_test, Y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "\n",
        "print(\"Generating test predictions...\")\n",
        "preds = model.predict_classes(X_test, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3d0bc468-cef3-bb40-adbb-846490cc273d"
      },
      "outputs": [],
      "source": [
        "print('prediction 8 accuracy: ', accuracy_score(test['Rating'], preds+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cedd0e04-c7ce-b276-6ce2-724752c8d5cf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}