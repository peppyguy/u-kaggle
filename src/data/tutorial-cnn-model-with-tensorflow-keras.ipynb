{"cells":[{"metadata":{"_cell_guid":"8c56a778-ba24-4d51-a89d-a16b7da3f47c","_uuid":"a04a2a1268aabc722a26e5aada6921afd8b19e2f"},"cell_type":"markdown","source":"### If you like this  kernel Greatly Appreciate if you can  UPVOTE .Thank you\n\n# Tutorial CNN Model with Tensorflow,Keras\n\n\n### Brief Context\n\n\nFashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\nThe original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try.  \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\"\n\nZalando seeks to replace the original MNIST dataset\n\n### Data Description\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n\nTo locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.\nFor example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below. \n\n### Get the Data\n\nYou can use direct links to download the dataset.\n\n| Name  | Content | Examples | Size | Link | MD5 Checksum|\n| --- | --- |--- | --- |--- |--- |\n| `train-images-idx3-ubyte.gz`  | training set images  | 60,000|26 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz)|`8d4fb7e6c68d591d4c3dfef9ec88bf0d`|\n| `train-labels-idx1-ubyte.gz`  | training set labels  |60,000|29 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz)|`25c81989df183df01b3e8a0aad5dffbe`|\n| `t10k-images-idx3-ubyte.gz`  | test set images  | 10,000|4.3 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz)|`bef4ecab320f06d8554ea6380940ec79`|\n| `t10k-labels-idx1-ubyte.gz`  | test set labels  | 10,000| 5.1 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz)|`bb300cfdad3c16e7a12a480ee83cd310`|\n\nAlternatively, you can clone this GitHub repository; the dataset appears under `data/fashion`. This repo also contains some scripts for benchmark and visualization.\n   \n```bash\ngit clone git@github.com:zalandoresearch/fashion-mnist.git\n```\n\n#### Labels\nEach training and test example is assigned to one of the following labels:\n\n* 0 T-shirt/top \n* 1 Trouser\n* 2 Pullover\n* 3 Dress\n* 4 Coat\n* 5 Sandal\n* 6 Shirt\n* 7 Sneaker\n* 8 Bag\n* 9 Ankle boot \n\n#### TL;DR\n\n* Each row is a separate image\n* Column 1 is the class label.\n* Remaining columns are pixel numbers (784 total).\n* Each value is the darkness of the pixel (1 to 255)\n\n### Acknowledgements\n\n* Original dataset was downloaded from https://github.com/zalandoresearch/fashion-mnist\n\n* Dataset was converted to CSV with this script: https://pjreddie.com/projects/mnist-in-csv/"},{"metadata":{"_cell_guid":"dec05004-ccb3-490e-b588-27c0f4f06d1e","_uuid":"41907ec74cae883fa8d56f6556cade5c67c8f3e0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from subprocess import check_output\n\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nimport pandas as pd\n\nimport numpy as np\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport keras\n\nfrom keras.models import Sequential\n\nfrom keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import TensorBoard\n\nnum_classes = 10\n\nepochs = 5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07e7dc997f3a62c3d36021550c1313a10bab4d1b"},"cell_type":"markdown","source":"**Create dataframes for train and test datasets**"},{"metadata":{"trusted":true,"_uuid":"994c029e48618920f7e363e8397712286e351ad4","_kg_hide-input":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/fashion-mnist_train.csv',sep=',')\ntest_df = pd.read_csv('/kaggle/input/fashion-mnist_test.csv', sep = ',')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1673df4108e5ac8fc4c32f05c4dd025c0db8458"},"cell_type":"markdown","source":"Let us explore the train and test data"},{"metadata":{"trusted":true,"_uuid":"42309d6099a4bb2da08538183ddba8f7a6373a62","_kg_hide-input":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea8cf38825ba66ec04a532c8012dcbdbe9d2b01d","_kg_hide-input":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e1fbecf244157e5e8b7660d812be8200882cdd2"},"cell_type":"markdown","source":"Now it is observed that the first column is the label data and because it has 10 classes so it is going to have from 0 to 9.The remaining columns are the actual pixel data.Here as you can see there are about 784 columns that contain pixel data.\nHere each row is a different image representation in the form pixel data.\n\nNow let us split the train data into x and y arrays where x represents the image data and y represents the labels.\n\nTo do that we need to convert the dataframes into numpy arrays of float32 type which is the acceptable form for tensorflow and keras."},{"metadata":{"trusted":true,"_uuid":"a6c10699c448487214315e0795284bc019b08fb5","_kg_hide-input":true},"cell_type":"code","source":"train_data = np.array(train_df, dtype = 'float32')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4823e55d2aff1e8d915383a4ffc85e0ceb9bf5e6"},"cell_type":"markdown","source":"Similarly let us do the same process for test data"},{"metadata":{"trusted":true,"_uuid":"bf132ccb32200a5b328c016e4035fa93221c5025","_kg_hide-input":true},"cell_type":"code","source":"test_data = np.array(test_df, dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2addec56cdadf3dbaa37f11819883b045baad8ca"},"cell_type":"markdown","source":"Now let us slice the train arrays into x and y arrays namely x_train,y_train to store all image data and label data respectively.\ni.e \n\n- x_train contains all the rows and all columns except the label column and excluding header info .\n- y_train contains all the rows and first column and excluding header info .\n\n\nSimilarly slice the test arrays into x and y arrays namely x_train,y_train to store all image data and label data respectively.\ni.e \n\n- x_test contains all the rows and all columns except the label column and excluding header info .\n- y_test contains all the rows and first column and excluding header info .\n\n####  Important Note : Since the image data in x_train and x_test is from 0 to 255 ,  we need to rescale this from 0 to 1.To do this we need to divide the x_train and x_test by 255 "},{"metadata":{"trusted":true,"_uuid":"d97ee0d425a0b27b2a827ea7ced09b32f160b5ef"},"cell_type":"code","source":"x_train = train_data[:,1:]/255\n\ny_train = train_data[:,0]\n\nx_test= test_data[:,1:]/255\n\ny_test=test_data[:,0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70550e4f6caf2f895e50b03fdaad358a9b25e57e"},"cell_type":"markdown","source":"Now we are gonna split the training data into validation and actual training data for training the model and testing it using the validation set. This is achieved using the train_test_split method of scikit learn library."},{"metadata":{"trusted":true,"_uuid":"b13fa2f08b0ff9a88614cafbaebf759cfb416654","_kg_hide-input":true},"cell_type":"code","source":"x_train,x_validate,y_train,y_validate = train_test_split(x_train,y_train,test_size = 0.2,random_state = 12345)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1993a73683cfcca0bef7fe602b26b382b635816"},"cell_type":"markdown","source":"Now let us visualise the sample image how it looks like in 28 * 28 pixel size"},{"metadata":{"trusted":true,"_uuid":"84617c9eda91b874568fd7c695f978715f1dfc93","_kg_hide-input":true},"cell_type":"code","source":"image = x_train[55,:].reshape((28,28))\nplt.imshow(image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c36ac2bcbbab7d9f3c160cea448cdc9f1360e26"},"cell_type":"markdown","source":"As you can observe above the shape of shoe from the sample image\n\n### Create the Convolutional Neural Networks (CNN)\n\n- #### Define the model\n\n- #### Compile the model\n\n- #### Fit the model\n\nFirst of all let us define the shape of the image before we define the model\n"},{"metadata":{"trusted":true,"_uuid":"504e5c30506b5c8e04c767450b292adebb491a96","_kg_hide-input":true},"cell_type":"code","source":"image_rows = 28\n\nimage_cols = 28\n\nbatch_size = 512\n\nimage_shape = (image_rows,image_cols,1) # Defined the shape of the image as 3d with rows and columns and 1 for the 3d visualisation","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4da137b824248b458d31259df09016144d04bc78"},"cell_type":"markdown","source":"Now we need to do more formating on the x_train,x_test and x_validate sets."},{"metadata":{"trusted":true,"_uuid":"45992f45e39c0e2170f07048415299f518503c8b","_kg_hide-input":true},"cell_type":"code","source":"x_train = x_train.reshape(x_train.shape[0],*image_shape)\nx_test = x_test.reshape(x_test.shape[0],*image_shape)\nx_validate = x_validate.reshape(x_validate.shape[0],*image_shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fededa051da1671a01614b43bdb7f4286f7a4e0f"},"cell_type":"markdown","source":"- #### Define the model "},{"metadata":{"trusted":true,"_uuid":"3e7ac49b5d3af585ba0ea700fffcc2640b90bfe0","_kg_hide-input":true},"cell_type":"code","source":"cnn_model = Sequential([\n    Conv2D(filters=32,kernel_size=3,activation='relu',input_shape = image_shape),\n    MaxPooling2D(pool_size=2) ,# down sampling the output instead of 28*28 it is 14*14\n    Dropout(0.2),\n    Flatten(), # flatten out the layers\n    Dense(32,activation='relu'),\n    Dense(10,activation = 'softmax')\n    \n])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65a718079d1fa5b6564a3edd20eb9e332d76f196"},"cell_type":"markdown","source":"- #### Compile the model"},{"metadata":{"trusted":true,"_uuid":"e4a6a6cd73c8e9f1089e24d4595985b47999b823","_kg_hide-input":true},"cell_type":"code","source":"cnn_model.compile(loss ='sparse_categorical_crossentropy', optimizer=Adam(lr=0.001),metrics =['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d5323fb34205db477e2087f1e034e1991e400c5","_kg_hide-input":true},"cell_type":"code","source":"history = cnn_model.fit(\n    x_train,\n    y_train,\n    batch_size=batch_size,\n    epochs=50,\n    verbose=1,\n    validation_data=(x_validate,y_validate),\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d4f52eccdba23e1cc1c2de369daf87381ec388a"},"cell_type":"markdown","source":"- #### Evaluate /Score the model"},{"metadata":{"_cell_guid":"4543df54-7373-49f8-a23a-6d8062eefe38","_uuid":"9de535d8a446b7168ac3790445610425527c8a47","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"score = cnn_model.evaluate(x_test,y_test,verbose=0)\nprint('Test Loss : {:.4f}'.format(score[0]))\nprint('Test Accuracy : {:.4f}'.format(score[1]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f860c207-af4d-4c32-b4d2-4ea2d39902fe","_uuid":"d18bc5dd0ec23cda8ea5ad846b9877704f484951"},"cell_type":"markdown","source":"### Results\n\n"},{"metadata":{"_cell_guid":"00dc5434-a008-490d-bb89-a394cf95c4bc","_uuid":"adbb96b3d90dcab280f06341a8483f246f583a03"},"cell_type":"markdown","source":"\nLet's plot training and validation accuracy as well as loss."},{"metadata":{"_cell_guid":"324c3a68-6381-4a39-a5a1-8fac766150c0","_uuid":"042117c73a10d7cbda6bc42ab60f9b0407cdf2c1","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\naccuracy = history.history['acc']\nval_accuracy = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'bo', label='Training Accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation Accuracy')\nplt.title('Training and Validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1311601b-9a53-4e59-adcd-4365c811f85e","_uuid":"28e5331842de7baaffb3a1e74f6a4c03e140759a"},"cell_type":"markdown","source":"### Classification Report\nWe can summarize the performance of our classifier as follows"},{"metadata":{"_cell_guid":"721514ef-520b-42a2-9d57-2f82fbae8393","_uuid":"0275d1189e6425353537fa3ebe5d97d7b6759551","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#get the predictions for the test data\npredicted_classes = cnn_model.predict_classes(x_test)\n#get the indices to be plotted\ny_true = test_df.iloc[:, 0]\ncorrect = np.nonzero(predicted_classes==y_true)[0]\nincorrect = np.nonzero(predicted_classes!=y_true)[0]\nfrom sklearn.metrics import classification_report\ntarget_names = [\"Class {}\".format(i) for i in range(num_classes)]\nprint(classification_report(y_true, predicted_classes, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dc36576e-c630-4581-89fe-3bed02c378bb","_uuid":"2863d680886cc22a2e6f11270b411e435410adaf"},"cell_type":"markdown","source":"It's apparent that our classifier is underperforming for class 6 in terms of both precision and recall. For class 2, classifier is slightly lacking precision whereas it is slightly lacking recall (i.e. missed) for class 4.\n\nPerhaps we would gain more insight after visualizing the correct and incorrect predictions."},{"metadata":{"_cell_guid":"866ef2af-d4e1-4697-bf87-9dcaa7cba363","_uuid":"6a78901cefbe9a3bdf89cc1d22ee910970cfaf93"},"cell_type":"markdown","source":"Here is a subset of correctly predicted classes."},{"metadata":{"_cell_guid":"5a33d457-ff62-4e32-81d7-89a6cdc1106d","_uuid":"d8148b3557ac27d216e0137bbeae06379c829779","trusted":true},"cell_type":"code","source":"for i, correct in enumerate(correct[:9]):\n    plt.subplot(3,3,i+1)\n    plt.imshow(x_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_true[correct]))\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9726e897-4d69-44d5-a1d5-358d316b5141","_uuid":"a22f4456fa1c09e609db1a6c0e8eb1438087fa91"},"cell_type":"markdown","source":"And here is a subset of incorrectly predicted classes."},{"metadata":{"_cell_guid":"803d7c3f-2099-420d-9612-dcf617d6cbe0","_uuid":"fa4d360a4e77bd3d83490065351860b1d9f58f8b","trusted":true},"cell_type":"code","source":"for i, incorrect in enumerate(incorrect[0:9]):\n    plt.subplot(3,3,i+1)\n    plt.imshow(x_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_true[incorrect]))\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"62bb86ddb2de0067158087ad62618fb41783972b","trusted":true},"cell_type":"markdown","source":"It looks like diversity of the similar patterns present on multiple classes effect the performance of the classifier although CNN is a robust architechture. A jacket, a shirt, and a long-sleeve blouse has similar patterns: long sleeves (or not!), buttons (or not!), and so on."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4034a8c82e307dc949d441b6111818b129495ace"},"cell_type":"markdown","source":"## If you like this kernel Greatly Appreciate if you can  UPVOTE this kernel as it greatly motivates me in contributing more to this community"}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}