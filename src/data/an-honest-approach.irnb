{"cells":[{"metadata":{"_uuid":"9211af1f386acd9745d2f4ab0c2b1166194e3103"},"cell_type":"markdown","source":"**Note: This title is only meant to poke fun and I don't mean to imply that \"leak hunting\" solutions are in any way dishonest. **\n\nHere is a kernel detailing my final submission. I used an ensemble of 8 models built on 2 sets of engineered features. My cross-validation score for this method was **1.3438** and the leaderboard score is **1.37**."},{"metadata":{"trusted":true,"_uuid":"681e629af17360bd64fa708e8e1baf818223c217"},"cell_type":"code","source":"## Dependencies\nrequire(dplyr)\nrequire(ngram)\nrequire(doMC)\nrequire(catboost)\nrequire(caret)\nrequire(glmnet)\nrequire(gbm)\nrequire(keras)\nrequire(corrplot)\n\n\n## Functions\nna_if_null <- function(x) {\n  return(ifelse(is.null(x) == TRUE, NA, x))\n}\n\nnumerify <- function(x, omit, dig) {\n    x1 <- data.matrix(x[, -omit])\n    x2 <- round(x1, dig)\n    x3 <- data.frame(cbind(x[, omit], x2))\n    return(x3)\n}\n\nRMSLE <- function(pred, act) {\n  sqrt(mean((log(pred + 1) - log(act + 1))^2))\n}\n\nfind_chunks <- function(range, size) {\n  foreach(i = 1:(length(range) + 1 - size)) %do% {\n    i:(i - 1 + size)\n  }\n}\n\nsummarize_chunk <- function(x, data) {\n  data_chunk <- data.matrix(data[, x])\n  data.frame(sum = apply(data_chunk, FUN = function(x) sum(x), 1),\n             mean = apply(data_chunk, FUN = function(x) mean(x), 1),\n             max = apply(data_chunk, FUN = function(x) max(x), 1),\n             sd = apply(data_chunk, FUN = function(x) sd(x), 1),\n             num_nz = apply(data_chunk, FUN = function(x) sum(x != 0), 1)\n             ) ->\n    new_data\n  return(new_data)\n}\n\nbuild_features_a <- function(x) {\n  try({\n    model_data <- data.matrix(x)\n    ordered <- data.matrix(x[, order])\n    data.frame(sum_total = apply(model_data, FUN = function(x) sum(x), 1),\n               mean_total = apply(model_data, FUN = function(x) mean(x), 1),\n               max_total = apply(model_data, FUN = function(x) max(x), 1),\n               sd_total = apply(model_data, FUN = function(x) sd(x), 1),\n               num_nz_total = apply(model_data, FUN = function(x) sum(x != 0), 1),\n               mean_nz_total = apply(model_data, FUN = function(x) mean(x[which(x != 0)]), 1),\n               median_nz_total = apply(model_data, FUN = function(x) median(x[which(x != 0)]), 1),\n               max_nz_total = apply(model_data, FUN = function(x) max(x[which(x != 0)]), 1),\n               min_nz_total = apply(model_data, FUN = function(x) min(x[which(x != 0)]), 1),\n               sd_nz_total = apply(model_data, FUN = function(x) sd(x[which(x != 0)]), 1),\n               most_common_nz_total_1 = apply(model_data, FUN = function(x) na_if_null(as.numeric(names(sort(table(x[which(x != 0)]), decreasing = TRUE)[1]))), 1),\n               most_common_nz_total_2 = apply(model_data, FUN = function(x) na_if_null(as.numeric(names(sort(table(x[which(x != 0)]), decreasing = TRUE)[2]))), 1),\n               most_common_nz_total_3 = apply(model_data, FUN = function(x) na_if_null(as.numeric(names(sort(table(x[which(x != 0)]), decreasing = TRUE)[3]))), 1),\n               sum_latest = apply(ordered, FUN = function(x) sum(x), 1),\n               mean_latest = apply(ordered, FUN = function(x) mean(x), 1),\n               max_latest = apply(ordered, FUN = function(x) max(x), 1),\n               sd_latest = apply(ordered, FUN = function(x) sd(x), 1),\n               num_nz_latest = apply(ordered, FUN = function(x) sum(x != 0), 1),\n               mean_nz_latest = apply(ordered, FUN = function(x) mean(x[which(x != 0)]), 1),\n               median_nz_latest = apply(ordered, FUN = function(x) median(x[which(x != 0)]), 1),\n               max_nz_latest = apply(ordered, FUN = function(x) max(x[which(x != 0)]), 1),\n               min_nz_latest = apply(ordered, FUN = function(x) min(x[which(x != 0)]), 1),\n               sd_nz_latest = apply(ordered, FUN = function(x) sd(x[which(x != 0)]), 1),\n               most_common_nz_latest_1 = apply(ordered, FUN = function(x) na_if_null(as.numeric(names(sort(table(x[which(x != 0)]), decreasing = TRUE)[1]))), 1),\n               most_common_nz_latest_2 = apply(ordered, FUN = function(x) na_if_null(as.numeric(names(sort(table(x[which(x != 0)]), decreasing = TRUE)[2]))), 1),\n               most_common_nz_latest_3 = apply(ordered, FUN = function(x) na_if_null(as.numeric(names(sort(table(x[which(x != 0)]), decreasing = TRUE)[3]))), 1),\n               slope_latest = apply(ordered, FUN = function(x) coef(lm(x ~ seq(1, length(x))))[2], 1),\n               last_value_1 = x[, order[1]],\n               last_value_2 = x[, order[2]],\n               last_value_3 = x[, order[3]]\n               ) ->\n      new_data\n    x <- data.matrix(new_data)\n    x[which(is.na(x))] <- 0\n    new_data <- data.frame(x)\n    new_data$max_nz_latest[which(!is.finite(new_data$max_nz_latest))] <- 0\n    new_data$min_nz_latest[which(!is.finite(new_data$min_nz_latest))] <- 0\n    new_data$range_nz_total <- new_data$max_nz_total - new_data$min_nz_total\n    new_data$range_nz_latest <- new_data$max_nz_latest - new_data$min_nz_latest\n  }, silent = TRUE)\n  return(new_data)\n}\n\nbuild_features_b <- function(x) {\n  ordered <- data.matrix(x[, order])\n  foreach(s = 1:ncol(ordered), .combine = c) %do% {\n    find_chunks(1:ncol(ordered), s)\n  } -> \n    chunks\n  do.call(\"cbind\", \n          lapply(chunks, \n                 summarize_chunk, \n                 data = ordered\n                 )\n          ) %>%\n    data.frame() ->\n    new_data\n  x <- data.matrix(new_data)\n  x[which(is.na(x))] <- 0\n  new_data <- data.frame(x)\n  return(new_data)\n}\n\nbuild_pca <- function(x, c, new_x = NULL) {\n  model_data <- data.matrix(x)\n  preproc <- preProcess(model_data, method = c(\"zv\", \"pca\"), pcaComp = c)\n  if(is.null(new_x)) {\n    new_data <- predict(preproc, model_data)\n  } else {\n    new_data <- predict(preproc, data.matrix(new_x))\n  }\n  return(new_data)\n}\n\nbagged_catboost <- function(xtrain, ytrain, xtest, n_models, boot_size = 1.00, params, verbose = TRUE) {\n  foreach(j = 1:n_models, .combine = cbind) %do% {\n    if(verbose) {cat(\"Training CatBoost model \", j, \"... \", sep = \"\")}\n    rm(.Random.seed, envir=globalenv())\n    inner_seed <- sample(1:nrow(xtrain), boot_size*nrow(xtrain), replace = TRUE)\n    cat_train <- catboost.load_pool(data.matrix(xtrain[-inner_seed, ]), label = ytrain[-inner_seed])\n    cat_eval <- catboost.load_pool(data.matrix(xtrain[inner_seed, ]), label = ytrain[inner_seed])\n    cat_test <- catboost.load_pool(data.matrix(xtest))\n    model <- catboost.train(cat_train,\n                            cat_eval,\n                            params\n                            )\n    pred <- exp(catboost.predict(model, cat_test))\n    pred[which(pred < 0)] <- 0\n    if(verbose) {cat(\"Done!\\n\")}\n    pred\n  } %>%\n    data.frame() %>%\n    rowMeans() ->\n    pred\n  return(pred)\n}\n\ncv_bagged_catboost <- function(xtrain, ytrain, xtest, n_folds, fold_size, n_models, boot_size = 1.00, params, verbose = TRUE) {\n  foreach(i = 1:n_folds, .combine = rbind) %do% {\n    if(verbose) {cat(\"Fold \", i, \"\\n\", sep = \"\")}\n    rm(.Random.seed, envir=globalenv())\n    seed <- sample(1:nrow(xtrain), fold_size*nrow(xtrain), replace = FALSE)\n    xtrain_i <- xtrain[-seed, ]\n    ytrain_i <- log(ytrain[-seed])\n    xtest_i <- xtrain[seed, ]\n    ytest_i <- ytrain[seed]\n    pred <- bagged_catboost(xtrain_i, ytrain_i, xtest_i, n_models, boot_size, params, verbose)\n    rmse <- RMSE(pred, ytest_i)\n    rmsle <- RMSLE(pred, ytest_i)\n    if(verbose) {\n      cat(\"RMSE: \",\n          rmse,\n          \" || RMSLE: \",\n          rmsle,\n          \"\\n\",\n          sep = \"\"\n          )\n    }\n    data.frame(fold = i,\n               rmse = rmse,\n               rmsle = rmsle\n               )\n  } %>%\n    data.frame() ->\n    cv_out\n  return(cv_out)\n}\n\nbagged_linear <- function(xtrain, ytrain, xtest, n_models, boot_size = 1.00, col_size, params, verbose = TRUE, print_every) {\n  foreach(j = 1:n_models, .combine = cbind) %do% {\n    if(verbose & j %% print_every == 0) {cat(\"Training GLM \", j, \"... \", sep = \"\")}\n    rm(.Random.seed, envir=globalenv())\n    seed <- sample(1:nrow(xtrain), boot_size*nrow(xtrain), replace = TRUE)\n    rm(.Random.seed, envir=globalenv())\n    col_index <- sample(1:ncol(xtrain), col_size*ncol(xtrain), replace = FALSE)\n    xboot <- xtrain[seed, col_index]\n    yboot <- ytrain[seed]\n    model <- glmnet(data.matrix(xboot),\n                       yboot,\n                       family = \"gaussian\",\n                       alpha = params$alpha,\n                       lambda = params$lambda\n                       )\n    pred <- exp(predict.glmnet(model, data.matrix(xtest[, col_index]), type = \"response\"))\n    pred[which(pred < 30000)] <- 30000\n    pred[which(pred > 40000000)] <- 40000000\n    if(verbose & j %% print_every == 0) {cat(\"Done!\\n\")}\n    pred\n  } %>%\n    data.frame() %>%\n    rowMeans() ->\n    pred\n  return(pred)\n}\n\ncv_bagged_linear <- function(xtrain, ytrain, xtest, n_folds, fold_size, n_models, boot_size = 1.00, col_size, params, verbose = TRUE, print_every) {\n  foreach(i = 1:n_folds, .combine = rbind) %do% {\n    if(verbose) {cat(\"Fold \", i, \"\\n\", sep = \"\")}\n    rm(.Random.seed, envir=globalenv())\n    seed <- sample(1:nrow(xtrain), fold_size*nrow(xtrain), replace = FALSE)\n    xtrain_i <- xtrain[-seed, ]\n    ytrain_i <- log(ytrain[-seed])\n    xtest_i <- xtrain[seed, ]\n    ytest_i <- ytrain[seed]\n    pred <- bagged_linear(xtrain_i, ytrain_i, xtest_i, n_models, boot_size, col_size, params, verbose, print_every)\n    rmse <- RMSE(pred, ytest_i)\n    rmsle <- RMSLE(pred, ytest_i)\n    if(verbose) {\n      cat(\"RMSE: \",\n          rmse,\n          \" || RMSLE: \",\n          rmsle,\n          \"\\n\",\n          sep = \"\"\n          )\n    }\n    data.frame(fold = i,\n               rmse = rmse,\n               rmsle = rmsle\n               )\n  } %>%\n    data.frame() ->\n    cv_out\n  return(cv_out)\n}\n\nadaboost <- function(xtrain, ytrain, xtest, params, verbose = TRUE) {\n  if(verbose) {cat(\"Training Adaboost model\", \"... \", sep = \"\")}\n  model <- gbm.fit(x = xtrain,\n                   y = ytrain,\n                   distribution = \"gaussian\",\n                   n.trees = params$n.trees,\n                   interaction.depth = params$interaction.depth,\n                   shrinkage = params$shrinkage,\n                   n.minobsinnode = params$n.minobsinnode,\n                   nTrain = params$nTrain,\n                   verbose = params$verbose\n                   )\n  pred <- exp(predict(model, data.matrix(xtest), n.trees = params$n.trees))\n  pred[which(pred < 0)] <- 0\n  if(verbose) {cat(\"Done!\\n\")}\n  pred\n}\n\ncv_adaboost <- function(xtrain, ytrain, xtest, n_folds, fold_size, params, verbose = TRUE) {\n  foreach(i = 1:n_folds, .combine = rbind) %do% {\n    if(verbose) {cat(\"Fold \", i, \"\\n\", sep = \"\")}\n    rm(.Random.seed, envir=globalenv())\n    seed <- sample(1:nrow(xtrain), fold_size*nrow(xtrain), replace = FALSE)\n    xtrain_i <- xtrain[-seed, ]\n    ytrain_i <- log(ytrain[-seed])\n    xtest_i <- xtrain[seed, ]\n    ytest_i <- ytrain[seed]\n    pred <- adaboost(xtrain_i, ytrain_i, xtest_i, params, verbose)\n    rmse <- RMSE(pred, ytest_i)\n    rmsle <- RMSLE(pred, ytest_i)\n    if(verbose) {\n      cat(\"RMSE: \",\n          rmse,\n          \" || RMSLE: \",\n          rmsle,\n          \"\\n\",\n          sep = \"\"\n          )\n    }\n    data.frame(fold = i,\n               rmse = rmse,\n               rmsle = rmsle\n               )\n  } %>%\n    data.frame() ->\n    cv_out\n  return(cv_out)\n}\n\nbuild_architecture <- function(input_shape, output_shape, activation = \"relu\", hidden, dropout_where = Inf, dropout_percent, regularize = TRUE, init_weights) {\n  input <- layer_input(shape = input_shape)\n  arch <- input\n  layers <- 0\n  while(layers < length(hidden)) {\n    arch %>%\n      layer_dense(units = hidden[(layers + 1)], activation = activation, weights = list(init_weights[[(layers + 1)]], init_weights[[(layers + 2)]])) ->\n      arch\n    if((layers + 1) == dropout_where) {\n      arch %>%\n        layer_dropout(dropout_percent) ->\n        arch\n    }\n    layers <- layers + 1\n  }\n  arch %>%\n    layer_dense(units = output_shape, weights = list(init_weights[[(length(init_weights) - 1)]], init_weights[[length(init_weights)]])) ->\n    arch\n  if(regularize) {\n    arch %>%\n      layer_activity_regularization() ->\n      arch\n  }\n  arch <- keras_model(inputs = list(input), outputs = arch)\n  return(arch)\n}\n\nmlp <- function(xtrain, ytrain, xtest, architecture, init_weights, params, verbose = TRUE) {\n  if(verbose) {cat(\"Training MLP\", \"... \", sep = \"\")}\n  arch <- build_architecture(architecture$input_shape,\n                             architecture$output_shape,\n                             architecture$activation,\n                             architecture$hidden,\n                             architecture$dropout_where,\n                             architecture$dropout_percent,\n                             architecture$regularize,\n                             architecture$init_weights\n                             )\n  arch %>%\n    compile(loss = params$loss,\n            optimizer = optimizer_sgd(lr = params$lr,\n                                      momentum = params$momentum,\n                                      decay = params$decay,\n                                      nesterov = params$nesterov,\n                                      clipnorm = params$clipnorm\n                                      )\n            )\n  \n  arch %>%\n    fit(xtrain,\n        ytrain,\n        epochs = params$epochs,\n        verbose = params$verbose,\n        batch_size = params$batch_size,\n        validation_split = params$validation_split,\n        callbacks = list(callback_early_stopping(monitor = \"val_loss\",\n                                                 patience = params$patience\n                                                 )\n                         )\n        )\n  \n  pred <- exp(predict(arch, xtest, batch_size = params$batch_size))\n  rm(list = \"arch\")\n  return(pred)\n}\n\ncv_mlp <- function(xtrain, ytrain, xtest, n_folds, fold_size, architecture, init_weights, params, verbose = TRUE) {\n  foreach(i = 1:n_folds, .combine = rbind) %do% {\n    if(verbose) {cat(\"Fold \", i, \"\\n\", sep = \"\")}\n    rm(.Random.seed, envir=globalenv())\n    seed <- sample(1:nrow(xtrain), fold_size*nrow(xtrain), replace = FALSE)\n    xtrain_i <- data.matrix(xtrain[-seed, ])\n    ytrain_i <- log(ytrain[-seed])\n    xtest_i <- data.matrix(xtrain[seed, ])\n    ytest_i <- ytrain[seed]\n    pred <- mlp(xtrain_i, ytrain_i, xtest_i, architecture, init_weights, params, verbose)\n    rmse <- RMSE(pred, ytest_i)\n    rmsle <- RMSLE(pred, ytest_i)\n    if(verbose) {\n      cat(\"RMSE: \",\n          rmse,\n          \" || RMSLE: \",\n          rmsle,\n          \"\\n\",\n          sep = \"\"\n          )\n    }\n    data.frame(fold = i,\n               rmse = rmse,\n               rmsle = rmsle\n               )\n  } %>%\n    data.frame() ->\n    cv_out\n  return(cv_out)\n}\n\ncatboost_catboost <- function(xtrain_1, xtrain_2, ytrain, xtest_1, xtest_2, params_1, params_2, verbose = TRUE) {\n  if(verbose) {cat(\"Training Layer 1 CatBoost Model...\")}\n  rm(.Random.seed, envir=globalenv())\n  seed <- sample(1:nrow(xtrain_1), 0.1*nrow(xtrain_1), replace = FALSE)\n  cat_train <- catboost.load_pool(data.matrix(xtrain_1[-seed, ]), label = ytrain[-seed])\n  cat_eval <- catboost.load_pool(data.matrix(xtrain_1[seed, ]), label = ytrain[seed])\n  cat_test <- catboost.load_pool(data.matrix(xtest_1))\n  model_1 <- catboost.train(cat_train,\n                            cat_eval,\n                            params_1\n                            )\n  test_pred_1 <- catboost.predict(model_1, cat_test)\n  train_pred <- catboost.predict(model_1, catboost.load_pool(data.matrix(xtrain_1)))\n  if(verbose) {cat(\"Done!\\n\")}\n  resids <- ytrain - train_pred\n  if(verbose) {cat(\"Training Layer 2 CatBoost Model...\")}\n  rm(.Random.seed, envir=globalenv())\n  seed <- sample(1:nrow(xtrain_2), 0.1*nrow(xtrain_2), replace = FALSE)\n  cat_train <- catboost.load_pool(data.matrix(xtrain_2[-seed, ]), label = resids[-seed])\n  cat_eval <- catboost.load_pool(data.matrix(xtrain_2[seed, ]), label = resids[seed])\n  cat_test <- catboost.load_pool(data.matrix(xtest_2))\n  model_2 <- catboost.train(cat_train,\n                            cat_eval,\n                            params_2\n                            )\n  test_pred_2 <- catboost.predict(model_2, cat_test)\n  new_pred <- exp((test_pred_1 + test_pred_2))\n  new_pred[which(new_pred < 0)] <- 0\n  if(verbose) {cat(\"Done!\\n\")}\n  return(new_pred)\n}\n\ncv_catboost_catboost <- function(xtrain_1, xtrain_2, ytrain, xtest_1, xtest_2, n_folds, fold_size, params_1, params_2, verbose = TRUE) {\n  foreach(i = 1:n_folds, .combine = rbind) %do% {\n    if(verbose) {cat(\"Fold \", i, \"\\n\", sep = \"\")}\n    rm(.Random.seed, envir=globalenv())\n    seed <- sample(1:nrow(xtrain_1), fold_size*nrow(xtrain_1), replace = FALSE)\n    xtrain_1_i <- xtrain_1[-seed, ]\n    xtrain_2_i <- xtrain_2[-seed, ]\n    ytrain_i <- log(ytrain[-seed])\n    xtest_1_i <- xtrain_1[seed, ]\n    xtest_2_i <- xtrain_2[seed, ]\n    ytest_i <- ytrain[seed]\n    pred <- catboost_catboost(xtrain_1_i, xtrain_2_i, ytrain_i, xtest_1_i, xtest_2_i, params_1, params_2, verbose)\n    rmse <- RMSE(pred, ytest_i)\n    rmsle <- RMSLE(pred, ytest_i)\n    if(verbose) {\n      cat(\"RMSE: \",\n          rmse,\n          \" || RMSLE: \",\n          rmsle,\n          \"\\n\",\n          sep = \"\"\n          )\n    }\n    data.frame(fold = i,\n               rmse = rmse,\n               rmsle = rmsle\n               )\n  } %>%\n    data.frame() ->\n    cv_out\n  return(cv_out)\n}\n\nadaboost_catboost <- function(xtrain_1, xtrain_2, ytrain, xtest_1, xtest_2, params_1, params_2, verbose = TRUE) {\n  if(verbose) {cat(\"Training Layer 1 Adaboost Model...\")}\n  model_1 <- gbm.fit(x = xtrain_1,\n                     y = ytrain,\n                     distribution = \"gaussian\",\n                     n.trees = params_1$n.trees,\n                     interaction.depth = params_1$interaction.depth,\n                     shrinkage = params_1$shrinkage,\n                     n.minobsinnode = params_1$n.minobsinnode,\n                     nTrain = params_1$nTrain,\n                     verbose = params_1$verbose\n                     )\n  test_pred_1 <- predict(model_1, data.matrix(xtest_1), n.trees = params_1$n.trees)\n  train_pred <- predict(model_1, data.matrix(xtrain_1), n.trees = params_1$n.trees)\n  if(verbose) {cat(\"Done!\\n\")}\n  resids <- ytrain - train_pred\n  if(verbose) {cat(\"Training Layer 2 CatBoost Model...\")}\n  rm(.Random.seed, envir=globalenv())\n  seed <- sample(1:nrow(xtrain_2), 0.1*nrow(xtrain_2), replace = FALSE)\n  cat_train <- catboost.load_pool(data.matrix(xtrain_2[-seed, ]), label = resids[-seed])\n  cat_eval <- catboost.load_pool(data.matrix(xtrain_2[seed, ]), label = resids[seed])\n  cat_test <- catboost.load_pool(data.matrix(xtest_2))\n  model_2 <- catboost.train(cat_train,\n                            cat_eval,\n                            params_2\n                            )\n  test_pred_2 <- catboost.predict(model_2, cat_test)\n  new_pred <- exp((test_pred_1 + test_pred_2))\n  new_pred[which(new_pred < 0)] <- 0\n  if(verbose) {cat(\"Done!\\n\")}\n  return(new_pred)\n}\n\ncv_adaboost_catboost <- function(xtrain_1, xtrain_2, ytrain, xtest_1, xtest_2, n_folds, fold_size, params_1, params_2, verbose = TRUE) {\n  foreach(i = 1:n_folds, .combine = rbind) %do% {\n    if(verbose) {cat(\"Fold \", i, \"\\n\", sep = \"\")}\n    rm(.Random.seed, envir=globalenv())\n    seed <- sample(1:nrow(xtrain_1), fold_size*nrow(xtrain_1), replace = FALSE)\n    xtrain_1_i <- xtrain_1[-seed, ]\n    xtrain_2_i <- xtrain_2[-seed, ]\n    ytrain_i <- log(ytrain[-seed])\n    xtest_1_i <- xtrain_1[seed, ]\n    xtest_2_i <- xtrain_2[seed, ]\n    ytest_i <- ytrain[seed]\n    pred <- adaboost_catboost(xtrain_1_i, xtrain_2_i, ytrain_i, xtest_1_i, xtest_2_i, params_1, params_2, verbose)\n    rmse <- RMSE(pred, ytest_i)\n    rmsle <- RMSLE(pred, ytest_i)\n    if(verbose) {\n      cat(\"RMSE: \",\n          rmse,\n          \" || RMSLE: \",\n          rmsle,\n          \"\\n\",\n          sep = \"\"\n          )\n    }\n    data.frame(fold = i,\n               rmse = rmse,\n               rmsle = rmsle\n               )\n  } %>%\n    data.frame() ->\n    cv_out\n  return(cv_out)\n}\n\ncross_predict <- function(xtrain_a, xtrain_b, xtrain_pca_a, xtrain_pca_b, train_orig, nfolds = 6, shuffle = TRUE, verbose = TRUE) {\n  if(shuffle) {\n    rm(.Random.seed, envir=globalenv())\n    seed <- sample(1:nrow(train_orig), nrow(train_orig), replace = FALSE)\n    xtrain_a <- xtrain_a[seed, ]\n    xtrain_b <- xtrain_b[seed, ]\n    xtrain_pca_a <- xtrain_pca_a[seed, ]\n    xtrain_pca_b <- xtrain_pca_b[seed, ]\n    train_orig <- train_orig[seed, ]\n  }\n  chunks <- cut(1:nrow(train_orig), nfolds, labels = 1:nfolds)\n  first_layer <- train_orig[, c(1, 2)]\n  foreach(i = unique(chunks), .combine = rbind) %do% {\n    if(verbose) {cat(\"Predicting on fold\", i, \"\\n\")}\n    train_a <- xtrain_a[which(chunks != i), ]\n    train_b <- xtrain_b[which(chunks != i), ]\n    train_pca_a <- xtrain_pca_a[which(chunks != i), ]\n    train_pca_b <- xtrain_pca_b[which(chunks != i), ]\n    test_a <- xtrain_a[which(chunks == i), ]\n    test_b <- xtrain_b[which(chunks == i), ]\n    test_pca_a <- xtrain_pca_a[which(chunks == i), ]\n    test_pca_b <- xtrain_pca_b[which(chunks == i), ]\n    ytrain <- log(train_orig$target[which(chunks != i)])\n    first_layer_i <- first_layer[which(chunks == i), ]\n    if(verbose) {cat(\"Training model 1...\")}\n    params <- list(use_best_model = TRUE,\n                   loss_function = \"RMSE\",\n                   eval_metric = \"RMSE\",\n                   iterations = 500000,\n                   depth = 6,\n                   learning_rate = 0.0001,\n                   l2_leaf_reg = 3.5,\n                   rsm = 0.50,\n                   od_type = \"Iter\",\n                   od_wait = 2000,\n                   bagging_temperature = 1,\n                   thread_count = 12,\n                   logging_level = \"Silent\"\n                   )\n    pred <- bagged_catboost(train_a, ytrain, test_a, n_models = 10, boot_size = 1.00, params = params, verbose = FALSE)\n    first_layer_i$pred_1 <- pred\n    if(verbose) {cat(\"Done!\\n\")}\n    if(verbose) {cat(\"Training model 2...\")}\n    params <- list(\"alpha\" = 0.0,\n                   \"lambda\" = 0.03\n                   )\n    pred <- bagged_linear(train_a, ytrain, test_a, n_models = 2000, boot_size = 0.25, col_size = 0.5, params = params, verbose = FALSE, print_every = 10)\n    first_layer_i$pred_2 <- pred\n    if(verbose) {cat(\"Done!\\n\")}\n    if(verbose) {cat(\"Training model 3...\")}\n    params <-  list(\"n.trees\" = 10000,\n                    \"interaction.depth\" = 2,\n                    \"shrinkage\" = 0.001,\n                    \"n.minobsinnode\" = 10,\n                    \"nTrain\" = NULL,\n                    \"verbose\" = FALSE\n                    )\n    pred <- adaboost(train_a, ytrain, test_a, params = params, verbose = FALSE)\n    first_layer_i$pred_3 <- pred\n    if(verbose) {cat(\"Done!\\n\")}\n    if(verbose) {cat(\"Training model 4...\")}\n    params = list(use_best_model = TRUE,\n                  loss_function = \"RMSE\",\n                  eval_metric = \"RMSE\",\n                  iterations = 75000,\n                  depth = 6,\n                  learning_rate = 0.001,\n                  l2_leaf_reg = 3.5,\n                  rsm = 0.50,\n                  od_type = \"Iter\",\n                  od_wait = 2000,\n                  bagging_temperature = 1,\n                  thread_count = 12,\n                  logging_level = \"Silent\",\n                  metric_period = 1000\n                  )\n    pred <- bagged_catboost(train_pca_a[, 1:8], ytrain, test_pca_a[, 1:8], n_models = 10, boot_size = 1.00, params = params, verbose = FALSE)\n    first_layer_i$pred_4 <- pred\n    if(verbose) {cat(\"Done!\\n\")}\n    if(verbose) {cat(\"Training model 5...\")}\n    params <- list(use_best_model = TRUE,\n                   loss_function = \"RMSE\",\n                   eval_metric = \"RMSE\",\n                   iterations = 20000,\n                   depth = 6,\n                   learning_rate = 0.0025,\n                   l2_leaf_reg = 3.5,\n                   rsm = 0.20,\n                   od_type = \"Iter\",\n                   od_wait = 1000,\n                   bagging_temperature = 1,\n                   thread_count = 12,\n                   logging_level = \"Silent\"\n                   )\n    pred <- bagged_catboost(train_b, ytrain, test_b, n_models = 10, boot_size = 1.00, params = params, verbose = FALSE)\n    first_layer_i$pred_5 <- pred\n    if(verbose) {cat(\"Done!\\n\")}\n    if(verbose) {cat(\"Training model 6...\")}\n    params <- list(\"alpha\" = 1.0,\n                   \"lambda\" = 0.03\n                   )\n    pred <- bagged_linear(train_b, ytrain, test_b, n_models = 100, boot_size = 1.00, col_size = 0.1, params = params, verbose = FALSE, print_every = 10)\n    first_layer_i$pred_6 <- pred\n    if(verbose) {cat(\"Done!\\n\")}\n    if(verbose) {cat(\"Training model 7...\")}\n    params <-  list(\"n.trees\" = 500,\n                    \"interaction.depth\" = 2,\n                    \"shrinkage\" = 0.05,\n                    \"n.minobsinnode\" = 10,\n                    \"nTrain\" = NULL,\n                    \"verbose\" = FALSE\n                    )\n    pred <- adaboost(train_b, ytrain, test_b, params = params, verbose = FALSE)\n    first_layer_i$pred_7 <- pred\n    if(verbose) {cat(\"Done!\\n\")}\n    if(verbose) {cat(\"Training model 8...\")}\n    params = list(use_best_model = TRUE,\n                  loss_function = \"RMSE\",\n                  eval_metric = \"RMSE\",\n                  iterations = 100000,\n                  depth = 6,\n                  learning_rate = 0.001,\n                  l2_leaf_reg = 3.5,\n                  rsm = 0.50,\n                  od_type = \"Iter\",\n                  od_wait = 2000,\n                  bagging_temperature = 1,\n                  thread_count = 12,\n                  logging_level = \"Silent\"\n                  )\n    pred <- bagged_catboost(train_pca_b[, 1:18], ytrain, test_pca_b[, 1:18], n_models = 10, boot_size = 1.00, params = params, verbose = FALSE)\n    first_layer_i$pred_8 <- pred\n    if(verbose) {cat(\"Done!\\n\")}\n    return(first_layer_i)\n    } %>%\n    data.frame() ->\n    first_layer\n  return(first_layer)\n}\n\nwmean_ensemble <- function(params, xtrain, ytrain) {\n  pred <- apply(xtrain, FUN = weighted.mean, w = params, MARGIN = 1)\n  RMSLE(pred, ytrain)\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39e9ef5d13f5987501805afc2c12270813344bfc"},"cell_type":"markdown","source":"Both feature sets make use of the *column order* discovered by [Giba](https://www.kaggle.com/titericz/the-property-by-giba) and extended as shown [here](https://www.kaggle.com/mannyelk/the-order-of-things):"},{"metadata":{"trusted":true,"_uuid":"ec3f9ca690b7f4dd4a10cffc043db30b96f6885f"},"cell_type":"code","source":"## Column Order\norder <- c('f190486d6', \n           'X58e2e02e6', \n           'eeb9cd3aa', \n           'X9fd594eec', \n           'X6eef030c1', \n           'X15ace8c9f', \n           'fb0f5dbfe', \n           'X58e056e12', \n           'X20aa07010', \n           'X024c577b9', \n           'd6bb78916', \n           'b43a7cfd5', \n           'X58232a6fb', \n           'X1702b5bf0', \n           'X324921c7b', \n           'X62e59a501', \n           'X2ec5b290f', \n           'X241f0f867', \n           'fb49e4212', \n           'X66ace2992', \n           'f74e8f13d', \n           'X5c6487af1', \n           'X963a49cdc', \n           'X26fc93eb7', \n           'X1931ccfdd', \n           'X703885424', \n           'X70feb1494', \n           'X491b9ee45', \n           'X23310aa6f', \n           'e176a204a', \n           'X6619d81fc', \n           'X1db387535',\n           'fc99f9426',\n           'X91f701ba2',\n           'X0572565c2',\n           'X190db8488',\n           'adb64ff71',\n           'c47340d97',\n           'c5a231d81',\n           'X0ff32eb98'\n           )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3447ce15be07380df1a7d2a3b97510d2060109f6"},"cell_type":"markdown","source":"First, the data is loaded and quickly tidied. "},{"metadata":{"trusted":true,"_uuid":"0cb03c6ba283f8820b57022b795c38de6bd60420"},"cell_type":"code","source":"## Load Data\ncat(\"Loading data...\")\n\ntrain_data <- read.csv(\"../input/santander-value-prediction-challenge/train.csv\")\ntest_data <- read.csv(\"../input/santander-value-prediction-challenge/test.csv\")\nsample_submission <- read.csv(\"../input/santander-value-prediction-challenge/sample_submission.csv\")\n\ncat(\"Done!\\n\")\n\n\n## Clean Data\ncat(\"Cleaning data...\")\n\n# Disable scientific notation\noptions(scipen = 999)\n\n# Numerify and round\ntrain_data <- numerify(train_data, c(1, 2), 0)\ntest_data <- numerify(test_data, 1, 0)\n\ncat(\"Done!\\n\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"158d01288fc45a2f94c2ad1ef778d52e224d4a2e"},"cell_type":"markdown","source":"Two feature sets **A** and **B** are created from summary statistics computed rowwise on both the training and testing data. Set A contains features making use of all columns (*total*) as well as features making use of only the 40 columns belonging to Giba's property (*latest*). Further, many of these summary statistics are compiled from non-zero values contained in a given row (in addition to or instead of these same statistics computed using all values contained in a row). Set B contains features making use of only the 40 ordered columns. Summary statistics (as in set A) are computed from each continuous fragment that can be taken from the column order (i.e: values 3 to 9, values 1 to 39, value 18 are all valid). Additionally, dimensionally reduced representations of both sets A and B were created using **Principal Component Analysis**. 12 components were used for set A and 48 were used for set B."},{"metadata":{"trusted":true,"_uuid":"641449535df3c682a8b276bf3277ed806b060b69"},"cell_type":"code","source":"## Build Feature Sets\ncat(\"Building feature sets...\\n\")\nxtrain <- train_data[, -c(1, 2)]\nxtest <- test_data[, -1]\n\n# Set A\ncat(\"Building training set A...\")\n# train_a <- build_features_a(xtrain)\ntrain_a <- read.csv(\"../input/santander-data/santander_train_a.csv\")         # Here's one I made earlier\ncat(\"Done!\\n\")\n\ncat(\"Building testing set A...\")\n# test_a <- build_features_a(xtest)\ntest_a <- read.csv(\"../input/santander-data/santander_test_a.csv\")           # Here's one I made earlier\ncat(\"Done!\\n\")\n\n# Set B\ncat(\"Building training set B...\")\n# train_b <- build_features_b(xtrain)\ntrain_b <- read.csv(\"../input/santander-data/santander_train_b.csv\")         # Here's one I made earlier\ncat(\"Done!\\n\")\n\ncat(\"Building testing set B...\")\n# test_b <- build_features_b(xtest)\ntest_b <- read.csv(\"../input/santander-data/santander_test_b.csv\")           # Here's one I made earlier\ncat(\"Done!\\n\")\n\n# PCA-A\ncat(\"Building training set PCA-A...\")\n# train_pca_a <- build_pca(train_a, 12)\ntrain_pca_a <- read.csv(\"../input/santander-data/santander_train_pca_a.csv\") # Here's one I made earlier\ncat(\"Done!\\n\")\n\ncat(\"Building testing set PCA-A...\")\n# test_pca_a <- build_pca(train_a, 12, test_a)\ntest_pca_a <- read.csv(\"../input/santander-data/santander_test_pca_a.csv\")   # Here's one I made earlier\ncat(\"Done!\\n\")\n\n# PCA-B\ncat(\"Building training set PCA-B...\")\n# train_pca_b <- build_pca(train_b, 48)\ntrain_pca_b <- read.csv(\"../input/santander-data/santander_train_pca_b.csv\") # Here's one I made earlier\ncat(\"Done!\\n\")\n\ncat(\"Building testing set PCA-B...\")\n# test_pca_b <- build_pca(train_b, 48, test_b)\ntest_pca_b <- read.csv(\"../input/santander-data/santander_test_pca_b.csv\")   # Here's one I made earlier\ncat(\"Done!\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f1ad81713b82771e6a91d250f45589d401c6101"},"cell_type":"markdown","source":"Various models were tested using these feature sets and results were logged for later comparison:"},{"metadata":{"trusted":true,"_uuid":"71dcd251f35ab1d970cb70aaecc1818d40c62f7f"},"cell_type":"code","source":"run <- FALSE\n\nif(run) {\n\n    ## Initialize CV Log\n    load(\"~/santander_cv_log.RData\")\n    if(!exists(\"cv_log\")) {cv_log <- NULL}\n\n\n    ## Set A Models\n    # Bagged CatBoost\n    params <- list(use_best_model = FALSE,\n                   loss_function = \"RMSE\",\n                   eval_metric = \"RMSE\",\n                   iterations = 100000,\n                   depth = 6,\n                   learning_rate = 0.001,\n                   l2_leaf_reg = 3.5,\n                   rsm = 0.50,\n                   od_type = \"Iter\",\n                   od_wait = 1000,\n                   bagging_temperature = 1,\n                   thread_count = 12,\n                   logging_level = \"Silent\"\n                   )\n\n    system.time({\n\n      cv_cb1 <- cv_bagged_catboost(xtrain = train_a,\n                                   ytrain = train_data$target,\n                                   xtest = test_a,\n                                   n_folds = 6,\n                                   fold_size = 0.1,\n                                   n_models = 10,\n                                   boot_size = 1.00,\n                                   params = params,\n                                   verbose = TRUE\n                                   )\n\n    }) -> run_time\n\n    cv_log <- c(cv_log,\n                list(\"feature_set\" = \"A\",\n                     \"model_type\" = \"CatBoost\",\n                     \"description\" = \"10x Bagged CatBoost: All Features\",\n                     \"params\" = params,\n                     \"result\" = cv_cb1,\n                     \"run_time\" = round(run_time[3]/60, 1)\n                     )\n                )\n    save(list = \"cv_log\", file = \"~/santander_cv_log.RData\")\n\n    # Bagged Linear\n    params <- list(\"alpha\" = 0.0,\n                   \"lambda\" = 0.03\n                   )\n\n    system.time({\n\n      cv_lm1 <- cv_bagged_linear(xtrain = train_a,\n                                 ytrain = train_data$target,\n                                 xtest = test_a,\n                                 n_folds = 6,\n                                 fold_size = 0.1,\n                                 n_models = 10000,\n                                 boot_size = 0.25,\n                                 col_size = 0.50,\n                                 params = params,\n                                 verbose = TRUE,\n                                 print_every = 1000\n                                 )\n\n    }) -> run_time; mean(cv_lm1$rmsle)\n\n    cv_log <- c(cv_log,\n                list(\"feature_set\" = \"A\",\n                     \"model_type\" = \"GLM\",\n                     \"description\" = \"10000x Bagged Linear: All Features, Contained Output\",\n                     \"params\" = params,\n                     \"result\" = cv_lm1,\n                     \"run_time\" = round(run_time[3]/60, 1)\n                     )\n                )\n    save(list = \"cv_log\", file = \"~/santander_cv_log.RData\")\n\n    # Adaboost\n    params <-  list(\"n.trees\" = 10000,\n                    \"interaction.depth\" = 2,\n                    \"shrinkage\" = 0.001,\n                    \"n.minobsinnode\" = 10,\n                    \"nTrain\" = NULL,\n                    \"verbose\" = FALSE\n                    )\n\n    system.time({\n\n      cv_ada1 <- cv_adaboost(xtrain = train_a,\n                             ytrain = train_data$target,\n                             xtest = test_a,\n                             n_folds = 6,\n                             fold_size = 0.1,\n                             params = params,\n                             verbose = TRUE\n                             )\n\n    }) -> run_time; mean(cv_ada1$rmsle)\n\n    cv_log <- c(cv_log,\n                list(\"feature_set\" = \"A\",\n                     \"model_type\" = \"Adaboost\",\n                     \"description\" = \"Adaboost: All Features\",\n                     \"params\" = params,\n                     \"result\" = cv_ada1,\n                     \"run_time\" = round(run_time[3]/60, 1)\n                     )\n                )\n    save(list = \"cv_log\", file = \"~/santander_cv_log.RData\")\n\n    # MLP\n    load(\"~/santander_mlp1_init.RData\")\n\n    params <-  list(\"loss\" = \"mean_squared_error\",\n                    \"lr\" = 0.000001,\n                    \"momentum\" = 0.90,\n                    \"decay\" = 1e-12, \n                    \"nesterov\" = TRUE,\n                    \"clipnorm\" = Inf,\n                    \"epochs\" = 2000,\n                    \"verbose\" = 1,\n                    \"batch_size\" = 10,\n                    \"validation_split\" = 0.10,\n                    \"patience\" = 30\n                    )\n\n    nn_arch <- list(\"input_shape\" = 32,\n                    \"output_shape\" = 1,\n                    \"activation\" = \"relu\",\n                    \"hidden\" = c(24),\n                    \"dropout_where\" = 1,\n                    \"dropout_percent\" = 0.50,\n                    \"regularize\" = TRUE,\n                    \"init_weights\" = init\n                    )\n\n    system.time({\n\n      cv_mlp1 <- cv_mlp(xtrain = train_a,\n                        ytrain = train_data$target,\n                        xtest = test_a,\n                        n_folds = 6,\n                        fold_size = 0.1,\n                        architecture = nn_arch,\n                        init_weights = init,\n                        params = params,\n                        verbose = TRUE\n                        )\n\n    }) -> run_time; mean(cv_mlp1$rmsle)\n\n    cv_log <- c(cv_log,\n                list(\"feature_set\" = \"A\",\n                     \"model_type\" = \"MLP\",\n                     \"description\" = \"MLP: 24 Hidden, 0.50 Dropout @ 1\",\n                     \"params\" = params,\n                     \"result\" = cv_mlp1,\n                     \"run_time\" = round(run_time[3]/60, 1)\n                     )\n                )\n    save(list = \"cv_log\", file = \"~/santander_cv_log.RData\")\n\n\n    ## Set B Models\n    # Bagged CatBoost\n    params <-  list(use_best_model = TRUE,\n                    loss_function = \"RMSE\",\n                    eval_metric = \"RMSE\",\n                    iterations = 20000,\n                    depth = 6,\n                    learning_rate = 0.0025,\n                    l2_leaf_reg = 3.5,\n                    rsm = 0.20,\n                    od_type = \"Iter\",\n                    od_wait = 1000,\n                    bagging_temperature = 1,\n                    thread_count = 12,\n                    metric_period = 100,\n                    logging_level = \"Verbose\"\n                    )\n\n    system.time({\n\n      cv_cb2 <- cv_bagged_catboost(xtrain = train_b,\n                                   ytrain = train_data$target,\n                                   xtest = test_b,\n                                   n_folds = 6,\n                                   fold_size = 0.1,\n                                   n_models = 10,\n                                   boot_size = 1.00,\n                                   params = params,\n                                   verbose = TRUE\n                                   )\n\n    }) -> run_time\n\n    cv_log <- c(cv_log,\n                list(\"feature_set\" = \"B\",\n                     \"model_type\" = \"CatBoost\",\n                     \"description\" = \"10x Bagged CatBoost: All Features\",\n                     \"params\" = params,\n                     \"result\" = cv_cb2,\n                     \"run_time\" = round(run_time[3]/60, 1)\n                     )\n                )\n    save(list = \"cv_log\", file = \"~/santander_cv_log.RData\")\n\n    # Bagged Linear\n    params <- list(\"alpha\" = 1.0,\n                   \"lambda\" = 0.03\n                   )\n\n    system.time({\n\n      cv_lm2 <- cv_bagged_linear(xtrain = train_b,\n                                 ytrain = train_data$target,\n                                 xtest = test_b,\n                                 n_folds = 6,\n                                 fold_size = 0.1,\n                                 n_models = 500,\n                                 boot_size = 1.00,\n                                 col_size = 0.10,\n                                 params = params,\n                                 verbose = TRUE,\n                                 print_every = 100\n                                 )\n\n    }) -> run_time; mean(cv_lm2$rmsle)\n\n    cv_log <- c(cv_log,\n                list(\"feature_set\" = \"B\",\n                     \"model_type\" = \"GLM\",\n                     \"description\" = \"500x Bagged Linear: All Features, Contained Output\",\n                     \"params\" = params,\n                     \"result\" = cv_lm2,\n                     \"run_time\" = round(run_time[3]/60, 1)\n                     )\n                )\n    save(list = \"cv_log\", file = \"~/santander_cv_log.RData\")\n\n    # Adaboost\n    params <-  list(\"n.trees\" = 500,\n                    \"interaction.depth\" = 2,\n                    \"shrinkage\" = 0.05,\n                    \"n.minobsinnode\" = 10,\n                    \"nTrain\" = NULL,\n                    \"verbose\" = TRUE\n                    )\n\n    system.time({\n\n      cv_ada2 <- cv_adaboost(xtrain = train_b,\n                             ytrain = train_data$target,\n                             xtest = test_b,\n                             n_folds = 6,\n                             fold_size = 0.1,\n                             params = params,\n                             verbose = TRUE\n                             )\n\n    }) -> run_time; mean(cv_ada2$rmsle)\n\n    cv_log <- c(cv_log,\n                list(\"feature_set\" = \"B\",\n                     \"model_type\" = \"Adaboost\",\n                     \"description\" = \"Adaboost: All Features\",\n                     \"params\" = params,\n                     \"result\" = cv_ada2,\n                     \"run_time\" = round(run_time[3]/60, 1)\n                     )\n                )\n    save(list = \"cv_log\", file = \"~/santander_cv_log.RData\")\n\n\n    ## Set A+B Models\n    # CatBoost >> CatBoost\n    params_1 <- list(use_best_model = TRUE,\n                     loss_function = \"RMSE\",\n                     eval_metric = \"RMSE\",\n                     iterations = 50000,\n                     depth = 6,\n                     learning_rate = 0.001,\n                     l2_leaf_reg = 1,\n                     rsm = 0.50,\n                     od_type = \"Iter\",\n                     od_wait = 1000,\n                     bagging_temperature = 1,\n                     thread_count = 12,\n                     metric_period = 1000,\n                     logging_level = \"Verbose\"\n                     )\n\n    params_2 <-  list(use_best_model = TRUE,\n                      loss_function = \"RMSE\",\n                      eval_metric = \"RMSE\",\n                      iterations = 40000,\n                      depth = 6,\n                      learning_rate = 0.005,\n                      l2_leaf_reg = 1,\n                      rsm = 0.50,\n                      od_type = \"Iter\",\n                      od_wait = 5000,\n                      bagging_temperature = 1,\n                      thread_count = 12,\n                      metric_period = 1000,\n                      logging_level = \"Verbose\"\n                      )\n\n    system.time({\n\n      cv_cbcb <- cv_catboost_catboost(xtrain_1 = train_a,\n                                      xtrain_2 = train_b,\n                                      ytrain = train_data$target,\n                                      xtest_1 = test_a,\n                                      xtest_2 = test_b,\n                                      n_folds = 1,\n                                      fold_size = 0.1,\n                                      params_1 = params_1,\n                                      params_2 = params_2,\n                                      verbose = TRUE\n                                      )\n\n    }) -> run_time; mean(cv_cbcb$rmsle)\n\n    cv_log <- c(cv_log,\n                list(\"feature_set\" = \"A+B\",\n                     \"model_type\" = \"CatBoost >> CatBoost\",\n                     \"description\" = \"Residual Boosting: CatBoost >> CatBoost\",\n                     \"params\" = c(params_1, params_2),\n                     \"result\" = cv_cbcb,\n                     \"run_time\" = round(run_time[3]/60, 1)\n                     )\n                )\n    save(list = \"cv_log\", file = \"~/santander_cv_log.RData\")\n\n    # Adaboost >> CatBoost\n    params_1 <-  list(\"n.trees\" = 30000,\n                      \"interaction.depth\" = 3,\n                      \"shrinkage\" = 0.001,\n                      \"n.minobsinnode\" = 10,\n                      \"nTrain\" = NULL,\n                      \"verbose\" = TRUE\n                      )\n\n    params_2 <-  list(use_best_model = TRUE,\n                      loss_function = \"RMSE\",\n                      eval_metric = \"RMSE\",\n                      iterations = 50000,\n                      depth = 6,\n                      learning_rate = 0.01,\n                      l2_leaf_reg = 1,\n                      rsm = 0.50,\n                      od_type = \"Iter\",\n                      od_wait = 2000,\n                      bagging_temperature = 1,\n                      thread_count = 12,\n                      metric_period = 1000,\n                      logging_level = \"Verbose\"\n                      )\n\n    system.time({\n\n      cv_abcb <- cv_adaboost_catboost(xtrain_1 = train_a,\n                                      xtrain_2 = train_b,\n                                      ytrain = train_data$target,\n                                      xtest_1 = test_a,\n                                      xtest_2 = test_b,\n                                      n_folds = 1,\n                                      fold_size = 0.1,\n                                      params_1 = params_1,\n                                      params_2 = params_2,\n                                      verbose = TRUE\n                                      )\n\n    }) -> run_time; mean(cv_abcb$rmsle)\n\n    cv_log <- c(cv_log,\n                list(\"feature_set\" = \"A+B\",\n                     \"model_type\" = \"Adaboost >> CatBoost\",\n                     \"description\" = \"Residual Boosting: Adaboost >> CatBoost\",\n                     \"params\" = c(params_1, params_2),\n                     \"result\" = cv_abcb,\n                     \"run_time\" = round(run_time[3]/60, 1)\n                     )\n                )\n    save(list = \"cv_log\", file = \"~/santander_cv_log.RData\")\n\n\n    ## Set PCA-A Models\n    # Bagged CatBoost\n    comps <- 8\n\n    params = list(use_best_model = TRUE,\n                  loss_function = \"RMSE\",\n                  eval_metric = \"RMSE\",\n                  iterations = 75000,\n                  depth = 6,\n                  learning_rate = 0.001,\n                  l2_leaf_reg = 3.5,\n                  rsm = 0.50,\n                  od_type = \"Iter\",\n                  od_wait = 2000,\n                  bagging_temperature = 1,\n                  thread_count = 12,\n                  logging_level = \"Silent\",\n                  metric_period = 1000\n                  )\n\n    system.time({\n\n      cv_cb3 <- cv_bagged_catboost(xtrain = train_pca_a[, 1:comps],\n                                   ytrain = train_data$target,\n                                   xtest = test_pca_a[, 1:comps],\n                                   n_folds = 6,\n                                   fold_size = 0.1,\n                                   n_models = 10,\n                                   boot_size = 1.00,\n                                   params = params,\n                                   verbose = TRUE\n                                   )\n\n    }) -> run_time; mean(cv_cb3$rmsle)\n\n    cv_log <- c(cv_log,\n                list(\"feature_set\" = \"PCA-A\",\n                     \"model_type\" = \"CatBoost\",\n                     \"description\" = \"10x Bagged CatBoost: 8 Components\",\n                     \"params\" = params,\n                     \"result\" = cv_cb3,\n                     \"run_time\" = round(run_time[3]/60, 1)\n                     )\n                )\n    save(list = \"cv_log\", file = \"~/santander_cv_log.RData\")\n\n    # Bagged Linear\n    comps <- 6\n\n    params <- list(\"alpha\" = 0.0,\n                   \"lambda\" = 0.03\n                   )\n\n    system.time({\n\n      cv_lm3 <- cv_bagged_linear(xtrain = train_pca_a[, 1:comps],\n                                 ytrain = train_data$target,\n                                 xtest = test_pca_a[, 1:comps],\n                                 n_folds = 6,\n                                 fold_size = 0.1,\n                                 n_models = 10000,\n                                 boot_size = 0.25,\n                                 col_size = 1.00,\n                                 params = params,\n                                 verbose = TRUE,\n                                 print_every = 1000\n                                 )\n\n    }) -> run_time; mean(cv_lm3$rmsle)\n\n    cv_log <- c(cv_log,\n                list(\"feature_set\" = \"PCA-A\",\n                     \"model_type\" = \"GLM\",\n                     \"description\" = \"10000x Bagged Linear: 6 Components, Contained Output\",\n                     \"params\" = params,\n                     \"result\" = cv_lm3,\n                     \"run_time\" = round(run_time[3]/60, 1)\n                     )\n                )\n    save(list = \"cv_log\", file = \"~/santander_cv_log.RData\")\n\n\n    ## Set PCA-B Models\n    # Bagged CatBoost\n    comps <- 18\n\n    params = list(use_best_model = TRUE,\n                  loss_function = \"RMSE\",\n                  eval_metric = \"RMSE\",\n                  iterations = 100000,\n                  depth = 6,\n                  learning_rate = 0.001,\n                  l2_leaf_reg = 3.5,\n                  rsm = 0.50,\n                  od_type = \"Iter\",\n                  od_wait = 2000,\n                  bagging_temperature = 1,\n                  thread_count = 12,\n                  logging_level = \"Silent\",\n                  metric_period = 1000\n                  )\n\n    system.time({\n\n      cv_cb4 <- cv_bagged_catboost(xtrain = train_pca_b[, 1:comps],\n                                   ytrain = train_data$target,\n                                   xtest = test_pca_b[, 1:comps],\n                                   n_folds = 6,\n                                   fold_size = 0.1,\n                                   n_models = 10,\n                                   boot_size = 1.00,\n                                   params = params,\n                                   verbose = TRUE\n                                   )\n\n    }) -> run_time\n\n    cv_log <- c(cv_log,\n                list(\"feature_set\" = \"PCA-B\",\n                     \"model_type\" = \"CatBoost\",\n                     \"description\" = \"10x Bagged CatBoost: 18 Components\",\n                     \"params\" = params,\n                     \"result\" = cv_cb4,\n                     \"run_time\" = round(run_time[3]/60, 1)\n                     )\n                )\n    save(list = \"cv_log\", file = \"~/santander_cv_log.RData\")\n    \n}","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"68ce7730b9aabbc81b1ad6d0882e755e2b58904d"},"cell_type":"code","source":"load(\"../input/santander-data/santander_data.RData\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14e62307a88a2878413b2fe9b1facdaccf3eefe0"},"cell_type":"markdown","source":"The cross-validation results are summarized below:"},{"metadata":{"trusted":true,"_uuid":"575c5d42a187bb377ad09aa45c8bd5c3896c5231"},"cell_type":"code","source":"## Summarize CV Results\n# Build summary\nfeature_set <- unlist(cv_log[seq(1, length(cv_log), 6)])\nmodel_type <- unlist(cv_log[seq(2, length(cv_log), 6)])\ndescription <- unlist(cv_log[seq(3, length(cv_log), 6)])\nparams <- unlist(lapply(cv_log[seq(4, length(cv_log), 6)], function(x) paste(paste(names(x), unlist(x), sep = \" = \"), collapse = \", \")))\nrmsle <- unlist(lapply(cv_log[seq(5, length(cv_log), 6)], function(x) mean(x$rmsle)))\n\ndata.frame(feature_set = feature_set,\n           model_type = model_type,\n           description = description,\n           params = params,\n           rmsle = rmsle\n           ) ->\n  cv_sum\n\nhead(cv_sum)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fed7db7c9952820df5b8b8d4142eac9ab64bac90"},"cell_type":"markdown","source":"Of the models that were tested, 8 were selected to the ensemble:"},{"metadata":{"trusted":true,"_uuid":"ff2e4b52d924c6ec509d870b73abfceaaad30095"},"cell_type":"code","source":"# Select models\n# Set A     || 10x Bagged CatBoost: All Features\n# Set A     || Adaboost: All Features\n# Set PCA-A || 10x Bagged CatBoost: 8 Components\n# Set B     || 10x Bagged CatBoost: All Features\n# Set B     || Adaboost: All Features\n# Set PCA-B || 10x Bagged CatBoost: 18 Components\n# Set A     || 2000x Bagged Linear: All Features, Contained Output\n# Set B     || 100x Bagged Linear: All Features, Contained Output","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a969265abd3a5c992af9073a65992cca38ed3110"},"cell_type":"markdown","source":"In order to test ensemble methods, *out-of-sample* predictions from each of the 8 sub-models were produced on the training set using 6 folds. "},{"metadata":{"trusted":true,"_uuid":"8c65dfe745062f1825c456e8ae7808122baea904"},"cell_type":"code","source":"run <- FALSE\n\nif(run) {\n    \n    ## Cross-Predict\n    first_layer <- cross_predict(xtrain_a = train_a, \n                                 xtrain_b = train_b, \n                                 xtrain_pca_a = train_pca_a, \n                                 xtrain_pca_b = train_pca_b, \n                                 train_orig = train_data, \n                                 nfolds = 6,\n                                 shuffle = TRUE,\n                                 verbose = TRUE\n                                 )\n    \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa7ad636b8ed1064343c963ee200d035a3c4645b"},"cell_type":"code","source":"## Ensemble\n# First layer analysis\nfirst_layer <- first_layer_train\n\ncorr_mat <- cor(data.matrix(first_layer[, -c(1, 2)]))\nprint(corr_mat)\n\ncorrplot(corr_mat,\n         method = \"square\",\n         order = \"hclust\"\n         )\n\nrmsle_1 <- RMSLE(first_layer$pred_1, first_layer$target) # 1.349627\nrmsle_2 <- RMSLE(first_layer$pred_2, first_layer$target) # 1.554539\nrmsle_3 <- RMSLE(first_layer$pred_3, first_layer$target) # 1.352440\nrmsle_4 <- RMSLE(first_layer$pred_4, first_layer$target) # 1.378814\nrmsle_5 <- RMSLE(first_layer$pred_5, first_layer$target) # 1.432816\nrmsle_6 <- RMSLE(first_layer$pred_6, first_layer$target) # 1.616978\nrmsle_7 <- RMSLE(first_layer$pred_7, first_layer$target) # 1.433627\nrmsle_8 <- RMSLE(first_layer$pred_8, first_layer$target) # 1.486710\n\n# Average\nfirst_layer$average <- rowMeans(first_layer[, -c(1, 2)])\nrmsle_avg <- RMSLE(first_layer$average, first_layer$target) # 1.409475\n\n# Selected average\nfirst_layer$avg_1 <- rowMeans(first_layer[, c(3, 5, 6, 7, 9)])\nrmsle_avg_1 <- RMSLE(first_layer$avg_1, first_layer$target) # 1.360608\n\nfirst_layer$avg_2 <- rowMeans(first_layer[, c(3, 5, 6, 7, 9, 10)])\nrmsle_avg_2 <- RMSLE(first_layer$avg_2, first_layer$target) # 1.371524\n\nfirst_layer$avg_3 <- rowMeans(first_layer[, c(3, 5, 6)])\nrmsle_avg_3 <- RMSLE(first_layer$avg_3, first_layer$target) # 1.347781\n\nfirst_layer$avg_4 <- rowMeans(first_layer[, c(3, 5, 6, 7)])\nrmsle_avg_4 <- RMSLE(first_layer$avg_4, first_layer$target) # 1.354230","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38d54c8497ff013bd7a935f43d757c8e4c90754d"},"cell_type":"markdown","source":"The chosen ensemble method was a weighted average of all 8 sub-models using **bagging** to optimize weights on 20 bootstrap samples."},{"metadata":{"trusted":true,"_uuid":"0f0505be7cffb00d2be1e8492ec7d0684df59355"},"cell_type":"code","source":"run <- FALSE\n\nif(run) {\n\n    # Weighted average\n    first_layer <- first_layer_train\n\n    foreach(i = 1:10, .combine = rbind) %do% {\n\n      rm(.Random.seed, envir=globalenv())\n      seed <- sample(1:nrow(first_layer), nrow(first_layer)/10, replace = FALSE)\n\n      xtrain <- first_layer[-seed, -c(1, 2)]\n      ytrain <- first_layer$target[-seed]\n\n      xtest <- first_layer[seed, -c(1, 2)]\n      ytest <- first_layer$target[seed]\n\n      foreach(j = 1:20, .combine = cbind) %do% {\n\n        cat(\"Optimizing on bootstrap sample \", j, \"...\", sep = \"\")\n        rm(.Random.seed, envir=globalenv())\n        boot <- sample(1:nrow(xtrain), nrow(xtrain), replace = TRUE)\n\n        xtrain_i <- xtrain[boot, ]\n        ytrain_i <- ytrain[boot]\n\n        params <- rep(1, times = 8)\n\n        opt <- optim(params, \n                     wmean_ensemble,\n                     xtrain = xtrain_i, \n                     ytrain = ytrain_i,\n                     method = \"L-BFGS-B\",\n                     lower = rep(0, times = 8),\n                     control = list(maxit = 10,\n                                    trace = 0\n                                    )\n                     )\n\n        pred <- apply(xtest, FUN = weighted.mean, w = opt$par, MARGIN = 1)\n        cat(\"Done!\\n\")\n        pred\n\n      } %>%\n        data.frame() %>%\n        rowMeans() ->\n        pred\n\n      rmse <- RMSE(pred, ytest)\n      rmsle <- RMSLE(pred, ytest)\n\n      cat(\"RMSE: \",\n          rmse,\n          \" || RMSLE: \",\n          rmsle,\n          \"\\n\",\n          sep = \"\"\n          )\n\n      data.frame(rmse = rmse,\n                 rmsle = rmsle\n                 )\n\n    } %>% \n      data.frame() ->\n      cv_out\n\n    mean(cv_out$rmsle) # 1.343794\n    \n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76e154d525448bc2283380cb3580d87b36f9bd16"},"cell_type":"markdown","source":"A submission is prepared as follows:"},{"metadata":{"trusted":true,"_uuid":"547d5ed551a68d29ce3bb61168f88fae7a6ac91b"},"cell_type":"code","source":"run <- FALSE\n\nif(run) {\n\n    ## Make Submission\n    # Load and clean data\n    cat(\"Loading data...\")\n    train_data <- read.csv(\"../input/santander-value-prediction-challenge/train.csv\")\n    test_data <- read.csv(\"../input/santander-value-prediction-challenge/test.csv\")\n    sample_submission <- read.csv(\"../input/santander-value-prediction-challenge/sample_submission.csv\")\n    cat(\"\\n\")\n    cat(\"Cleaning data...\")\n    options(scipen = 999)\n    train_data <- numerify(train_data, c(1, 2), 0)\n    test_data <- numerify(test_data, 1, 0)\n    train_a <- read.csv(\"../input/santander-data/santander_train_a.csv\")         # Here's one I made earlier\n    test_a <- read.csv(\"../input/santander-data/santander_test_a.csv\")           # Here's one I made earlier\n    train_b <- read.csv(\"../input/santander-data/santander_train_b.csv\")         # Here's one I made earlier\n    test_b <- read.csv(\"../input/santander-data/santander_test_b.csv\")           # Here's one I made earlier\n    train_pca_a <- read.csv(\"../input/santander-data/santander_train_pca_a.csv\") # Here's one I made earlier\n    test_pca_a <- read.csv(\"../input/santander-data/santander_test_pca_a.csv\")   # Here's one I made earlier\n    train_pca_b <- read.csv(\"../input/santander-data/santander_train_pca_b.csv\") # Here's one I made earlier\n    test_pca_b <- read.csv(\"../input/santander-data/santander_test_pca_b.csv\")   # Here's one I made earlier\n\n    # First layer predictions\n    ytrain <- log(train_data$target)\n    first_layer <- data.frame(sample_submission[, 1])\n    cat(\"Training model 1...\")\n    params <- list(use_best_model = TRUE,\n                   loss_function = \"RMSE\",\n                   eval_metric = \"RMSE\",\n                   iterations = 500000,\n                   depth = 6,\n                   learning_rate = 0.0001,\n                   l2_leaf_reg = 3.5,\n                   rsm = 0.50,\n                   od_type = \"Iter\",\n                   od_wait = 2000,\n                   bagging_temperature = 1,\n                   thread_count = 12,\n                   logging_level = \"Silent\"\n                   )\n    pred <- bagged_catboost(train_a, ytrain, test_a, n_models = 10, boot_size = 1.00, params = params, verbose = FALSE)\n    first_layer$pred_1 <- pred\n    cat(\"Done!\\n\")\n    cat(\"Training model 2...\")\n    params <- list(\"alpha\" = 0.0,\n                   \"lambda\" = 0.03\n                   )\n    pred <- bagged_linear(train_a, ytrain, test_a, n_models = 2000, boot_size = 0.25, col_size = 0.5, params = params, verbose = FALSE, print_every = 10)\n    first_layer$pred_2 <- pred\n    cat(\"Done!\\n\")\n    cat(\"Training model 3...\")\n    params <-  list(\"n.trees\" = 10000,\n                    \"interaction.depth\" = 2,\n                    \"shrinkage\" = 0.001,\n                    \"n.minobsinnode\" = 10,\n                    \"nTrain\" = NULL,\n                    \"verbose\" = FALSE\n                    )\n    pred <- adaboost(train_a, ytrain, test_a, params = params, verbose = FALSE)\n    first_layer$pred_3 <- pred\n    cat(\"Done!\\n\")\n    cat(\"Training model 4...\")\n    params = list(use_best_model = TRUE,\n                  loss_function = \"RMSE\",\n                  eval_metric = \"RMSE\",\n                  iterations = 75000,\n                  depth = 6,\n                  learning_rate = 0.001,\n                  l2_leaf_reg = 3.5,\n                  rsm = 0.50,\n                  od_type = \"Iter\",\n                  od_wait = 2000,\n                  bagging_temperature = 1,\n                  thread_count = 12,\n                  logging_level = \"Silent\",\n                  metric_period = 1000\n                  )\n    pred <- bagged_catboost(train_pca_a[, 1:8], ytrain, test_pca_a[, 1:8], n_models = 10, boot_size = 1.00, params = params, verbose = FALSE)\n    first_layer$pred_4 <- pred\n    cat(\"Done!\\n\")\n    cat(\"Training model 5...\")\n    params <- list(use_best_model = TRUE,\n                   loss_function = \"RMSE\",\n                   eval_metric = \"RMSE\",\n                   iterations = 20000,\n                   depth = 6,\n                   learning_rate = 0.0025,\n                   l2_leaf_reg = 3.5,\n                   rsm = 0.20,\n                   od_type = \"Iter\",\n                   od_wait = 1000,\n                   bagging_temperature = 1,\n                   thread_count = 12,\n                   logging_level = \"Silent\"\n                   )\n    pred <- bagged_catboost(train_b, ytrain, test_b, n_models = 10, boot_size = 1.00, params = params, verbose = FALSE)\n    first_layer$pred_5 <- pred\n    cat(\"Done!\\n\")\n    cat(\"Training model 6...\")\n    params <- list(\"alpha\" = 1.0,\n                   \"lambda\" = 0.03\n                   )\n    pred <- bagged_linear(train_b, ytrain, test_b, n_models = 100, boot_size = 1.00, col_size = 0.1, params = params, verbose = FALSE, print_every = 10)\n    first_layer$pred_6 <- pred\n    cat(\"Done!\\n\")\n    cat(\"Training model 7...\")\n    params <-  list(\"n.trees\" = 500,\n                    \"interaction.depth\" = 2,\n                    \"shrinkage\" = 0.05,\n                    \"n.minobsinnode\" = 10,\n                    \"nTrain\" = NULL,\n                    \"verbose\" = FALSE\n                    )\n    pred <- adaboost(train_b, ytrain, test_b, params = params, verbose = FALSE)\n    first_layer$pred_7 <- pred\n    cat(\"Done!\\n\")\n    cat(\"Training model 8...\")\n    params = list(use_best_model = TRUE,\n                  loss_function = \"RMSE\",\n                  eval_metric = \"RMSE\",\n                  iterations = 100000,\n                  depth = 6,\n                  learning_rate = 0.001,\n                  l2_leaf_reg = 3.5,\n                  rsm = 0.50,\n                  od_type = \"Iter\",\n                  od_wait = 2000,\n                  bagging_temperature = 1,\n                  thread_count = 12,\n                  logging_level = \"Silent\"\n                  )\n    pred <- bagged_catboost(train_pca_b[, 1:18], ytrain, test_pca_b[, 1:18], n_models = 10, boot_size = 1.00, params = params, verbose = FALSE)\n    first_layer$pred_8 <- pred\n    cat(\"Done!\\n\")\n\n    # Optimize weights\n    xtrain <- first_layer_train[, -c(1, 2)]\n    ytrain <- first_layer_train$target\n    xtest <- first_layer[, -1]\n\n    foreach(j = 1:20, .combine = cbind) %do% {\n\n      cat(\"Optimizing on bootstrap sample \", j, \"...\", sep = \"\")\n      rm(.Random.seed, envir=globalenv())\n      boot <- sample(1:nrow(xtrain), nrow(xtrain), replace = TRUE)\n\n      xtrain_i <- xtrain[boot, ]\n      ytrain_i <- ytrain[boot]\n\n      params <- rep(1, times = 8)\n\n      opt <- optim(params, \n                   wmean_ensemble,\n                   xtrain = xtrain_i, \n                   ytrain = ytrain_i,\n                   method = \"L-BFGS-B\",\n                   lower = rep(0, times = 8),\n                   control = list(maxit = 10,\n                                  trace = 0\n                                  )\n                   )\n\n      pred <- apply(xtest, FUN = weighted.mean, w = opt$par, MARGIN = 1)\n      cat(\"Done!\\n\")\n      pred\n\n    } %>%\n      data.frame() %>%\n      rowMeans() ->\n      pred\n\n    # Write CSV\n    submit <- sample_submission\n    submit$target <- pred\n\n    write.csv(submit, file = \"submit.csv\", row.names = FALSE)\n    \n}\n\nsubmit <- read.csv(\"../input/santander-data/santander_submit.csv\") # Here's one I made earlier\nwrite.csv(submit, file = \"submit.csv\", row.names = FALSE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e6eb24f0caee8a7e22209ce595849dae56077d5"},"cell_type":"markdown","source":"This approach registers a public leaderboard score of **1.37** without exploiting Giba's property. "}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}