{"cells":[{"metadata":{"_uuid":"7c66e88d582c7a0e56396054a8d51906a6fe1749"},"cell_type":"markdown","source":"# Pima indianas Diabetes Database\n\nHello, today i am going to  estimate the liklihood of person to be diabetic, this is my first MachineLearning real application away from the theory.\n \n"},{"metadata":{"_uuid":"dbd847bbd0377d2506a4df122578c26ed6e0624f"},"cell_type":"markdown","source":"## Prepare , load the data \n\nfirst things first, import the modules needed"},{"metadata":{"trusted":true,"_uuid":"ab394d483f877119e59458c0421648644e79dcee"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# disable warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7258199f40e72c346b3e93298efc828da45a16d8"},"cell_type":"code","source":"filename = '../input/diabetes.csv'\ndata=pd.read_csv(filename)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcae7a838cd297f673d748daec8fbfa8a9a6858c"},"cell_type":"markdown","source":"let's take a very quick look at our data"},{"metadata":{"trusted":true,"_uuid":"6bfbd2fb0cdc7eecfbcdf42f6183afb41e3d738b"},"cell_type":"code","source":"print(data.columns) # to know all the features(variables) we got in our data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f15bc34b41afdcbd6b34c461d8719e6030ff49c2"},"cell_type":"code","source":"print(data.head()) #the first 5 rows","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33a2f7955d8c54beba2a9fe1370b04143ee71f90"},"cell_type":"markdown","source":" ## First things first: analysing the outcome\n \n by the outcome we mean whether the person is diabetic or no (1 = yes; 0 = no)\n\ndescriptive statistics summary :\n\n \n "},{"metadata":{"trusted":true,"_uuid":"0d7881a24f1d97d508af09e97637b3c964f572ee"},"cell_type":"code","source":"data['Outcome'].describe()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3da9c2a77babd0a0fcde5f087a67f4e790fd18f4"},"cell_type":"markdown","source":"now the histogram of the outcome : \n"},{"metadata":{"trusted":true,"_uuid":"7fb506897dad4bc395402124c6d712622b7b575c"},"cell_type":"code","source":"data['Outcome'].hist(figsize=(7,7))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04080068fb1cea191d60a689fc2043d15e0dd619"},"cell_type":"markdown","source":"we can easily see that  the Non-Diabetic persons is more than the diabetics by what it seems the half\n"},{"metadata":{"_uuid":"955a00dab388e10e0009c3830cfdba17b3b90e42"},"cell_type":"markdown","source":"#  ' Diabetes ' feature Relationships !!!!\n\n**Diabetes** love Aged,ppl with high level of Glucose  \" based from what I learned from high School\", \nSo let's start with them, by that i mean there relation with Diabetes.\n\n"},{"metadata":{"trusted":true,"_uuid":"baed329a56d5dee22319fabc14aa5f0c1878f5d5"},"cell_type":"code","source":"\n#borrowed from my friend Ayoub Benaissa.\ndef plot_diabetic_per_feature(data, feature):\n    grouped_by_Outcome = data[feature].groupby(data[\"Outcome\"])\n    diabetic_per_feature = pd.DataFrame({\"Sick\": grouped_by_Outcome.get_group(1),\n                                        \"Not Sick\": grouped_by_Outcome.get_group(0),\n                                        })\n    hist = diabetic_per_feature.plot.hist(bins=60, alpha=0.6)\n    hist.set_xlabel(feature)\n    plt.show()\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae8171d79f3cce7c69081d85cd8ee28bd01b88ed"},"cell_type":"markdown","source":"let's start with the age :"},{"metadata":{"trusted":true,"_uuid":"753519b61d0e93f0c1e8f55f7b5daedc99264849"},"cell_type":"code","source":"plot_diabetic_per_feature(data, \"Age\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72e20483a500d9a764db3dbf0c81436adcf55c20"},"cell_type":"markdown","source":"*Intersting*, as u see, The older the persons,The higher the number of diabetics.\nofc that's based from the dataset ' my doctor friend told me there's 2 types of Diabetes, so maybe this data concern the one who target the olders\"\n"},{"metadata":{"trusted":true,"_uuid":"37631ea4af6acdb725d75e0a2e463365be960f6e"},"cell_type":"code","source":"plot_diabetic_per_feature(data, \"Glucose\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c00c4b768b81a4a3d8d9365147ab968469b85900"},"cell_type":"markdown","source":"Same ... the more Glucose you have to more likley to have Diabetes, i guess that will be pretty obvious for you  if you know bit of biology.\nWe notice the odd 0 information, let's confirm: \n\n"},{"metadata":{"trusted":true,"_uuid":"be75d8e7c8023d1ae0bb7ad4161cb4767fe51c1a"},"cell_type":"code","source":"print(data[\"Glucose\"].min())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10d9700b06290d55656de5c0f084ff9abc632d8b"},"cell_type":"markdown","source":"that true, I don't think that a person can have 0 Glucose and still alive, so i'll delete it later + It also may effect the accuracy of the model.\n\n\nWhat about the **BMI** ( it's basically the weight of the person with kg) :\n"},{"metadata":{"trusted":true,"_uuid":"1113bb66e59683bc2498db312fc9537d577e1ca0"},"cell_type":"code","source":"plot_diabetic_per_feature(data, \"BMI\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b2ccf5579855f04a3edb2e67a629712e585e123"},"cell_type":"markdown","source":"very reasonable results, same notes from the 2 above, we can also notice the 0 odd info so we'll delete it late for the same reasons."},{"metadata":{"_uuid":"7729af4a6a718160a10bd1eb3d60bdbed198b214"},"cell_type":"markdown","source":"# Think beyond the box\nlet's think beyond the box, how is that? \n    with :\n* Correlation matrix\n* Scatter plots between the most correlated variables\n"},{"metadata":{"_uuid":"ccfb8c551b86ad62e591e14923ca3184cacc2ab6"},"cell_type":"markdown","source":"**Correlation matrix  : **\n"},{"metadata":{"trusted":true,"_uuid":"229b0cb31c5eff3df9de84d1eca803c05d87aa84"},"cell_type":"code","source":"import seaborn as sns #the librery we'll use for the job xD\n\ncorrmat = data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, cbar=True, annot=True, square=True, vmax=.8);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58e1e0a8244d238e610a253a2aa9dca22f8e0ad0"},"cell_type":"markdown","source":"Alright, let's break this matrix down to some few notes : \n* Glucose, Age and  BMI are the most Correlated features with the 'Outcome'\n* Bloodpressure, SkinThikness have tiny Correlation with the outcome, hummm !\n* check how the SkinThikness and BMI Correlated, make me think of rolling it out since mose of the fat ppl tends to have thicc skin\n* Age with Pregnancies are the most Correlated features\n* Insulin with Glucuse ' BIOLOGY  :) \"\n* DiabetesPedigreeFunction bit Correlated with most of them ' I am not sure with feature really mean\"\n* finnaly SkinThikness with Insulin, that's odd !"},{"metadata":{"_uuid":"29a9e66cb22529392f44429ec2edbb43bfa4d3a0"},"cell_type":"markdown","source":"### Scatter plots between 'Outcome' and correlated variables"},{"metadata":{"trusted":true,"_uuid":"0927210d408764f20878b40a43232d78d23c27dd"},"cell_type":"code","source":"sns.set()\ncols = ['Pregnancies','Glucose','BloodPressure','Insulin','BMI','DiabetesPedigreeFunction','Age','Outcome']\nsns.pairplot(data[cols], size = 2.5)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5d630112ffbdb1dc0b1ac86a1e5d44bf7be0b29"},"cell_type":"markdown","source":"opss! that's a huge amount of graphs to analyse but let's carry on :\n* bloodpressure and age tend to have a relation and that's kinda obviose since most of aged ppl have bloodpressure\n* Glucose and insulin have very strong relation makes me think about deleting the inslin futuere\n* in pregnancy/age we notice some kind of a liniar line in the right bottom \n\nthat's it, no need to repeat the past notes ' even tho i did '\n\n"},{"metadata":{"_uuid":"b15f59352dc5ece90cee59eabb624de5e29a1b67"},"cell_type":"markdown","source":"# Outliers \nOutliers is also something that we should be aware of. Why? Because outliers can markedly affect our models.\n\nlet's see what we can do :"},{"metadata":{"trusted":true,"_uuid":"9a8bdbeac7815fdfad04e7739cc9c5b58bcf2bda"},"cell_type":"code","source":"data.min()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddcde78bf7707a752cd300923bce29b6ff3e9a3c"},"cell_type":"markdown","source":"It's okay to have 0 Pregnancies, for the rest, we'll delete the rows containing the 0 values:\n"},{"metadata":{"trusted":true,"_uuid":"63e2b043c9f38a678c5a2a63c05d0804d655672f"},"cell_type":"code","source":"data = data.drop(data[data['Glucose'] == 0].index)\ndata = data.drop(data[data['SkinThickness'] == 0].index) # even it will be deleted xD\ndata = data.drop(data[data['BloodPressure'] == 0].index) #same\ndata = data.drop(data[data['BMI'] == 0].index)\ndata = data.drop(data[data['Insulin'] == 0].index)\n\nprint(data.min()) # let's check\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9fedf43b8dd08be5983b55158342a049415b2fa"},"cell_type":"markdown","source":"*excellent*, NEXT !!"},{"metadata":{"_uuid":"77e96df489c07a80bf1595b113a5d549585300df"},"cell_type":"markdown","source":"# Preparing the data/Selecting a model\n\n"},{"metadata":{"trusted":true,"_uuid":"1847335fb9e7b59b4749f558268c2296038795d0"},"cell_type":"code","source":"#just the libreries we need\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.tree import DecisionTreeClassifier  \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split as tts\n\n\n#preparing the data\nY=data['Outcome']\ncols = ['Pregnancies','Glucose','DiabetesPedigreeFunction','Insulin','BMI','Age']\n# I deleted BloodPressure and Skinthikness\n\nX_train, X_test, Y_train, Y_test = tts(data[cols], Y, test_size = 0.2, random_state = 2)\n\n\n\n#Selecting a model\n\nlr=LogisticRegression()\nlr.fit(X_train,Y_train)\naccuracy=lr.score(X_test,Y_test)\nprint(\"LogisticRegression Accuracy: {}%\".format(accuracy * 100))\n\n\nGs = GaussianNB()\nGs.fit(X_train,Y_train)\naccuracy=Gs.score(X_test,Y_test)\nprint(\"GaussianNB Accuracy: {}%\".format(accuracy * 100))\n\nKn = KNeighborsClassifier(n_neighbors=12)  \nKn.fit(X_train, Y_train)\naccuracy=Kn.score(X_test,Y_test)\nprint(\"KNeighborsClassifier Accuracy: {}%\".format(accuracy * 100))\n\n\nDs = DecisionTreeClassifier()  \nDs.fit(X_train, Y_train) \naccuracy=Ds.score(X_test,Y_test)\nprint(\"DecisionTreeClassifier Accuracy: {}%\".format(accuracy * 100))\n\nRs = RandomForestClassifier(n_estimators=63, random_state=0)  \nRs.fit(X_train, Y_train)  \naccuracy=Rs.score(X_test,Y_test)\nprint(\"RandomForestClassifier Accuracy: {}%\".format(accuracy * 100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29f7b76eb4a795553a18b4a2bfdbafbc26813bee"},"cell_type":"markdown","source":"I'll go with RandomForestClassifier for having the highest Accuracy ( didn't study the variance in this version so wait for the next one )\n\n**thanks for reading  and to the next version/kernels **"},{"metadata":{"trusted":true,"_uuid":"bcf5ed62236ddcb7e55d47c0bed2b67bf3f87c45"},"cell_type":"code","source":"\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bde3a9e78f6c219e7737bd1690e57d7bf8001993"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}