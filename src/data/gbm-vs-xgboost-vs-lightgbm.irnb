{"nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "77fa257e680d18739ce82d0059ed6fda2e8805db", "_cell_guid": "f2e65080-5c69-cd2e-d515-8c569cbf8fca"}, "source": ["# Motivation\n", "\n", "I have been \"Kaggling\" (that can be a verb, right?) since before **GBM** was cool. An over trained set of **GBM** models got me a 2nd place finish in the [Dunnhumby's Shopper Challenge](https://www.kaggle.com/c/dunnhumbychallenge/leaderboard).  Since then I have spent my time working on projects that didn't require boosted algorithms. Now that I have some free time, I wanted to explore **xgboost** (the GBM killer) and its new challenger **lightGBM**. This notebook will explore the speed and accuracy of each model and discuss any observations I have along the way.\n", "\n", "# Boring Setup"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "249b3008eb0369e9798770deef1b718cf383f01a", "_cell_guid": "d3c61522-9fc4-8337-a68a-ec10015da972"}, "execution_count": null, "source": ["# Libraries\n", "library(pROC, quietly=TRUE)\n", "library(microbenchmark, quietly=TRUE)\n", "\n", "# Set seed so the train/test split is reproducible\n", "set.seed(42)\n", "\n", "# Read in the data and split it into train/test subsets\n", "credit.card.data = read.csv(\"../input/creditcard.csv\")\n", "\n", "train.test.split <- sample(2\n", "\t, nrow(credit.card.data)\n", "\t, replace = TRUE\n", "\t, prob = c(0.7, 0.3))\n", "train = credit.card.data[train.test.split == 1,]\n", "test = credit.card.data[train.test.split == 2,]"]}, {"cell_type": "markdown", "metadata": {"_uuid": "66894ad6e819cdbf8fcc6f405aa21a8e253bb9fb", "_cell_guid": "287aaa2e-5014-1a66-3d17-5c6b557ae1f1"}, "source": ["# Feature Creation\n", "\n", "This section is empty. Converting the time values to hour or day would probably improve the accuracy, but that is not the purpose of this kernel."]}, {"cell_type": "markdown", "metadata": {"_uuid": "917d1460eacd67d667c65776419cfad619cd05ee", "_cell_guid": "61aff1b1-0c3e-f917-387f-29b05fde367a"}, "source": ["# Modeling\n", "\n", "I have attempted to select a common set of parameters for each model, but that is not entirely possible. (*max_depth* vs *num_leaves* in **xgboost** and **lightGBM**) The following are some of the assumptions and choices made during this modeling process.\n", "\n", "* The data will be placed into the their preferred data formats before calling the models.\n", "* Models will not be trained with cross-validation.\n", "* If possible, different number of cores will be used during the speed analysis. (future mod)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "b61a0bd60e061b13b17ce9c74e91e9b24f696625", "_cell_guid": "6ce35725-33be-6d6e-2720-6e2cd0ffea21"}, "source": ["## GBM\n", "\n", "Training the GBM is slow enough, I am not going to bother microbenchmarking it."]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "dcaf3b8b22c9c04a4b17d96c5d2ed854fab07665", "_cell_guid": "b9c54cbb-a876-558c-fff1-ccdb0b4f60c3"}, "execution_count": null, "source": ["library(gbm, quietly=TRUE)\n", "\n", "# Get the time to train the GBM model\n", "system.time(\n", "\tgbm.model <- gbm(Class ~ .\n", "\t\t, distribution = \"bernoulli\"\n", "\t\t, data = rbind(train, test)\n", "\t\t, n.trees = 500\n", "\t\t, interaction.depth = 3\n", "\t\t, n.minobsinnode = 100\n", "\t\t, shrinkage = 0.01\n", "\t\t, bag.fraction = 0.5\n", "\t\t, train.fraction = nrow(train) / (nrow(train) + nrow(test))\n", "\t\t)\n", ")\n", "# Determine best iteration based on test data\n", "best.iter = gbm.perf(gbm.model, method = \"test\")\n", "\n", "# Get feature importance\n", "gbm.feature.imp = summary(gbm.model, n.trees = best.iter)\n", "\n", "# Plot and calculate AUC on test data\n", "gbm.test = predict(gbm.model, newdata = test, n.trees = best.iter)\n", "auc.gbm = roc(test$Class, gbm.test, plot = TRUE, col = \"red\")\n", "print(auc.gbm)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "70730eb828cd134045fa852cd82337289b49cc1a", "_cell_guid": "7f694f3f-43e3-0c58-8f6f-f07de5c2967d"}, "source": ["## xgboost\n", "\n", "Per [Andrea's comment][1] about tree depth between lightGBM and xgboost, I am adding a second xgboost model to address this. The original xgboost model has `max.depth = 3` which allows for up to 7 decision splits in the tree. \n", "\n", "I have now added in `tree_growth = 'hist'` into the comparisons. Thanks [CPMP][2]!\n", "\n", "\n", "  [1]: https://www.kaggle.com/nschneider/d/dalpozz/creditcardfraud/gbm-vs-xgboost-vs-lightgbm#178605 \"Andrea's Comment\"\n", "  [2]: https://www.kaggle.com/nschneider/gbm-vs-xgboost-vs-lightgbm#209155 \"CPMP's Comment\""]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "2c2d6c8eaa480ec5c3e890d6ba9649d9e7d0b6eb", "_cell_guid": "f8d702bc-f7c9-d00a-b82d-ff74b5167974"}, "execution_count": null, "source": ["library(xgboost, quietly=TRUE)\n", "xgb.data.train <- xgb.DMatrix(as.matrix(train[, colnames(train) != \"Class\"]), label = train$Class)\n", "xgb.data.test <- xgb.DMatrix(as.matrix(test[, colnames(test) != \"Class\"]), label = test$Class)\n", "\n", "# Get the time to train the xgboost model\n", "xgb.bench.speed = microbenchmark(\n", "\txgb.model.speed <- xgb.train(data = xgb.data.train\n", "\t\t, params = list(objective = \"binary:logistic\"\n", "\t\t\t, eta = 0.1\n", "\t\t\t, max.depth = 3\n", "\t\t\t, min_child_weight = 100\n", "\t\t\t, subsample = 1\n", "\t\t\t, colsample_bytree = 1\n", "\t\t\t, nthread = 3\n", "\t\t\t, eval_metric = \"auc\"\n", "\t\t\t)\n", "\t\t, watchlist = list(test = xgb.data.test)\n", "\t\t, nrounds = 500\n", "\t\t, early_stopping_rounds = 40\n", "\t\t, print_every_n = 20\n", "\t\t)\n", "    , times = 5L\n", ")\n", "print(xgb.bench.speed)\n", "print(xgb.model.speed$bestScore)\n", "\n", "# Make predictions on test set for ROC curve\n", "xgb.test.speed = predict(xgb.model.speed\n", "                   , newdata = as.matrix(test[, colnames(test) != \"Class\"])\n", "                   , ntreelimit = xgb.model.speed$bestInd)\n", "auc.xgb.speed = roc(test$Class, xgb.test.speed, plot = TRUE, col = \"blue\")\n", "print(auc.xgb.speed)"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "8391f54c08512bddf0e8d61202ae28cecb5491e6", "_cell_guid": "703c52e7-5c3b-c2b3-c9b3-7f72dbd13b5a"}, "execution_count": null, "source": ["# Train a deeper xgboost model to compare accuarcy.\n", "xgb.bench.acc = microbenchmark(\n", "\txgb.model.acc <- xgb.train(data = xgb.data.train\n", "\t\t, params = list(objective = \"binary:logistic\"\n", "\t\t\t, eta = 0.1\n", "\t\t\t, max.depth = 7\n", "\t\t\t, min_child_weight = 100\n", "\t\t\t, subsample = 1\n", "\t\t\t, colsample_bytree = 1\n", "\t\t\t, nthread = 3\n", "\t\t\t, eval_metric = \"auc\"\n", "\t\t\t)\n", "\t\t, watchlist = list(test = xgb.data.test)\n", "\t\t, nrounds = 500\n", "\t\t, early_stopping_rounds = 40\n", "\t\t, print_every_n = 20\n", "\t\t)\n", "    , times = 5L\n", ")\n", "print(xgb.bench.acc)\n", "print(xgb.model.acc$bestScore)\n", "\n", "#Get feature importance\n", "xgb.feature.imp = xgb.importance(model = xgb.model.acc)\n", "\n", "# Make predictions on test set for ROC curve\n", "xgb.test.acc = predict(xgb.model.acc\n", "                   , newdata = as.matrix(test[, colnames(test) != \"Class\"])\n", "                   , ntreelimit = xgb.model.acc$bestInd)\n", "auc.xgb.acc = roc(test$Class, xgb.test.acc, plot = TRUE, col = \"blue\")\n", "print(auc.xgb.acc)"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "9d69ae1b9a8187eac4d98754402fb5270c877a05", "_cell_guid": "afbb958b-8943-4d3a-a883-e15751a6f7b5"}, "execution_count": null, "source": ["# xgBoost with Histogram\n", "xgb.bench.hist = microbenchmark(\n", "\txgb.model.hist <- xgb.train(data = xgb.data.train\n", "\t\t, params = list(objective = \"binary:logistic\"\n", "\t\t\t, eta = 0.1\n", "\t\t\t, max.depth = 7\n", "\t\t\t, min_child_weight = 100\n", "\t\t\t, subsample = 1\n", "\t\t\t, colsample_bytree = 1\n", "\t\t\t, nthread = 3\n", "\t\t\t, eval_metric = \"auc\"\n", "            , tree_method = \"hist\"\n", "            , grow_policy = \"lossguide\"\n", "\t\t\t)\n", "\t\t, watchlist = list(test = xgb.data.test)\n", "\t\t, nrounds = 500\n", "\t\t, early_stopping_rounds = 40\n", "\t\t, print_every_n = 20\n", "\t\t)\n", "    , times = 5L\n", ")\n", "print(xgb.bench.hist)\n", "print(xgb.model.hist$bestScore)\n", "\n", "#Get feature importance\n", "xgb.feature.imp = xgb.importance(model = xgb.model.hist)\n", "\n", "# Make predictions on test set for ROC curve\n", "xgb.test.hist = predict(xgb.model.hist\n", "                   , newdata = as.matrix(test[, colnames(test) != \"Class\"])\n", "                   , ntreelimit = xgb.model.hist$bestInd)\n", "auc.xgb.hist = roc(test$Class, xgb.test.hist, plot = TRUE, col = \"blue\")\n", "print(auc.xgb.hist)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "0948d8eadf22b1b7fa72ece6756ae22c88e238b1", "_cell_guid": "59fec7ed-727a-f92b-d59d-1ffa447629b2"}, "source": ["## lightGBM"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "3c2deddfd747795ca730e62c78b9671c1c163f99", "_cell_guid": "964fa5d7-597f-1a15-d666-34d7a4abc12d"}, "execution_count": null, "source": ["library(lightgbm, quietly=TRUE)\n", "lgb.train = lgb.Dataset(as.matrix(train[, colnames(train) != \"Class\"]), label = train$Class)\n", "lgb.test = lgb.Dataset(as.matrix(test[, colnames(test) != \"Class\"]), label = test$Class)\n", "\n", "params.lgb = list(\n", "\tobjective = \"binary\"\n", "\t, metric = \"auc\"\n", "\t, min_data_in_leaf = 1\n", "\t, min_sum_hessian_in_leaf = 100\n", "\t, feature_fraction = 1\n", "\t, bagging_fraction = 1\n", "\t, bagging_freq = 0\n", "\t)\n", "\n", "# Get the time to train the lightGBM model\n", "lgb.bench = microbenchmark(\n", "\tlgb.model <- lgb.train(\n", "\t\tparams = params.lgb\n", "\t\t, data = lgb.train\n", "\t\t, valids = list(test = lgb.test)\n", "\t\t, learning_rate = 0.1\n", "\t\t, num_leaves = 7\n", "\t\t, num_threads = 2\n", "\t\t, nrounds = 500\n", "\t\t, early_stopping_rounds = 40\n", "\t\t, eval_freq = 20\n", "\t\t)\n", "\t\t, times = 5L\n", ")\n", "print(lgb.bench)\n", "print(max(unlist(lgb.model$record_evals[[\"test\"]][[\"auc\"]][[\"eval\"]])))\n", "\n", "# get feature importance\n", "lgb.feature.imp = lgb.importance(lgb.model, percentage = TRUE)\n", "\n", "# make test predictions\n", "lgb.test = predict(lgb.model, data = as.matrix(test[, colnames(test) != \"Class\"]), n = lgb.model$best_iter)\n", "auc.lgb = roc(test$Class, lgb.test, plot = TRUE, col = \"green\")\n", "print(auc.lgb)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "4565fd2ae49868d6b95f9ca5e1fd6ca98e766936", "_cell_guid": "6b627a20-fe3e-1b39-8818-4047fdb0e656"}, "source": ["# Results\n", "\n", "## Speed\n", "\n", "-----\n", "\n", "The following shows the estimated **GBM** benchmark (see above for actual) and the microbenchmark results for the **xgboost** and **lightgbm** models."]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "a26bac004e697d5623cd42a81215cf4b387c1343", "_cell_guid": "4a8bc848-1841-7ac9-d81e-401b3e5aeae2"}, "execution_count": null, "source": ["print(\"GBM = ~243s\")\n", "print(xgb.bench.speed)\n", "print(xgb.bench.hist)\n", "print(lgb.bench)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "cdd6dcfcc4926571ae7fcaa4537997352478fa8c", "_cell_guid": "37d069b2-5a6e-b383-27c9-4a1a993d8ada"}, "source": ["## Accuracy\n", "\n", "-----\n", "\n", "The following are the *AUC* results for the test set. \n", "\n", "\n", "### GBM"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "1633a06c6c5c20b77a28effa3c1d11f2180dc1e4", "_cell_guid": "82bbcf8f-96fb-80dd-09e9-1b279666a8f4"}, "execution_count": null, "source": ["print(auc.gbm)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "6e443b82b8f0394aa3622058c33775bde43ef2e4", "_cell_guid": "7f977fd5-5963-29cf-57cd-6e7bf2b61423"}, "source": ["## xgboost"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "d2aa8341542b1ff0aad51ec92235b5bf0994de7f", "_cell_guid": "e80c7564-19b7-9fa8-d0a1-277ac07f12c0"}, "execution_count": null, "source": ["print(auc.xgb.acc)\n", "print(auc.xgb.hist)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "81da63611f4224cc9c96f5430a2f3f9c2b8f6983", "_cell_guid": "ba775a11-0d5b-aa78-8bb5-8c75ef068870"}, "source": ["## lightGBM"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "3154a0b24582206168a8d820190ee24d8e960758", "_cell_guid": "4bcc82db-8b51-b8da-6813-202598c1976e"}, "execution_count": null, "source": ["print(auc.lgb)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "63c9d6cbc53abd215861d7ae7a97af68c06d493b", "_cell_guid": "6e8f528d-1035-9b8d-5c67-a022a1104354"}, "source": ["## Feature Importance\n", "\n", "-----\n", "\n", "The top features selected by all three models were very similar. Although, my understanding is that GBM is only based on frequency. The top 5 features were the same expect for GBM selecting v20 as an important feature. It is interesting that xgboost selects so few features. \n", "\n", "### GBM"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "c6a20be791d88a8ece53fd8baa18857ce50307f5", "collapsed": true, "_cell_guid": "33a77ad3-7dca-2628-f1a2-04386e2f89d4"}, "execution_count": null, "source": ["print(gbm.feature.imp)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "aa3cd2b3126f3c6a50183a31dff231e32761f550", "_cell_guid": "71123f33-f448-4faf-c8ca-3b826ef1896c"}, "source": ["### xgboost"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "78e827b3b4e5ebf050d88c0426f165e93a9e7931", "collapsed": true, "_cell_guid": "28ac3cd1-85f9-d33b-d111-119494e75b37"}, "execution_count": null, "source": ["print(xgb.feature.imp)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "57c5e061685bd040556102e7f391a8aaa284b272", "_cell_guid": "a20f7b26-6067-ecaf-3de2-9fbbf99058a9"}, "source": ["### lightGBM"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "3a8e4cc9a4cc0d87872c54e26e4adca7ce69892a", "collapsed": true, "_cell_guid": "c2c16d4e-11d5-7d6f-5706-b0f7aa9906ef"}, "execution_count": null, "source": ["print(lgb.feature.imp)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "ebcf8dc86e1bb3ec6a6478ae47f6ed7972d563ca", "_cell_guid": "170a3f49-33cb-d066-d8dd-f87a71f5f8ed"}, "source": ["# Additional Observations\n", "\n", "## GBM\n", "\n", "Advantages:\n", "\n", "* None\n", "\n", "Disadvantages:\n", "\n", "* No early exit\n", "* Slower training\n", "* Less accurate\n", "\n", "## xgboost\n", "\n", "Advantages:\n", "\n", "* Proven success (on kaggle)\n", "* Now with Histogram Binning\n", "\n", "Disadvantages:\n", "\n", "* Traditionally slower than lightGBM, but `tree_method = 'hist'` is a big improvement.\n", "\n", "## lightGBM\n", "\n", "Advantages:\n", "\n", "* Fast training efficiency\n", "* Low memory usage\n", "* Better accuracy\n", "* Parallel learning supported\n", "* Deal with large scale data\n", "* Corporate supported\n", "\n", "Disadvantages:\n", "\n", "* Newer, so less community documentation"]}, {"cell_type": "markdown", "metadata": {"_uuid": "7b418651f091fa89ddea011db459d7b259a759d5", "_cell_guid": "77d9178c-6446-640d-c8a9-f834a9697a6a"}, "source": ["## Post Script\n", "\n", "In this example lightGBM completed in 15% of the time it took xgboost. That seems too extreme to me. Any one have feedback on how I may not be fairly parameterizing xgboost in this comparison? **Update:** Switching xgboost to `tree_method = 'hist'` makes the comparison much closer.\n", "\n", "## Release Notes\n", "\n", "### - version(17)\n", "\n", "#### major changes\n", "\n", "* Added a section for `xgboost` using `tree_method = 'hist'`\n", "\n", "### - version(16)\n", "\n", "#### Major changes\n", "\n", "* Added feature importance comparison\n", "\n", "#### Minor changes\n", "\n", "* some code comments\n", "\n", "### - version(12)\n", "\n", "#### Major changes\n", "\n", "* Added second xgboost model to compare accuracy (allows for deeper tree growth)\n", "\n", "#### Minor changes\n", "\n", "* Reduced warning levels on depreciated parameters in xgboost\n", "* Updated GBM run time based on latest kernel run."]}], "metadata": {"kernelspec": {"language": "R", "name": "ir", "display_name": "R"}, "language_info": {"file_extension": ".r", "version": "3.4.1", "pygments_lexer": "r", "name": "R", "codemirror_mode": "r", "mimetype": "text/x-r-source"}, "_is_fork": false, "_change_revision": 0}, "nbformat_minor": 1}